{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Dv-wHVlz5XIK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pz8Ptj8p5WD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Relevance Classifier**"
      ],
      "metadata": {
        "id": "Dv-wHVlz5XIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Requirement**: Llama 3.2 3B\n",
        "\n",
        "**Task:** Design a prompt that evaluates if a given research paper title and abstract is relevant to the user query on a scale of 1-4 (low to high).\n",
        "\n",
        "**Instructions:**\n",
        "1. Define clear criteria for each relevance level (1-4)\n",
        "2. Consider paper title, abstract, and user question as input.\n",
        "3. Provide structured output with justification.\n",
        "4. You can take random title, abstract and queries from any internet sources."
      ],
      "metadata": {
        "id": "Ed4don1rkPNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# for openai api structures\n",
        "!pip install openai\n",
        "!pip install chromadb\n"
      ],
      "metadata": {
        "id": "xJkKTUMGkteI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom exception handler\n",
        "import traceback\n",
        "import os\n",
        "import sys\n",
        "async def handle_exception(exception : Exception):\n",
        "    exception_type = type(exception).__name__\n",
        "    exception_message = str(exception)\n",
        "    exception_traceback = traceback.extract_tb(exception.__traceback__)\n",
        "    line_number = exception_traceback[-1].lineno\n",
        "    print(f\"Exception Type: {exception_type}\")\n",
        "    print(f\"Exception Message: {exception_message}\")\n",
        "    print(f\"Line Number: {line_number}\")\n",
        "    print(\"Full Traceback:\")\n",
        "    print(\"\".join(traceback.format_tb(exception.__traceback__)))\n",
        "\n",
        "    return {'error': str(exception)}\n"
      ],
      "metadata": {
        "id": "WNeiaWrLqMR-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retry on rate limiting due to limited concurrent requests which will eventually solve future production bug"
      ],
      "metadata": {
        "id": "RavmcQD9wuOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Union, Any, Optional,AsyncGenerator,Tuple\n",
        "from openai import AsyncOpenAI\n",
        "import json\n",
        "import re\n",
        "\n",
        "llama3b = 'meta-llama/Llama-3.2-3B-Instruct'\n",
        "hf_base_url = 'https://api-inference.huggingface.co/v1/'\n",
        "\n",
        "# meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n",
        "# \"https://api.deepinfra.com/v1/openai\"\n",
        "\n",
        "\n",
        "class GenResponse:\n",
        "    # default model ID\n",
        "    DEFAULT_MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "    def __init__(self, api_key: str,base_url : str, openai:bool = False):\n",
        "        if not api_key or not isinstance(api_key, str):\n",
        "            raise ValueError(\"API key is required and must be a string.\")\n",
        "\n",
        "        if (not base_url or not isinstance(base_url, str)) and not openai:\n",
        "            raise ValueError(\"Base Url is required and must be a string.\")\n",
        "\n",
        "        self.key = api_key\n",
        "        self.client = AsyncOpenAI(\n",
        "            api_key=self.key,\n",
        "            base_url = base_url,\n",
        "        ) if not openai else AsyncOpenAI(api_key= api_key)\n",
        "\n",
        "    async def get_response(\n",
        "        self,\n",
        "        query: str,\n",
        "        system: str,\n",
        "        chat_history: Optional[List[Dict]] = None,\n",
        "        model_id: Optional[str] = DEFAULT_MODEL_ID,\n",
        "        temperature: float = 0.5,\n",
        "        max_tokens: int = 500,\n",
        "        top_p: float = 0.95,\n",
        "        top_k: int = 500,\n",
        "        frequency_penalty: float = 0,\n",
        "        presence_penalty: float = 0,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        response_format: Optional[Dict] = None,\n",
        "        tools: Optional[List[Dict]] = None,\n",
        "    ) -> Union[str, Dict[str, Any]]:\n",
        "\n",
        "        # required inputs\n",
        "        if not query or not isinstance(query, str):\n",
        "            raise ValueError(\"Query is required and must be a string.\")\n",
        "        if not system or not isinstance(system, str):\n",
        "            raise ValueError(\"System is required and must be a string.\")\n",
        "        if chat_history is not None and not isinstance(chat_history, list):\n",
        "            raise TypeError(\"Chat history must be a list of dictionaries.\")\n",
        "\n",
        "        # chat history if not provided\n",
        "        if chat_history is None:\n",
        "            chat_history = []\n",
        "\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            *chat_history,\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "\n",
        "            response = await self.client.chat.completions.create(\n",
        "                model=model_id,\n",
        "                temperature=temperature,\n",
        "                max_tokens=max_tokens,\n",
        "                top_p=top_p,\n",
        "                response_format=response_format,\n",
        "                messages=messages,\n",
        "                tools=tools,\n",
        "            )\n",
        "\n",
        "            return response\n",
        "\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "          error = await handle_exception(e)\n",
        "          return error\n",
        "\n",
        "    async def get_stream_response(\n",
        "        self,\n",
        "        query: str,\n",
        "        system: str,\n",
        "        chat_history: Optional[List[Dict]] = None,\n",
        "        model_id: Optional[str] = DEFAULT_MODEL_ID,\n",
        "        temperature: float = 0.5,\n",
        "        max_tokens: int = 500,\n",
        "        top_p: float = 0.95,\n",
        "        top_k: int = 1000,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        response_format: Optional[Dict] = None,\n",
        "    ) -> AsyncGenerator:\n",
        "\n",
        "        # required inputs\n",
        "        if not query or not isinstance(query, str):\n",
        "            raise ValueError(\"Query is required and must be a string.\")\n",
        "        if not system or not isinstance(system, str):\n",
        "            raise ValueError(\"System is required and must be a string.\")\n",
        "        if chat_history is not None and not isinstance(chat_history, list):\n",
        "            raise TypeError(\"Chat history must be a list of dictionaries.\")\n",
        "\n",
        "        # chat history if not provided\n",
        "        if chat_history is None:\n",
        "            chat_history = []\n",
        "\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            *chat_history,\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "\n",
        "            response = await self.client.chat.completions.create(\n",
        "                model=model_id,\n",
        "                temperature=temperature,\n",
        "                max_tokens=max_tokens,\n",
        "                top_p=top_p,\n",
        "                messages=messages,\n",
        "                stream=True,\n",
        "            )\n",
        "\n",
        "            full_content = ''\n",
        "            async for event in response:\n",
        "              if event.choices[0].finish_reason:\n",
        "                print(event.choices[0].finish_reason)\n",
        "                yield full_content\n",
        "\n",
        "              else:\n",
        "                print(event.choices[0].delta.content)\n",
        "                full_content += event.choices[0].delta.content\n",
        "\n",
        "                yield event.choices[0].delta.content\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "          error = await handle_exception(e)\n",
        "          yield error\n",
        "\n",
        "    async def format_sse(self,data: str, event: Optional[str] = None) -> str:\n",
        "      \"\"\"Formats the data into a Server-Sent Events (SSE) format.\n",
        "\n",
        "      Args:\n",
        "        data (str): The data to be formatted.\n",
        "        event (Optional[str]): The event type (e.g., 'event' or 'body'). Defaults to None.\n",
        "\n",
        "      Returns:\n",
        "        str: The formatted SSE data.\n",
        "\n",
        "      Example:\n",
        "        >>> await format_sse(\"Hello, world!\", \"text\")\n",
        "        'event: text\\ndata: \"Hello, world!\"\\n\\n'\n",
        "         \"\"\"\n",
        "      formatted_data = f'data: {json.dumps(data)}\\n\\n'\n",
        "\n",
        "      if event:\n",
        "        formatted_data = f'event: {event}\\n{formatted_data}'\n",
        "\n",
        "      return formatted_data\n",
        "\n",
        "    async def extract_xml(self,text: str, tag: str) -> str:\n",
        "      \"\"\"\n",
        "      Extracts the content of the specified XML tag from the given text. Used for parsing structured responses\n",
        "      Args:\n",
        "        text (str): The text containing the XML.\n",
        "        tag (str): The XML tag to extract content from.\n",
        "      Returns:\n",
        "        str: The content of the specified XML tag, or an empty string if the tag is not found.\n",
        "      \"\"\"\n",
        "      match = re.search(f'<{tag}>(.*?)</{tag}>', text, re.DOTALL)\n",
        "      return match.group(1) if match else None\n",
        "\n",
        "    async def get_json(self, text: str) -> Dict:\n",
        "      \"\"\"\n",
        "      Extracts the JSON content from the given text.\n",
        "      Args:\n",
        "        text (str): The text containing the JSON.\n",
        "      Returns:\n",
        "        Dict: The JSON content as a Python dictionary.\n",
        "      \"\"\"\n",
        "      try:\n",
        "        json_content = json.loads(text)\n",
        "        return json_content\n",
        "      except Exception as e:\n",
        "        error = await handle_exception(e)\n",
        "        return error"
      ],
      "metadata": {
        "id": "-lKBiLSWz7u6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Research paper Relevance Classifier**\n",
        "\n",
        "1. **Relevance** - As Relevance have 4 levels. I will go with the minimum relevance or irrelevant to Most relevant (title and Abstract is semantically,keywords,aspect wise ,etc showing most relevance)\n",
        "\n",
        "2. **Prompt**- As llama 3.2 model has been trained on scientific data .**Zero Shot** prompting should work.\n"
      ],
      "metadata": {
        "id": "mUU67_xTkmtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class RCOutput(BaseModel):\n",
        "    analysis: str = Field(\n",
        "        description=\"A detailed analysis of the relevance criteria.\"\n",
        "    )\n",
        "    justification: str = Field(\n",
        "        description=\"A brief explanation of why the paper received the given score, based on the alignment of the title, abstract, and user query.\"\n",
        "    )\n",
        "    relevance_level: int = Field(\n",
        "        description=\" A score between 1 and 4 indicating the relevance of the paper to the user query.\"\n",
        "    )\n",
        "\n",
        "output_json = RCOutput.model_json_schema()\n",
        "print(output_json)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bIma_Mdqf-Z",
        "outputId": "5490b5b2-c52b-4587-975e-1b54cbb3e601"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'properties': {'analysis': {'description': 'A detailed analysis of the relevance criteria.', 'title': 'Analysis', 'type': 'string'}, 'justification': {'description': 'A brief explanation of why the paper received the given score, based on the alignment of the title, abstract, and user query.', 'title': 'Justification', 'type': 'string'}, 'relevance_level': {'description': ' A score between 1 and 4 indicating the relevance of the paper to the user query.', 'title': 'Relevance Level', 'type': 'integer'}}, 'required': ['analysis', 'justification', 'relevance_level'], 'title': 'RCOutput', 'type': 'object'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Role prompting, or persona prompting,\n",
        "# SimToM (Simulated Theory of Mind)\n",
        "# RaR,R2e\n",
        "\n",
        "zero_shot_system = \"\"\"<role>\n",
        "You are a expert research paper relevance classifier who will always give output in json format. Before classification you will analyze title and abstract of the research paper in multi aspect relative to user query\\\n",
        ".</role>\n",
        "<task>\n",
        "Evaluate  and reason the relevance of a given research paper's **title** and **abstract** to a user query on a scale of 1-4 (low to high).\n",
        "</task>\n",
        "\n",
        "<inputs>\n",
        "\n",
        "<title>\n",
        "{title}\n",
        "</title>\n",
        "\n",
        "<abstract>\n",
        "{abstract}\n",
        "</abstract>\n",
        "\n",
        "</inputs>\n",
        "\n",
        "\n",
        "<guidelines>\n",
        "Let's think step by step-\n",
        "1. Rephrase and expand the user query if ambiguous for reasoning.\n",
        "2. Break user query in multi aspect, analyze all scientific keywords or their semantic meaning,topic  and what actually user is looking for.\n",
        "3. Analyze title's alignmnet with user query. provide a score between 0 to 1.\n",
        "4. Analyze abstrcat's alignmnet with user query. provide a score between 0 to 1.\n",
        "5. Finally provide a relevance_level between 0 to 1 by following this:\n",
        "<relevance_level>\n",
        "1 (Low Relevance): The paper title and abstract have no direct connection to the user query. The topic, keywords, and context do not align.\n",
        "2 (Moderate Relevance): The paper title or abstract contains some related keywords or concepts, but the connection is weak or tangential. The paper may touch on a broader topic without addressing the specific query.\n",
        "3 (High Relevance): The paper title and abstract align well with the user query. The topic, keywords, and context are closely related, and the paper likely provides useful insights or answers to the query.\n",
        "4 (Very High Relevance): The paper title and abstract are highly aligned with the user query. The topic, keywords, and context are directly relevant, and the paper is likely to provide a comprehensive answer or solution to the query.\n",
        "</relevance_level>\n",
        "</guidelines>\n",
        "\n",
        "You will always give output in this valid Json format only which will be consume by automated json parsing system:\n",
        "{{\n",
        "    \"analysis\": \"Your analysis as per guidelines\",\n",
        "    \"justification\": \"A brief explanation of why the paper received the given score, based on the alignment of the title, abstract, and user query\",\n",
        "    \"relevance_level\": \"A score between 1 and 4 indicating the relevance of the paper to the user query.\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "async def generate_prompt(title,abstract):\n",
        "  return zero_shot_system.format(\n",
        "      title = title,\n",
        "      abstract = abstract,\n",
        "  )"
      ],
      "metadata": {
        "id": "wOeLSyowklXL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paper = (\"\"\"Tab-CoT: Zero-shot Tabular Chain of Thought\"\"\",\"\"\"The chain-of-though (CoT) prompting methods\n",
        "were successful in various natural language processing (NLP) tasks thanks to their ability to\n",
        "unveil the underlying complex reasoning processes. Such reasoning processes typically exhibit implicitly structured steps. Recent efforts\n",
        "also started investigating methods to encourage\n",
        "more explicitly structured reasoning procedures\n",
        "to be captured (Zhou et al., 2022). In this work,\n",
        "we propose Tab-CoT, a novel tabular-format\n",
        "CoT prompting method, which allows the complex reasoning process to be explicitly modelled in a highly structured manner. Despite\n",
        "its simplicity, we show that our approach is capable of performing reasoning across multiple\n",
        "dimensions (i.e., both rows and columns). We\n",
        "demonstrate our approach’s strong zero-shot\n",
        "and few-shot capabilities through extensive experiments on a range of reasoning tasks.\"\"\")\n"
      ],
      "metadata": {
        "id": "0INDuaIu0_8l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system = await generate_prompt(\n",
        "    title = paper[0],\n",
        "    abstract = paper[1],\n",
        "\n",
        ")\n",
        "print(system)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21PnU-6l1YcB",
        "outputId": "8cb5824c-c06d-498a-f861-6563fa5488fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<role>\n",
            "You are a expert research paper relevance classifier who will always give output in json format. Before classification you will analyze title and abstract of the research paper in multi aspect relative to user query.</role>\n",
            "<task>\n",
            "Evaluate  and reason the relevance of a given research paper's **title** and **abstract** to a user query on a scale of 1-4 (low to high).\n",
            "</task>\n",
            "\n",
            "<inputs>\n",
            "\n",
            "<title>\n",
            "Tab-CoT: Zero-shot Tabular Chain of Thought\n",
            "</title>\n",
            "\n",
            "<abstract>\n",
            "The chain-of-though (CoT) prompting methods\n",
            "were successful in various natural language processing (NLP) tasks thanks to their ability to\n",
            "unveil the underlying complex reasoning processes. Such reasoning processes typically exhibit implicitly structured steps. Recent efforts\n",
            "also started investigating methods to encourage\n",
            "more explicitly structured reasoning procedures\n",
            "to be captured (Zhou et al., 2022). In this work,\n",
            "we propose Tab-CoT, a novel tabular-format\n",
            "CoT prompting method, which allows the complex reasoning process to be explicitly modelled in a highly structured manner. Despite\n",
            "its simplicity, we show that our approach is capable of performing reasoning across multiple\n",
            "dimensions (i.e., both rows and columns). We\n",
            "demonstrate our approach’s strong zero-shot\n",
            "and few-shot capabilities through extensive experiments on a range of reasoning tasks.\n",
            "</abstract>\n",
            "\n",
            "</inputs>\n",
            "\n",
            "\n",
            "<guidelines>\n",
            "Let's think step by step-\n",
            "1. Rephrase and expand the user query if ambiguous for reasoning.\n",
            "2. Break user query in multi aspect, analyze all scientific keywords or their semantic meaning,topic  and what actually user is looking for.\n",
            "3. Analyze title's alignmnet with user query. provide a score between 0 to 1.\n",
            "4. Analyze abstrcat's alignmnet with user query. provide a score between 0 to 1.\n",
            "5. Finally provide a relevance_level between 0 to 1 by following this:\n",
            "<relevance_level>\n",
            "1 (Low Relevance): The paper title and abstract have no direct connection to the user query. The topic, keywords, and context do not align.\n",
            "2 (Moderate Relevance): The paper title or abstract contains some related keywords or concepts, but the connection is weak or tangential. The paper may touch on a broader topic without addressing the specific query.\n",
            "3 (High Relevance): The paper title and abstract align well with the user query. The topic, keywords, and context are closely related, and the paper likely provides useful insights or answers to the query.\n",
            "4 (Very High Relevance): The paper title and abstract are highly aligned with the user query. The topic, keywords, and context are directly relevant, and the paper is likely to provide a comprehensive answer or solution to the query.\n",
            "</relevance_level>\n",
            "</guidelines>\n",
            "\n",
            "You will always give output in this valid Json format only which will be consume by automated json parsing system:\n",
            "{\n",
            "    \"analysis\": \"Your analysis as per guidelines\",\n",
            "    \"justification\": \"A brief explanation of why the paper received the given score, based on the alignment of the title, abstract, and user query\",\n",
            "    \"relevance_level\": \"A score between 1 and 4 indicating the relevance of the paper to the user query.\"\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "DEEPINFRA_TOKEN = userdata.get('DEEP_INFRA_KEY')\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "DEEP_INFRA_BASE_URL = 'https://api.deepinfra.com/v1/openai'"
      ],
      "metadata": {
        "id": "iJn7DIb-696S"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llm handling class\n",
        "model = GenResponse(api_key=DEEPINFRA_TOKEN,base_url = DEEP_INFRA_BASE_URL)\n",
        "\n",
        "async def classify_paper(\n",
        "    query: str,\n",
        "    system: str,\n",
        "    title :str,\n",
        "    abstract: str,\n",
        "    llm_handler : GenResponse = model\n",
        "    )->Dict:\n",
        "\n",
        "    # system formatting\n",
        "    system = system.format(\n",
        "      title = title,\n",
        "      abstract = abstract,\n",
        "    )\n",
        "    response = await llm_handler.get_response(\n",
        "        system = system,\n",
        "        query = f\"<query>{query}</query>\",\n",
        "        chat_history = [],\n",
        "        temperature = .5,\n",
        "        max_tokens = 1000,\n",
        "        response_format = {\"type\": \"json\"}\n",
        "        )\n",
        "\n",
        "    if isinstance(response,Dict):\n",
        "      return response\n",
        "    # json validate\n",
        "    json_response = await llm_handler.get_json(response.choices[0].message.content)\n",
        "    # try with feedback\n",
        "    if 'error' in json_response:\n",
        "       print(\"Feedback\")\n",
        "       feedback = f\"This is json parsing error in your last response :\\n{json_response['error']}.\\nLast Response :\\n{response.choices[0].message.content}.\\n Correct it and make sure all the keys and values are present in valid json format.\"\n",
        "       chat_history = [\n",
        "           {\"role\": \"system\", \"content\": system},\n",
        "          {\"role\": \"user\", \"content\": f\"<query>{query}</query>\"},\n",
        "          {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
        "           ]\n",
        "       response = await llm_handler.get_response(\n",
        "          system = system,\n",
        "          query = f\"<feedback>{feedback}</feedback>\",\n",
        "          chat_history = chat_history,\n",
        "          temperature = .5,\n",
        "          max_tokens = 1000,\n",
        "          response_format = {\"type\": \"json\"}\n",
        "          )\n",
        "       json_response = await llm_handler.get_json(response.choices[0].message.content)\n",
        "    return json_response\n"
      ],
      "metadata": {
        "id": "do4bpY1959CO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = GenResponse(api_key=DEEPINFRA_TOKEN,base_url = DEEP_INFRA_BASE_URL)\n",
        "await classify_paper(\n",
        "    query = 'what is tabular chain of thought',\n",
        "    system = zero_shot_system,\n",
        "    title = paper[0],\n",
        "    abstract = paper[1],\n",
        "    llm_handler = model\n",
        ")"
      ],
      "metadata": {
        "id": "UDP5pQXo1ucb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ba8286-c479-44e6-c1ae-0fdc7f9f6475"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'analysis': \"The user query 'what is tabular chain of thought' can be rephrased as 'Tabular Chain of Thought: A method for explicit reasoning in tabular formats'. The query breaks down into multi-aspects: 'Tabular', 'Chain of Thought', and 'explicit reasoning'. The scientific keywords 'Tabular CoT' and 'explicit reasoning' are present in the paper title. The topic of the query is related to Natural Language Processing (NLP) and Reasoning. The context of the query is inquiring about a specific method or approach. The paper title and abstract are aligned with the query as they discuss a novel method 'Tab-CoT' for explicit reasoning in tabular formats.\",\n",
              " 'justification': \"The query is closely related to the paper title and abstract as they discuss a novel method 'Tab-CoT' for explicit reasoning in tabular formats. The query is not ambiguous and directly inquires about the topic of the paper. The alignment between the query and the paper is high due to the presence of similar keywords and the relevance of the topic.\",\n",
              " 'relevance_level': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokens = model.get_stream_response(\n",
        "    query='hi',\n",
        "    system = \" \",\n",
        "    temperature = .5,\n",
        "    max_tokens = 100,\n",
        ")\n",
        "async for token in tokens:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjyxgZOoZ7id",
        "outputId": "5336c950-2e3a-4919-a8f5-7c69339d9df9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "How\n",
            "How\n",
            " are\n",
            " are\n",
            " you\n",
            " you\n",
            " today\n",
            " today\n",
            "?\n",
            "?\n",
            "\n",
            "\n",
            "stop\n",
            "How are you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG**"
      ],
      "metadata": {
        "id": "miQ4lrWRpihh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "MODEL_ID = 'models/gemini-1.5-flash'\n",
        "GEMINI_KEY = \"AIzaSyBGi3Z_7Ns_DtSAG_LWJwe2C1YkXaMG7AI\"\n",
        "GEMINI_BASE_URL = 'https://generativelanguage.googleapis.com/v1beta/openai/'"
      ],
      "metadata": {
        "id": "IHanWKtjgRKm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenResponse(api_key=GEMINI_KEY,base_url = GEMINI_BASE_URL)\n",
        "tokens = model.get_stream_response(\n",
        "    query='hi',\n",
        "    system = \" \",\n",
        "    model_id = 'gemini-1.5-flash',\n",
        "    temperature = .5,\n",
        "    max_tokens = 100,\n",
        ")\n",
        "async for token in tokens:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXPv5LNWg_Xm",
        "outputId": "168a1283-65a2-42a8-94f7-00e552e382eb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi\n",
            "Hi\n",
            " there! How can I help you today?\n",
            "\n",
            " there! How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=GEMINI_KEY)\n",
        "def embedding_model(user_message : str) -> List[float]:\n",
        "  embedding = genai.embed_content(\n",
        "        model=\"models/text-embedding-004\",\n",
        "        content=user_message\n",
        "        )\n",
        "  return embedding['embedding']\n"
      ],
      "metadata": {
        "id": "XWlSgcNLRq_F"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "import pandas as pd\n",
        "\n",
        "max_results = 10\n",
        "train_data_path = 'train_data.xlsx'\n",
        "chromadb_path = 'chromadb'\n",
        "\n",
        "class RetrievalPipeline(object):\n",
        "    \"\"\"\n",
        "    Manages all the fuction related to retrival and vector database\n",
        "    \"\"\"\n",
        "    class MyEmbeddingFunction(chromadb.EmbeddingFunction):\n",
        "        \"\"\"\n",
        "        This is a custom function that generates embeddings for text data using the given model.\n",
        "        \"\"\"\n",
        "        def __call__(self, Docs: chromadb.Documents) -> chromadb.Embeddings:\n",
        "            \"\"\"\n",
        "            This function generates embeddings for a list of text documents using the given model.\n",
        "            Args:\n",
        "                Docs (chromadb.Documents): A list of text documents.\n",
        "            Returns:\n",
        "                chromadb.Embeddings: A list of embeddings (numerical representations) for the input text documents.\n",
        "            \"\"\"\n",
        "            embeddings = [embedding_model(chunk) for chunk in Docs]\n",
        "            return embeddings\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            chromadb_path:str = chromadb_path,\n",
        "            train_data_path:str = train_data_path,\n",
        "            collection_name : str = 'contexts'\n",
        "        ) -> None:\n",
        "\n",
        "        self.client = chromadb.PersistentClient(path=chromadb_path)\n",
        "        self.training_data_path = train_data_path\n",
        "        self.collection_name = collection_name\n",
        "\n",
        "    def train(self, train:bool = False) -> None:\n",
        "        if train or not self.client.list_collections():\n",
        "\n",
        "            self.client.get_settings().allow_reset=True\n",
        "\n",
        "            self.client.reset()\n",
        "            print(\"All the collections has been removed\")\n",
        "\n",
        "            excel_data = pd.read_excel(self.training_data_path)\n",
        "            print(excel_data.shape)\n",
        "\n",
        "            print(f\"Starting training for {self.collection_name}\")\n",
        "\n",
        "            collection = self.client.create_collection(\n",
        "                name= self.collection_name,\n",
        "                embedding_function=self.MyEmbeddingFunction(),\n",
        "                metadata={\"hnsw:space\": \"cosine\"}\n",
        "                )\n",
        "            print(\"Collection has been created\")\n",
        "            collection.add(\n",
        "                documents=excel_data['chunks'].to_list(),\n",
        "                ids=excel_data.index.astype(str).to_list()\n",
        "\n",
        "                )\n",
        "            print(\"Data has been loaded succesfully\")\n",
        "\n",
        "\n",
        "\n",
        "    async def retrieve_chunks(\n",
        "            self,\n",
        "            user_message : str,\n",
        "            )->Tuple[str,Dict]:\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        try:\n",
        "            collection_name = self.collection_name\n",
        "\n",
        "            vectordb = self.client.get_collection(collection_name,embedding_function=self.MyEmbeddingFunction())\n",
        "            results = vectordb.query(query_texts = user_message, n_results = max_results)\n",
        "\n",
        "            chunks = results['documents'][0]\n",
        "            return chunks\n",
        "\n",
        "        except Exception as e:\n",
        "          error = await handle_exception(e)\n",
        "          return error"
      ],
      "metadata": {
        "id": "rNoktRpCMGpa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def generative_prompt(all_chunks : List[str],system :str) -> str:\n",
        "    \"\"\"This function will return final system_prompt for bot,which can be directly used without any modification.\n",
        "\n",
        "    Args:\n",
        "        all_chunks (List[str]): List of chunks retrieved from vector database\n",
        "\n",
        "    Returns:\n",
        "        str: Final System Bot Prompt\n",
        "    \"\"\"\n",
        "    if not isinstance(all_chunks,list):\n",
        "        raise ValueError(\"all_chunks must be a string\")\n",
        "\n",
        "    if len(all_chunks)>0:\n",
        "        knowledge_source = ''\n",
        "        for i,doc in enumerate(all_chunks):\n",
        "            knowledge_source += f\"content_{i}: {doc} \\n\\n\"\n",
        "    else:\n",
        "        knowledge_source = \"NO DATA\"\n",
        "\n",
        "    system_prompt = system.format(knowledge_source = knowledge_source)\n",
        "    return system_prompt"
      ],
      "metadata": {
        "id": "TwRsECg9poU4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPAND_QUERY_PROMPT = \"\"\"You are expert in contextual query rephraser for better similarity search retrieval for research paper content chatbot.\n",
        "\n",
        "Task\n",
        "- Break current query in different segments if user is looking for more topic.\n",
        "- Rephrase and complete the current query by generating alternative versions using previous conversation to make complete contextual query .\n",
        "- Alternate queries should be related to current query if it is not completely different.\n",
        "- The alternative queries should not change the actual semantic meaning of current user query.\n",
        "- Alternate queries should not be more than 2.\n",
        "\n",
        "Inputs\n",
        "prev_conversation:\n",
        "{prev_conv}\n",
        "\n",
        "current query: {query}\n",
        "\n",
        "Return a json response with a single key `rephrased_query` ,value as a list of generated alternate query as string-\n",
        "{{\n",
        "    \"rephrased_query\" :List[str] (list of alternative queries.)\n",
        "}}\n",
        "You can not return anything apart from List of generated queries which should be parsed by python.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "V_ekyWM7ESVQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def get_expanded_query(\n",
        "    user_message: str,\n",
        "    chat_history: List[Dict],\n",
        "    llm_handler : GenResponse\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Asynchronously generates an expanded query based on the user message and chat history.\n",
        "\n",
        "    It constructs a prompt using the provided parameters, sends it to the LLM, and processes the response to\n",
        "    generate an expanded query. If the LLM fails to generate a valid response, the original user\n",
        "    message is returned as the fallback.\n",
        "\n",
        "    Args:\n",
        "        user_message (str): The user's input message.\n",
        "        chat_history (List[Dict]): The history of the conversation as a list of dictionaries.\n",
        "\n",
        "    Returns:\n",
        "        str: The expanded query in the format ``Ques : expanded_query``. If the LLM fails or\n",
        "             returns an invalid response, the original user message is returned as the fallback.\n",
        "\n",
        "    Raises:\n",
        "        Any exceptions raised by the LLM or JSON parsing are caught and logged, but the function\n",
        "        does not raise them further. Instead, it returns an empty string or the original message.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Construct the prompt for query expansion\n",
        "\n",
        "        chats = ''\n",
        "        for chat in chat_history:\n",
        "          if chat['role'] == 'user':\n",
        "            chats += f\"user: {chat['content']}\\n\"\n",
        "          elif chat['role'] == 'assistant':\n",
        "            chats += f\"Bot: {chat['content']}\\n\"\n",
        "        prompt = EXPAND_QUERY_PROMPT.format(\n",
        "            prev_conv = chats,\n",
        "            query = user_message\n",
        "        )\n",
        "\n",
        "        # Call the LLM to generate the expanded query\n",
        "        response_text = await llm_handler.get_response(\n",
        "            system = \" \",\n",
        "            model_id = 'gemini-1.5-flash',\n",
        "            query = prompt,\n",
        "            chat_history = [],\n",
        "            temperature = .5,\n",
        "            max_tokens = 300,\n",
        "            response_format = {\"type\": \"json_object\"}\n",
        "        )\n",
        "        # Process the rephrased query\n",
        "        if not isinstance(response_text,Dict):\n",
        "          # Parse the LLM response\n",
        "          response_text = await llm_handler.get_json(response_text.choices[0].message.content)\n",
        "          response_text = response_text['rephrased_query']\n",
        "          print(response_text)\n",
        "          if isinstance(response_text, list):\n",
        "            # If the response is a non-empty list, use the first item\n",
        "            if response_text:\n",
        "                  user_mes_expanded = response_text[0]\n",
        "            else:\n",
        "                  user_mes_expanded = user_message\n",
        "          else:\n",
        "                # If the response is a string, use it directly\n",
        "                user_mes_expanded = response_text\n",
        "        else:\n",
        "            # If no rephrased query is returned, use the original message\n",
        "            user_mes_expanded = user_message\n",
        "\n",
        "    except Exception as e:\n",
        "      error = await handle_exception(e)\n",
        "      print(error)\n",
        "      # Fallback to an empty string if an exception occurs\n",
        "      user_mes_expanded = \"\"\n",
        "\n",
        "    print(\"Query Expanded:\\n\", user_mes_expanded)\n",
        "\n",
        "    return user_mes_expanded"
      ],
      "metadata": {
        "id": "xxbbewKPBgda"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# like <p>This is a statement with a reference<sup><a href=\"#ref1\">[1]</a></sup>.</p>\n",
        "bot_prompt = \"\"\"-Role--\n",
        "You are a question-answering chatbot for research paper. Your job is to answer user queries strictly using the content provided in the delimited by <ctx></ctx>). You cannot use any external knowledge or information outside the provided context.\n",
        "\n",
        "<ctx>\n",
        "{knowledge_source}\n",
        "</ctx>\n",
        "\n",
        "--Response--\n",
        "1. Always and only respond in HTML tag formatted which should render on website smoothly. Do not use markdown, plain text, or any non-HTML formatting. \\\n",
        "Use semantic HTML tags like <h1> to <h6> for headings, <p> for paragraphs, <ul> and <li> for lists, <strong> for bold, <em> for italics, \\\n",
        "<mark> for highlights, <code> for inline code, <pre> for code blocks, <a> for links, and <table> for tabular data. \\\n",
        "Ensure the HTML is well-formed, valid, and self-contained (no external CSS or JavaScript)\n",
        "2. Cite relevant chunks from the context using ^[Context_n] notation. For multiple citations, use ^[Context_1] ^[Context_2], etc.\n",
        "3. Respond in customizable response lengths:\n",
        "   - Concise\n",
        "   - Medium\n",
        "   - Detailed\n",
        "4. Respond in customizable Format:\n",
        "  - use <ul> and <li> for bullets points and <p> for paragraph\n",
        "5. For questions unrelated to the given context, return : \"Sorry I dont have information about this,Please ask related to your research paper only.\" or politely deny and guide the user back to the given context.\n",
        "\n",
        "--Task--\n",
        "1. Answer questions using only the content from the provided context. Do not rely on external knowledge.\n",
        "2. Keep responses concise, crisp, and to the point unless the user requests a detailed answer.\n",
        "3. Always cite relevant chunks from the context using ^[Context_n] notation.\n",
        "4. If the user asks for a specific format (bullet points or paragraphs), adhere to their request.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "InakAyxumvEc"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "async def retrieval_with_query_expansion(\n",
        "    user_message: str,\n",
        "    prev_conversation: List[Dict],\n",
        "    retrieval: RetrievalPipeline,\n",
        "    llm_handler : GenResponse\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Performs retrieval with query expansion to enhance search results.\n",
        "\n",
        "    This function generates an expanded query using the user's message and conversation history,\n",
        "    retrieves relevant chunks from a knowledge base using both the expanded and original queries,\n",
        "    and constructs a system prompt based on the retrieved chunks.\n",
        "\n",
        "    Args:\n",
        "        user_message (str): The user's input message.\n",
        "        prev_conversation (List[Dict]): The history of the conversation as a list of dictionaries.\n",
        "        retrieval (RetrievalPipeline): The retrieval pipeline used to fetch chunks.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - all_chunks (List[str]): A list of unique retrieved chunks, limited to a maximum of 5.\n",
        "            - expanded_query (str): The expanded query generated from the user message and chat history.\n",
        "            - system_prompt (str): The final system prompt constructed from the retrieved chunks.\n",
        "\n",
        "    Raises:\n",
        "        Exception: If an error occurs during retrieval, query expansion, or prompt generation.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Generate expanded query using the user message and conversation history\n",
        "        expanded_query = await get_expanded_query(\n",
        "            user_message=user_message,\n",
        "            chat_history=prev_conversation,\n",
        "            llm_handler = llm_handler\n",
        "        )\n",
        "\n",
        "        # Retrieve chunks using the expanded query if it is not empty\n",
        "        if len(expanded_query)>0:\n",
        "            retrieval_task1 = asyncio.create_task(\n",
        "                retrieval.retrieve_chunks(\n",
        "                    user_message=expanded_query,\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            retrieval_task1 = None\n",
        "\n",
        "        # Retrieve chunks using the original user message\n",
        "        retrieval_task2 = asyncio.create_task(\n",
        "            retrieval.retrieve_chunks(\n",
        "                user_message=user_message,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Gather results from both retrieval tasks\n",
        "        retrieval_results = await asyncio.gather(retrieval_task1, retrieval_task2)\n",
        "\n",
        "        # Process results from the expanded query retrieval\n",
        "        retrieved_chunks1 = []\n",
        "        if retrieval_task1:\n",
        "            semantic_search_result1 = retrieval_results[0]\n",
        "            if 'error' not in semantic_search_result1:\n",
        "                retrieved_chunks1 = semantic_search_result1\n",
        "\n",
        "        # Process results from the original query retrieval\n",
        "        semantic_search_result2 = retrieval_results[1]\n",
        "        if 'error' not in semantic_search_result2:\n",
        "            retrieved_chunks2 = semantic_search_result2\n",
        "        else:\n",
        "            retrieved_chunks2 = []\n",
        "\n",
        "        # Combine and deduplicate chunks from both retrievals\n",
        "        all_chunks = retrieved_chunks1.copy()\n",
        "        for chunk in retrieved_chunks2:\n",
        "            if chunk not in all_chunks:\n",
        "                all_chunks.append(chunk)\n",
        "\n",
        "        # Limit the number of chunks to 5\n",
        "        if len(all_chunks) > max_results:\n",
        "            all_chunks = all_chunks[:max_results]\n",
        "\n",
        "        # Generate the final system prompt using the retrieved chunks\n",
        "        system_prompt = await generative_prompt(all_chunks=all_chunks,system = bot_prompt)\n",
        "\n",
        "        return all_chunks, expanded_query, system_prompt\n",
        "\n",
        "    except Exception as e:\n",
        "        error = await handle_exception(e)\n",
        "        print(error)\n",
        "        return error, error, error"
      ],
      "metadata": {
        "id": "WN9TlpfprsUm"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunks\n",
        "   1. **Document based chunking** which will be beeter for research paper chunks to capture complete section content\n"
      ],
      "metadata": {
        "id": "y7T8meTgcYRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# !pip install -qU 'docling-core[chunking]' sentence-transformers transformers\n",
        "!pip install docling\n",
        "!pip install chonkie[all]"
      ],
      "metadata": {
        "id": "OMbTyxXez5lD"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from docling.document_converter import DocumentConverter\n",
        "\n",
        "DOC_SOURCE = \"/content/SLLM.pdf\"\n",
        "\n",
        "doc = DocumentConverter().convert(source=DOC_SOURCE).document\n",
        "text = doc.export_to_markdown()"
      ],
      "metadata": {
        "id": "vdt40Bpx96qK"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc.export_to_markdown())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARdZApr-rGEH",
        "outputId": "eeba6a9a-8006-4f49-e86f-b20ba26b727d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## WHISMA: A SPEECH-LLM TO PERFORM ZERO-SHOT SPOKEN LANGUAGE UNDERSTANDING\n",
            "\n",
            "Mohan Li, Cong-Thanh Do, Simon Keizer, Youmna Farag, Svetlana Stoyanchev, Rama Doddipatla\n",
            "\n",
            "Cambridge Research Laboratory, Toshiba Europe Ltd, Cambridge, UK\n",
            "\n",
            "## ABSTRACT\n",
            "\n",
            "Speech large language models (speech-LLMs) integrate speech and text-based foundation models to provide a unified framework for handling a wide range of downstream tasks. In this paper, we introduce WHISMA, a speech-LLM tailored for spoken language understanding (SLU) that demonstrates robust performance in various zero-shot settings. WHISMA combines the speech encoder from Whisper with the Llama-3 LLM, and is fine-tuned in a parameter-efficient manner on a comprehensive collection of SLU-related datasets. Our experiments show that WHISMA significantly improves the zero-shot slot filling performance on the SLURP benchmark, achieving a relative gain of 26.6% compared to the current state-of-the-art model. Furthermore, to evaluate WHISMA's generalisation capabilities to unseen domains, we develop a new task-agnostic benchmark named SLU-GLUE. The evaluation results indicate that WHISMA outperforms an existing speech-LLM (Qwen-Audio) with a relative gain of 33.0%.\n",
            "\n",
            "Index Terms -spoken language understanding, speech large language model, zero-shot learning\n",
            "\n",
            "## 1. INTRODUCTION\n",
            "\n",
            "Traditional speech processing techniques typically depend on specialised models tailored to individual tasks. These models, trained with limited data and constrained architectures, often face difficulties in generalising to new domains and applications. However, recent advancements in speech foundation models (SFMs) [1, 2, 3] and large language models (LLMs) [4, 5, 6] have fundamentally reshaped this approach, leading to the emergence of multi-functional end-to-end (E2E) speech-LLM systems [7, 8, 9, 10]. By utilising off-the-shelf SFMs and LLMs, the development of a speech-LLM system can be streamlined, often requiring only optimisation of adaptors and eliminating the need for extensive training data.\n",
            "\n",
            "Similar to their text-based counterparts, speech-LLMs perform speech tasks through instruction following. Each task is defined by a textual prompt, which the LLM decoder processes alongside the speech embeddings produced by the SFMencoder. Conditioned on the speech input, the LLM then generates a response to fulfill the given instruction. Leveraging the vast knowledge embedded in the LLM decoder,\n",
            "\n",
            "speech-LLMs demonstrate emergent abilities not explicitly imparted during training [8]. Additionally, the prompt-driven approach enables speech-LLMs to address various speech classification tasks in a zero-shot manner. In this paradigm, users maintain the flexibility to incorporate different class labels into the prompt, rather than relying on fixed classification heads as used in conventional methods. These attributes notably enhance the potential of speech-LLMs to serve as a universal solution in the speech industry.\n",
            "\n",
            "Previous studies have explored a wide range of speechcentric challenges within the speech-LLM framework. However, the evaluation of its spoken language understanding (SLU) capabilities, particularly in zero-shot scenarios, remains limited. While several existing speech-LLMs have showcased competence in common speech tasks like automatic speech recognition (ASR), speech-to-text translation (ST), and spoken question answering (SQA), their effectiveness in some key SLU tasks such as intent classification (IC) and slot filling (SF) has not been thoroughly examined.\n",
            "\n",
            "To address this gap, this paper introduces WHISMA, a speech-LLM system designed to enhance the zero-shot SLU performance across various domains. In this context, SLU refers to inferring semantics from spoken utterances. The proposed system employs Whisper [11] and Llama-3 [6] models as the speech encoder and text decoder, respectively. These components remain fixed during training and are connected by a trainable modality aligner. Low-rank adaptation (LoRA) [12] is implemented on Llama-3 to accommodate speech modality inputs. WHISMA is fine-tuned using approximately 2000 hours of speech data, covering tasks of ASR, IC, SF, SQA, and spoken (query) instruction tuning (SQIT/SIT). We adopt a training strategy that enables the system to perform an auxiliary ASR step before SLU through speech chain-of-thought (SCoT) [10] or multi-round (MR) inference, while maintaining the E2E property of WHISMA.\n",
            "\n",
            "To facilitate reproducing the proposed system, we ensure that all the training and test data used in this work is openly accessible. Specifically, we publish our self-developed Spoken-Alpaca dataset utilised for the SIT task. Derived from the text-based Alpaca instruction tuning corpus [13], we conduct pre-processing on the input and instruction fields, and synthesise speech for them using an in-house text-to-speech (TTS) model. Moreover, we curate a new SLU benchmark,\n",
            "\n",
            "named SLU-GLUE, to assess speech-LLMs on task-agnostic data beyond IC and SF. Tasks within SLU-GLUE include sentiment analysis (SA), semantic equivalence recognition (SER), and spoken-textual entailment recognition (STER). This benchmark is also made publicly available.\n",
            "\n",
            "The contributions of our work are summarised as follows:\n",
            "\n",
            "- · We introduce WHISMA, a speech-LLM that incorporates cutting-edge speech (Whisper) and language (Llama-3) foundation models to support a diverse range of SLU tasks in zero-shot settings.\n",
            "- · Through evaluations on the SLU tasks seen in training, WHISMA demonstrates superior zero-shot performance compared to existing baselines on the SLURP [14], FSC [15] and SmartLight [16] benchmarks.\n",
            "- · To illustrate the robustness of WHISMA on unseen SLU tasks, we further evaluate the system on the curated SLU-GLUE benchmark 1 , showing that it outperforms both the modular Whisper-Llama-3 system and an existing speech-LLM, Qwen-Audio [9].\n",
            "- · To mitigate over-fitting during WHISMA training, we develop the Spoken-Alpaca 2 dataset for speech-based instruction tuning, and show that it facilitates the model generalisation to unseen tasks.\n",
            "\n",
            "## 2. RELATED WORK\n",
            "\n",
            "Motivated by the success of vision language models (VLMs) [17, 18], there has been a growing interest in extending LLMs with auditory capabilities. Initial attempts to incorporate speech perception into LLMs involve LLM-based ASR models [19, 20]. Building upon this, Speech-Llama [21] integrates both ST and ASR to the framework through multi-task learning. Further, SLM [22] enhances the system with speech instruction tuning. However, these models typically undergo fine-tuning with a limited set of tasks and lack generalised understanding abilities.\n",
            "\n",
            "Recent advancements have introduced speech-LLMs with more diverse speech processing functions. Notable examples include BLSP [7], LLaSM [23], SALMONN [8], QwenAudio [9], and WavLLM [10]. Most of these systems utilise a variety of speech or audio resources to achieve cross-modal perception. Among them, zero-shot SLU evaluation results have been reported by BLSP for IC and SA tasks, SALMONN for a SF task, and WavLLM for a SQA task. In this work, we aim to conduct a comprehensive evaluation on these tasks using our proposed WHISMA system.\n",
            "\n",
            "UniverSLU [24] is the latest study that exclusively examines the SLU capabilities of the SFM, which fine-tunes the\n",
            "\n",
            "Fig. 1 . An overview of WHISMA model architecture.\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "Whisper model with up to 17 SLU datasets encompassing both acoustic and semantic domain tasks. The zero-shot performance of UniverSLU, as reported, reveals that the system struggles to generalise to unseen datasets and tasks, which is the issue we would like to address in this work.\n",
            "\n",
            "The most relevant research to this study is ZS-WhisperSLU proposed by [25], which investigates zero-shot IC and SF tasks within a QA-driven framework based on Whisper. Despite the promising performance achieved by the system, it is challenging to utilise ZS-Whisper-SLU for unseen tasks due to the limitation of the Whisper decoder. In this regard, we seek to bridge this gap by leveraging an LLM that possess broader knowledge than an SFM.\n",
            "\n",
            "## 3. METHOD\n",
            "\n",
            "In this section, we present the architecture of the proposed WHISMA system and describe the fine-tuning speech tasks along with their corresponding datasets. Additionally, we introduce the training strategy that integrates ASR auxiliary into SLU tasks to enhance the reliability of LLM inference, while preserving WHISMA's end-to-end (E2E) nature.\n",
            "\n",
            "## 3.1. Model architecture\n",
            "\n",
            "The model architecture of WHISMA is illustrated in Fig. 1. We adopt the encoder from Whisper-large-v2 , a 32layer Transformer model with two convolutional neural network (CNN) down-samplers, as the speech encoder in our system. The resulting speech embeddings are fed to a modality aligner, which converts these embeddings to align with the input space of the LLM decoder. Our modality aligner is structured similarly to a previous system [10], consisting\n",
            "\n",
            "Table 1 . Multi-task training dataset.\n",
            "\n",
            "| Task   | Data Source                              | #Hours   | #Samples   |\n",
            "|--------|------------------------------------------|----------|------------|\n",
            "| ASR    | GigaSpeech (M) [26]                      | 1000     | 910k       |\n",
            "| IC     | SLURP (zero-shot) [14]                   | 80       | 94k        |\n",
            "| SF     | SLURP (zero-shot) [14]                   | 80       | 94k        |\n",
            "| SQA    | LibriSQA-partI [27] LibriSQA-partII [27] | 360      | 104k 104k  |\n",
            "| SQIT   | Spoken-Alpaca                            | 360 30   | 29k        |\n",
            "| SIT    | Spoken-Alpaca                            | 14       | 14k        |\n",
            "\n",
            "of two CNN layers for further down-sampling, a bottleneck adaptor, and a linear output layer to match dimensions.\n",
            "\n",
            "WHISMA's decoder is implemented using Llama-38B-Instruct , one of the most promising open-sourced LLMs optimised for instruction following. This decoder processes both the aligned speech embeddings and a taskdescriptive text prompt, generating a response to execute the given instruction. We integrate low-rank adaptation (LoRA) weights into the attention module of all 32 Transformer layers in Llama-3, enabling it to effectively handle speech modality inputs. During training, only the modality aligner and LoRA parameters are optimised.\n",
            "\n",
            "## 3.2. Training data\n",
            "\n",
            "The fine-tuning of WHISMA is conducted with a multi-task learning approach. We compile a substantial training dataset that encompasses the following tasks: automatic speech recognition (ASR), intent classification (IC), slot filling (SF), spoken question answering (SQA), spoken query instruction tuning (SQIT), and spoken instruction tuning (SIT). Although ASR is not categorized as an SLU task, it significantly aids in aligning between the speech and text modalities. These tasks correspond to data from different sources, including the medium subset of GigaSpeech [26], the zero-shot data split of SLURP [14], LibriSQA-partI for open-ended QA, partII for multi-choice QA [27], and Spoken-Alpaca, developed as part of this research endeavor. The total duration of speech data amounts to approximately 2000 hours. Detailed information on this dataset is provided in Table 1.\n",
            "\n",
            "Although previous studies have employed Alpaca [13] with synthesised speech for instruction tuning [10, 22], the associated data remains unpublished, which hinders the replication of their findings. To remedy this, we introduce Spoken-Alpaca and release it for public access. The original text-based Alpaca dataset comprises two types of examples: i) containing fields { instruction, input, output } , where instruction describes the task, input provides context, and output is the expected system response; and ii) with fields\n",
            "\n",
            "{ instruction, output } , where the instruction stands alone as a query, not requiring contextual information. For the first type of examples, we generate speech for the input field while retaining instruction as the text prompt for the speechLLM system. This configuration defines the task referred to as spoken instruction tuning (SIT). Concerning the second type, speech synthesis is applied to instruction , which the system receives without a text prompt. This task category is denoted as spoken query instruction tuning (SQIT). We develop an in-house variational inference with adversarial learning for end-to-end text-to-speech (VITS) model [28] using the Librispeech [29] dataset featuring around 2200 speakers. Examples in Alpaca unsuitable for speech synthesis are filtered out, including those containing lengthy texts, mathematical equations, tables, etc. We also alter the wording of some instructions to make them speech-oriented. The diverse instructions within Spoken-Alpaca could significantly reduce the risk of over-fitting WHISMA to seen tasks with restricted prompt variations, such as ASR, IC and SF. Our experiments (Section 4.3) demonstrate that incorporating the dataset enables the proposed system to generalise more effectively to unseen tasks.\n",
            "\n",
            "## 3.3. Training strategy\n",
            "\n",
            "The training examples are organised according to Llama-3's standard prompt template, as outlined in Fig. 1. To enhance the robustness of WHISMA in handling diverse instructions during inference, we devise 10 distinct prompts for each task in ASR, IC, and SF. These prompts are randomly selected for each training example during fine-tuning. For the remaining tasks, we directly employ the provided question or instruction from the data as the text prompt.\n",
            "\n",
            "Unlike ZS-Whisper-SLU [25], which tackles IC and SF through prompting the text decoder with individual questions for each intent class or slot type, WHISMA performs the tasks more efficiently by amalgamating candidate labels into a single prompt. Example prompts are: 'Classify the intent of the spoken utterance into one of the following labels: [intent 1], [intent 2], ..., [intent N]' for IC, and 'Perform slot filling on the spoken utterance for the following slots: [slot 1], [slot 2], ..., [slot M]' for SF. During training, we do not include all the candidate labels into the prompts but instead sample a varying number of them (including the ground-truth label) for each example. This approach introduces diversity to the prompts and helps prevent over-fitting.\n",
            "\n",
            "The primary challenge of E2E SLU lies in the direct extraction of semantic elements from speech, particularly for the SF task, which requires identifying entities explicitly mentioned in the user utterances. Previous studies indicate that ASR can significantly improve SLU performance [30]. We investigate this method for our proposed model. To distinguish WHISMA from modular ASR-LLM systems, speech transcription is performed as a part of the model inference,\n",
            "\n",
            "Table 2 . SLU-GLUE benchmark.\n",
            "\n",
            "| Task   |         |                                                                         |      |\n",
            "|--------|---------|-------------------------------------------------------------------------|------|\n",
            "|        | SST-2   | Classify the sentiment of [SPEECH] into positive or negative.           | 2790 |\n",
            "| SER    | QQP     | Identify if the question in is a paraphrase of the question in [TEXT] . |      |\n",
            "|        |         | Identify if the context in                                              |      |\n",
            "|        |         | Identify if the sentence in [SPEECH] entails the sentence in [TEXT] .   | 2088 |\n",
            "|        | SciTail | Identify if the premise in [SPEECH] supports the hypothesis in [TEXT] . | 2736 |\n",
            "\n",
            "Fig. 2 . E2E Inference strategies integrating ASR to SLU.\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "maintaining its E2E decoding property. We explore two strategies to integrate an auxiliary ASR step into SLU tasks:\n",
            "\n",
            "- · Speech chain-of-thought (SCoT) [10]: The system processes a concatenation of the ASR and SLU prompts, executing the tasks one-shot in sequential order. During inference, the SLU response is conditioned on the previously generated speech transcript.\n",
            "- · Multi-round (MR): Inference is conducted in multiple dialogue rounds. The system is first instructed to produce a speech transcript, then based on the speech embeddings and the transcript (in the dialogue history), we prompt the system to execute the SLU task.\n",
            "\n",
            "An illustration of the above inference strategies is depicted in Fig. 2. In SLU tasks (excluding SQIT), each example is randomly assigned to one of the following training configurations: SLU-alone, SCoT-ASR-SLU, and MR-ASR-SLU. Despite the fact that LLMs are inherently capable of executing CoT and MR inference, our observation suggests that speechLLMs do not develop such capabilities unless integrated into the fine-tuning process.\n",
            "\n",
            "## 4. EXPERIMENTS\n",
            "\n",
            "## 4.1. Experimental setup\n",
            "\n",
            "The speech encoder and text decoder of WHISMA are based on Whisper-large-v2 and Llama-3-8B-Instruct models, respectively. The modality aligner adopts two 1-D CNN layers with a stride of 2, down-sampling the speech by a factor of 8 along with the Whisper encoder. This configuration produces 375 speech embeddings per input. The bottleneck dimension of the adaptor is set to 320. Within Llama-3, a light-weight LoRA scheme with a rank of 8 and an alpha of 16 is implemented. WHISMA is fine-tuned on 6 V6000 GPUs using a batch size of 12. To prevent over-fitting, the system undergoes only one epoch of training, using the AdamW optimiser with a constant learning rate of 0.0001.\n",
            "\n",
            "## 4.2. Evaluation tasks\n",
            "\n",
            "We evaluate the proposed WHISMA model under three levels of zero-shot settings, ranging from easy to hard:\n",
            "\n",
            "- · seen-task-seen-corpus (STSC) , targeting the SF task using the zero-shot split of the SLURP dataset [14].\n",
            "- · seen-task-unseen-corpus (STUC) , focusing on IC and SF tasks within the test sets from FSC [15] and SmartLight [16] benchmarks.\n",
            "- · unseen-task-unseen-corpus (UTUC) , representing the most challenging scenario, utilising a new SLU benchmark named SLU-GLUE.\n",
            "\n",
            "For the STSC and STUC evaluations, we focus on data within the domain of in-home robot assistant. The same setups as\n",
            "\n",
            "Table 3 . STSC zero-shot evaluation. WER (%) and SF SLU-F1 (%) on the zero-shot test set of SLURP.\n",
            "\n",
            "| Model               | WER ↓   | SLU-F1 ↑   |\n",
            "|---------------------|---------|------------|\n",
            "| supervised          |         |            |\n",
            "| SNLU [32]           | 13.6    | 69.9       |\n",
            "| zero-shot           |         |            |\n",
            "| ZS-Whisper-SLU [25] | 8.3     | 50.0       |\n",
            "| WHISMA (proposed)   | -       | 46.8       |\n",
            "| + SCoT              | 11.9    | 63.1       |\n",
            "| + MR                | 13.8    | 63.3       |\n",
            "\n",
            "described in [25] are followed to structure the test data. In the case of SLURP, a zero-shot data split is created by holding out 5 slot types: { podcast name, artist name, audiobook name, business name, radio name } , which results in a test set containing 18k utterances. Regarding FSC, the original { action, object, location } labels are reorganised into 15 intents and 2 slots. As for SmartLight, we employ the full close-field subset as the test set, comprising 6 intents and 3 slot types.\n",
            "\n",
            "In addition to the task-oriented datasets mentioned above, we curate SLU-GLUE for the UTUC evaluation in our experiment. Derived from ASR-GLUE [31], SLU-GLUE comprises three primary tasks: sentiment analysis (SA), semantic equivalence recognition (SER), and spoken-textual entailment recognition (STER). These tasks encompass five sub-tasks, namely: Stanford Sentiment Treebank (SST-2), Quora Question Pairs (QQP), Question-answering NLI (QNLI), Recognizing Textual Entailment (RTE), and SciTail. We exclude Semantic Textual Similarity Benchmark (STS-B) as it is not suitable for zero-shot evaluation. The evaluation metric for all the tasks is binary accuracy. The data domains of SLUGLUE span from everyday language, such as movie reviews, to specialised scientific terminology. Speech samples in the dataset are recorded by six native speakers and are mixed with various levels of noise. Further detailed information of SLU-GLUE is provided in Table. 2.\n",
            "\n",
            "The performance of WHISMA is compared against multiple robust baselines, including state-of-the-art (SOTA) supervised models, SOTA zero-shot models, the modular ASRLLM counterpart to WHISMA, and a well-established opensourced speech-LLM called Qwen-Audio [9].\n",
            "\n",
            "## 4.3. Main results\n",
            "\n",
            "The STSC evaluation results on the zero-shot test set of SLURP are presented in Table 3, focusing on ASR worderror-rate (WER) and SF SLU-F1 score. To ensure a fair com-\n",
            "\n",
            "Table 4 . STUC zero-shot evaluation. WER (%) and IC accuracy (IC Acc. %) on the test set of FSC.Table 5 . STUC zero-shot evaluation. WER (%), IC accuracy (IC Acc. %), SF SLU-F1 (%), and perfect parsing (PP, %) on the full set of SmartLight.\n",
            "\n",
            "| Model                     | WER ↓   | IC Acc. ↑   |\n",
            "|---------------------------|---------|-------------|\n",
            "| supervised                |         |             |\n",
            "| Finstreder [33]           | -       | 99.7        |\n",
            "| zero-shot                 |         |             |\n",
            "| BLSP [7]                  | -       | 77.5        |\n",
            "| ZS-Whisper-SLU [25]       | 0.8     | 95.0        |\n",
            "| Whisper-Llama-3 (modular) | 1.5     | 89.3        |\n",
            "| WHISMA (proposed)         | -       | 90.9        |\n",
            "| + SCoT                    | 1.7     | 97.3        |\n",
            "| + MR                      | 2.7     | 97.3        |\n",
            "\n",
            "| Model                     | WER ↓   | IC Acc. ↑   | SLU-F1 ↑   | PP ↑   |\n",
            "|---------------------------|---------|-------------|------------|--------|\n",
            "| supervised                |         |             |            |        |\n",
            "| Finstreder [33]           | 6.1     | -           | -          | 88.0   |\n",
            "| Whisper-TS [24]           | -       | 96.3        | -          | -      |\n",
            "| zero-shot                 |         |             |            |        |\n",
            "| BLSP [7]                  | -       | 78.8        | -          | -      |\n",
            "| ZS-Whisper-SLU [25]       | 2.7     | 91.6        | 90.9       | 82.5   |\n",
            "| UniverSLU-14 [24]         | -       | 44.6        | -          | -      |\n",
            "| Whisper-Llama-3 (modular) | 4.0     | 94.3        | 82.4       | 75.7   |\n",
            "| WHISMA (proposed)         | -       | 81.7        | 87.8       | 68.9   |\n",
            "| + SCoT                    | 4.5     | 95.9        | 90.2       | 82.5   |\n",
            "| + MR                      | 5.5     | 94.4        | 90.7       | 81.6   |\n",
            "\n",
            "parison, the modular Whisper-Llama-3 system is excluded as it has not been tuned on the SLURP dataset. For supervised model performance, we re-implement the spoken and natural language understanding (SNLU) system described in [32] and conduct the same evaluation. Given the rich knowledge embedded in Llama-3 and the wide array of SLU datasets used in training, our proposed WHISMA system with either SCoT or MR inference significantly outperforms the existing SOTA model, ZS-Whisper-SLU, achieving a 26.6% relative gain in SLU-F1. This comparison is equitable as ZS-whisper-SLU also conducts ASR prior to SF. The SCoT inference strategy achieves a lower WER compared to MR (11.9% vs. 13.8%), which we believe is because SCoT combines both tasks into a\n",
            "\n",
            "Table 6 . UTUC zero-shot evaluation. Accuracy (%) on SLU-GLUE sub-tasks.\n",
            "\n",
            "| Model                     |   SST-2 ↑ |   QQP ↑ |   QNLI ↑ |   RTE ↑ |   SciTail ↑ |   Avg. ↑ |\n",
            "|---------------------------|-----------|---------|----------|---------|-------------|----------|\n",
            "| Qwen-Audio [9]            |      53.4 |    62.4 |     58.8 |    65   |        54.5 |     58.8 |\n",
            "| + MR                      |      52.9 |    68.4 |     52.4 |    67.2 |        56.3 |     59.4 |\n",
            "| Whisper-Llama-3 (modular) |      88.3 |    74.9 |     81.6 |    76.7 |        68.3 |     78   |\n",
            "| WHISMA (proposed)         |      79   |    65.1 |     84   |    72.8 |        60.5 |     72.3 |\n",
            "| + SCoT                    |      88   |    65.1 |     78.9 |    73.7 |        71.7 |     75.5 |\n",
            "| + MR                      |      89.2 |    74.2 |     88   |    78   |        65.4 |     79   |\n",
            "| WHISMA w/o Spoken-Alpaca  |      78.6 |    45.9 |     50.9 |    61.2 |        52.6 |     57.8 |\n",
            "| + SCoT                    |      87.6 |    69   |     64.8 |    74.6 |        66.2 |     72.4 |\n",
            "| + MR                      |      86.6 |    52.6 |     73.8 |    70   |        57   |     68   |\n",
            "\n",
            "single prompt, enhancing ASR with the information in the SF instruction. WHISMA exhibits competitive SF performance (46.8%) to ZS-Whisper-SLU even without utilising speech transcripts. However, it lags behind cases with SCoT or MR inference, which emphasises the importance of performing ASR for SF tasks.\n",
            "\n",
            "The results of STUC evaluations on the FSC and SmartLight benchmarks are provided in Table 4 and 5, respectively. In the FSC benchmark, WHISMA+SCoT/MR obtains superior zero-shot performance compared to both the BLSP and ZS-Whisper-SLU baselines, showing relative improvements of 25.5% and 2.4% in IC accuracy. For the SmartLight benchmark, WHISMA+SCoT achieves SOTA performance in IC accuracy (95.9%) compared to all the zero-shot baselines. It also delivers SF SLU-F1 (90.2%) and perfect parsing (PP, 82.5%) results comparable to ZS-Whisper-SLU. Similar performance is observed with MR inference. On both benchmarks, WHISMA without ASR still surpasses the modular Whisper-Llama-3 model in various scenarios, highlighting the reduced complexity of E2E speech-LLMs over the modular approach. Regarding the SCoT and MR inferences, due to the limited ASR data used in fine-tuning, WHISMA generally displays higher WER relative to ZS-Whisper-SLU and Whisper-Llama-3. However, the joint modelling of speech and text modalities in WHISMA+SCoT/MR compensates for this shortcoming, leading to improved SLU performance.\n",
            "\n",
            "Table 6 presents the UTUC evaluation results on the SLU-GLUE benchmark. Alongside the modular system, we include Qwen-Audio, the most comprehensive open-sourced speech-LLM (available at the time of this study), for comparison. Qwen-Audio serves as a relevant baseline to WHISMA, since its training data includes SLU tasks similar to those used in our work. Furthermore, the system supports MR inference. As shown in the table, WHISMA demonstrates promising generalisation capabilities to unseen tasks during training. Specifically, WHISMA+MR achieves the highest performance among all the speech-LLMs, with an averaged\n",
            "\n",
            "accuracy of 79.0% across the five sub-tasks. This represents relative gains of 1.3% and 33.0% over Whisper-Llama-3 and Qwen-Audio+MR, respectively. Consistent with the observations in STSC and STUC settings, incorporating ASR into SLU substantially enhances WHISMA's performance on the challenging SLU-GLUE data, resulting in a relative gain of 9.3% when MR inference is performed. In contrast, QwenAudio+MR only provides minimal improvements over the SLU-alone inference fashion.\n",
            "\n",
            "In Table 6, we further illustrate the impact of utilising the Spoken-Alpaca dataset into the fine-tuning process of WHISMA. One can see that excluding this dataset results in a notable performance decline for WHISMA, with a 20.1% relative decrease in averaged accuracy when ASR is omitted. Similar degradation is observed with SCoT and MR inference, suffering relative reductions of 4.1% and 13.9% in averaged accuracy, respectively. Therefore, although SpokenAlpaca contributes only a small fraction (2.2% of total duration) of the entire training data, it plays a crucial role in strengthening the generalisability of speech-LLMs.\n",
            "\n",
            "## 5. CONCLUSION\n",
            "\n",
            "In this paper, we present WHISMA, a speech-LLM system that excels in various zero-shot SLU tasks. WHISMA integrates a Whisper-based speech encoder with a Llama-3 text decoder, and is fine-tuned on a diverse set of SLU tasks using a modality aligner and LoRA adaptors. Additionally, we enable the system to perform an auxiliary ASR step before SLU through SCoT or MR inference strategies. Comprehensive zero-shot evaluations demonstrate WHISMA's ability to achieve SOTA performance on several common SLU benchmarks and, more importantly, to generalise to tasks not encountered during training. To ensure the reproducibility of our results, we release the Spoken-Alpaca and SLU-GLUE datasets utilised in our experiments for public access.\n",
            "\n",
            "## 6. REFERENCES\n",
            "\n",
            "- [1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli, 'wav2vec 2.0: A framework for self-supervised learning of speech representations,' Advances in neural information processing systems , vol. 33, pp. 12449-12460, 2020.\n",
            "- [2] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed, 'Hubert: Self-supervised speech representation learning by masked prediction of hidden units,' IEEE/ACM Transactions on Audio, Speech, and Language Processing , vol. 29, pp. 3451-3460, 2021.\n",
            "- [3] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al., 'WavLM: Large-scale self-supervised pre-training for full stack speech processing,' IEEE Journal of Selected Topics in Signal Processing , vol. 16, no. 6, pp. 1505-1518, 2022.\n",
            "- [4] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al., 'Mistral 7B,' arXiv preprint arXiv:2310.06825 , 2023.\n",
            "- [5] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al., 'GPT-4 technical report,' arXiv preprint arXiv:2303.08774 , 2023.\n",
            "- [6] AI@Meta, 'Llama 3 model card,' 2024.\n",
            "- [7] Chen Wang, Minpeng Liao, Zhongqiang Huang, Jinliang Lu, Junhong Wu, Yuchen Liu, Chengqing Zong, and Jiajun Zhang, 'BLSP: Bootstrapping languagespeech pre-training via behavior alignment of continuation writing,' arXiv preprint arXiv:2309.00916 , 2023.\n",
            "- [8] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, and Chao Zhang, 'SALMONN: Towards generic hearing abilities for large language models,' in The Twelfth International Conference on Learning Representations , 2024.\n",
            "- [9] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou, 'Qwen-Audio: Advancing universal audio understanding via unified large-scale audio-language models,' arXiv preprint arXiv:2311.07919 , 2023.\n",
            "- [10] Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, Linquan Liu, et al., 'WavLLM: Towards\n",
            "- robust and adaptive speech large language model,' arXiv preprint arXiv:2404.00656 , 2024.\n",
            "- [11] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever, 'Robust speech recognition via large-scale weak supervision,' in International conference on machine learning . PMLR, 2023, pp. 28492-28518.\n",
            "- [12] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen, 'LoRA: Low-rank adaptation of large language models,' in International Conference on Learning Representations , 2022.\n",
            "- [13] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto, 'Stanford alpaca: An instruction-following llama model,' https://gith ub.com/tatsu-lab/stanford\\_alpaca , 2023.\n",
            "- [14] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and Verena Rieser, 'SLURP: A spoken language understanding resource package,' in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , Online, Nov. 2020, pp. 7252-7262, Association for Computational Linguistics.\n",
            "- [15] Loren Lugosch, Mirco Ravanelli, Patrick Ignoto, Vikrant Singh Tomar, and Yoshua Bengio, 'Speech model pre-training for end-to-end spoken language understanding,' arXiv preprint arXiv:1904.03670 , 2019.\n",
            "- [16] Alaa Saade, Joseph Dureau, David Leroy, Francesco Caltagirone, Alice Coucke, Adrien Ball, Cl'ement Doumouro, Thibaut Lavril, Alexandre Caulier, Th'eodore Bluche, et al., 'Spoken language understanding on the edge,' in 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS) . IEEE, 2019, pp. 57-61.\n",
            "- [17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi, 'BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,' in International conference on machine learning . PMLR, 2023, pp. 19730-19742.\n",
            "- [18] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee, 'Improved baselines with visual instruction tuning,' 2023.\n",
            "- [19] Yuang Li, Yu Wu, Jinyu Li, and Shujie Liu, 'Prompting large language models for zero-shot domain adaptation in speech recognition,' in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1-8.\n",
            "- [20] Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, et al., 'Prompting large language models with speech recognition abilities,' in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2024, pp. 13351-13355.\n",
            "- [21] Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, et al., 'On decoder-only architecture for speech-to-text and large language model integration,' in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1-8.\n",
            "- [22] Mingqiu Wang, Wei Han, Izhak Shafran, Zelin Wu, Chung-Cheng Chiu, Yuan Cao, Nanxin Chen, Yu Zhang, Hagen Soltau, Paul K Rubenstein, et al., 'SLM: Bridge the thin gap between speech and text foundation models,' in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1-8.\n",
            "- [23] Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, and Yemin Shi, 'LLaSM: Large language and speech model,' arXiv preprint arXiv:2308.15930 , 2023.\n",
            "- [24] Siddhant Arora, Hayato Futami, Jee-weon Jung, Yifan Peng, Roshan Sharma, Yosuke Kashiwagi, Emiru Tsunoo, Karen Livescu, and Shinji Watanabe, 'UniverSLU: Universal spoken language understanding for diverse tasks with natural language instructions,' in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , 2024, pp. 2754-2774.\n",
            "- [25] Mohan Li, Simon Keizer, and Rama Doddipatla, 'Prompting whisper for qa-driven zero-shot end-toend spoken language understanding,' arXiv preprint arXiv:2406.15209 , 2024.\n",
            "- [26] Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, et al., 'GigaSpeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio,' arXiv preprint arXiv:2106.06909 , 2021.\n",
            "- [27] Zihan Zhao, Yiyang Jiang, Heyang Liu, Yanfeng Wang, and Yu Wang, 'Librisqa: Pioneering freeform and open-ended spoken question answering with a novel dataset and framework,' arXiv preprint arXiv:2308.10390 , 2023.\n",
            "- [28] Jaehyeon Kim, Jungil Kong, and Juhee Son, 'Conditional variational autoencoder with adversarial learning\n",
            "- for end-to-end text-to-speech,' in International Conference on Machine Learning . PMLR, 2021, pp. 55305540.\n",
            "- [29] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, 'Librispeech: an ASR corpus based on public domain audio books,' in 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP) . IEEE, 2015, pp. 5206-5210.\n",
            "- [30] Mohan Li and Rama Doddipatla, 'Non-autoregressive end-to-end approaches for joint automatic speech recognition and spoken language understanding,' in 2022 IEEE Spoken Language Technology Workshop (SLT) . IEEE, 2023, pp. 390-397.\n",
            "- [31] Lingyun Feng, Jianwei Yu, Deng Cai, Songxiang Liu, Haitao Zheng, and Yan Wang, 'ASR-GLUE: A new multi-task benchmark for asr-robust natural language understanding,' arXiv preprint arXiv:2108.13048 , 2021.\n",
            "- [32] Mohan Li, C˘at˘alin Zoril˘a, Cong-Thanh Do, and Rama Doddipatla, 'Towards a unified end-to-end language understanding system for speech and text inputs,' in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1-8.\n",
            "- [33] Daniel Bermuth, Alexander Poeppel, and Wolfgang Reif, 'Finstreder: Simple and fast spoken language understanding with finite state transducers using modern speech-to-text models,' arXiv preprint arXiv:2206.14589 , 2022.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = doc.export_to_markdown()"
      ],
      "metadata": {
        "id": "rLKl2bhK7rpb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from chonkie import SemanticChunker\n",
        "\n",
        "# # Basic initialization with default parameters\n",
        "# chunker = SemanticChunker(\n",
        "#     embedding_model=\"minishlab/potion-base-8M\",  # Default model\n",
        "#     threshold=0.5,                               # Similarity threshold (0-1) or (1-100) or \"auto\"\n",
        "#     chunk_size=512,                              # Maximum tokens per chunk\n",
        "#     min_sentences=1                              # Initial sentences per chunk\n",
        "# )\n",
        "# chunks = chunker.chunk(text)\n",
        "\n",
        "# for chunk in chunks:\n",
        "#     print(f\"Chunk text: {chunk.text}\")\n",
        "#     print(f\"Token count: {chunk.token_count}\")\n",
        "#     print(f\"Number of sentences: {len(chunk.sentences)}\")"
      ],
      "metadata": {
        "id": "VTgycs3u_Rv_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chonkie import LateChunker\n",
        "import pandas as pd\n",
        "\n",
        "def process_and_save_chunks(text: str, df_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Processes the input text using LateChunker, saves the chunks into a DataFrame,\n",
        "    and exports the DataFrame to an Excel file.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be processed.\n",
        "        df_path (str): The file path where the DataFrame will be saved as an Excel file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the chunks in the 'chunks' column.\n",
        "\n",
        "    Raises:\n",
        "        Exception: If an error occurs during processing or saving.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize the LateChunker\n",
        "        chunker = LateChunker(\n",
        "            embedding_model=\"all-MiniLM-L6-v2\",\n",
        "            mode=\"sentence\",\n",
        "            chunk_size=512,\n",
        "            min_sentences_per_chunk=1,\n",
        "            min_characters_per_sentence=12,\n",
        "            delim=['\\\\n', '##']\n",
        "        )\n",
        "\n",
        "        # Generate chunks\n",
        "        chunks = chunker(text)\n",
        "\n",
        "        # Create a DataFrame\n",
        "        df = pd.DataFrame({\"chunks\": chunks})\n",
        "\n",
        "        # Save the DataFrame\n",
        "        df.to_excel(df_path, index=False)\n",
        "\n",
        "        print(f\"DataFrame saved successfully at {df_path}\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        error = handle_exception(e)\n",
        "        print(error)\n",
        "        return error\n",
        "process_and_save_chunks(text, \"train_data.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "VIYTakc37bvL",
        "outputId": "3548d282-bd1b-49c6-e84b-e9ffb274474a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (307 > 256). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame saved successfully at train_data.xlsx\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               chunks\n",
              "0   (#, #,  , W, H, I, S, M, A, :,  , A,  , S, P, ...\n",
              "1   ( , 1, .,  , I, N, T, R, O, D, U, C, T, I, O, ...\n",
              "2   ( , 2, .,  , R, E, L, A, T, E, D,  , W, O, R, ...\n",
              "3   ( , 3, .,  , M, E, T, H, O, D, \\n, \\n, I, n,  ...\n",
              "4   ( , 3, ., 1, .,  , M, o, d, e, l,  , a, r, c, ...\n",
              "5   ( , 3, ., 2, .,  , T, r, a, i, n, i, n, g,  , ...\n",
              "6   ( , 3, ., 3, .,  , T, r, a, i, n, i, n, g,  , ...\n",
              "7   ( , 4, .,  , E, X, P, E, R, I, M, E, N, T, S, ...\n",
              "8   ( , 4, ., 2, .,  , E, v, a, l, u, a, t, i, o, ...\n",
              "9   ( , 4, ., 3, .,  , M, a, i, n,  , r, e, s, u, ...\n",
              "10  ( , 5, .,  , C, O, N, C, L, U, S, I, O, N, \\n,...\n",
              "11  ( , 6, .,  , R, E, F, E, R, E, N, C, E, S, \\n,..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0e129c80-6d9f-48aa-a1c0-9bce456bab7c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chunks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(#, #,  , W, H, I, S, M, A, :,  , A,  , S, P, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>( , 1, .,  , I, N, T, R, O, D, U, C, T, I, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>( , 2, .,  , R, E, L, A, T, E, D,  , W, O, R, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>( , 3, .,  , M, E, T, H, O, D, \\n, \\n, I, n,  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>( , 3, ., 1, .,  , M, o, d, e, l,  , a, r, c, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>( , 3, ., 2, .,  , T, r, a, i, n, i, n, g,  , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>( , 3, ., 3, .,  , T, r, a, i, n, i, n, g,  , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>( , 4, .,  , E, X, P, E, R, I, M, E, N, T, S, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>( , 4, ., 2, .,  , E, v, a, l, u, a, t, i, o, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>( , 4, ., 3, .,  , M, a, i, n,  , r, e, s, u, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>( , 5, .,  , C, O, N, C, L, U, S, I, O, N, \\n,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>( , 6, .,  , R, E, F, E, R, E, N, C, E, S, \\n,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e129c80-6d9f-48aa-a1c0-9bce456bab7c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0e129c80-6d9f-48aa-a1c0-9bce456bab7c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0e129c80-6d9f-48aa-a1c0-9bce456bab7c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3225983d-02e5-4bd1-8188-22bf363b8113\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3225983d-02e5-4bd1-8188-22bf363b8113')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3225983d-02e5-4bd1-8188-22bf363b8113 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"process_and_save_chunks(text, \\\"train_data\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"chunks\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation"
      ],
      "metadata": {
        "id": "Sr6DgqV9wX4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenResponse(api_key=GEMINI_KEY,base_url = GEMINI_BASE_URL)"
      ],
      "metadata": {
        "id": "xAryAjkjddmG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval = RetrievalPipeline(\n",
        "    chromadb_path = 'chromadb',\n",
        "    train_data_path = 'train_data.xlsx',\n",
        "    collection_name = 'contexts'\n",
        ")\n",
        "\n",
        "retrieval.train(train = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "ea7lRtYKCtmV",
        "outputId": "8c6ce8e8-b354-4264-a6a4-6fe453b5e688"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All the collections has been removed\n",
            "(12, 1)\n",
            "Starting training for contexts\n",
            "Collection has been created\n",
            "Data has been loaded succesfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await get_expanded_query(\n",
        "    user_message = 'what is tabular chain of thought',\n",
        "    chat_history = [],\n",
        "    llm_handler = model\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "gNihdT4AZvvg",
        "outputId": "56434bc2-5c75-40ae-c137-8d02c3f6fd16"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Explain the concept of tabular chain of thought reasoning.', 'What are the characteristics and applications of tabular chain of thought?']\n",
            "Query Expanded:\n",
            " Explain the concept of tabular chain of thought reasoning.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Explain the concept of tabular chain of thought reasoning.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks,rephrased_query,system = await retrieval_with_query_expansion(\n",
        "    user_message = 'what is tabular chain of thought',\n",
        "    prev_conversation = [],\n",
        "    retrieval = retrieval,\n",
        "    llm_handler = model\n",
        ")\n",
        "print(system)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F_pHYBMWlt71",
        "outputId": "37ef1537-2205-41f5-8136-4f0a6ada6dce"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Explain the concept of tabular chain of thought reasoning.', 'What are tabular chain of thought methods and how do they work?']\n",
            "Query Expanded:\n",
            " Explain the concept of tabular chain of thought reasoning.\n",
            "-Role--\n",
            "You are a question-answering chatbot for research paper. Your job is to answer user queries strictly using the content provided in the delimited by <ctx></ctx>). You cannot use any external knowledge or information outside the provided context.\n",
            "\n",
            "<ctx>\n",
            "content_0:  3.3. Training strategy\n",
            "\n",
            "The training examples are organised according to Llama-3's standard prompt template, as outlined in Fig. 1. To enhance the robustness of WHISMA in handling diverse instructions during inference, we devise 10 distinct prompts for each task in ASR, IC, and SF. These prompts are randomly selected for each training example during fine-tuning. For the remaining tasks, we directly employ the provided question or instruction from the data as the text prompt.\n",
            "\n",
            "Unlike ZS-Whisper-SLU [25], which tackles IC and SF through prompting the text decoder with individual questions for each intent class or slot type, WHISMA performs the tasks more efficiently by amalgamating candidate labels into a single prompt. Example prompts are: 'Classify the intent of the spoken utterance into one of the following labels: [intent 1], [intent 2], ..., [intent N]' for IC, and 'Perform slot filling on the spoken utterance for the following slots: [slot 1], [slot 2], ..., [slot M]' for SF. During training, we do not include all the candidate labels into the prompts but instead sample a varying number of them (including the ground-truth label) for each example. This approach introduces diversity to the prompts and helps prevent over-fitting.\n",
            "\n",
            "The primary challenge of E2E SLU lies in the direct extraction of semantic elements from speech, particularly for the SF task, which requires identifying entities explicitly mentioned in the user utterances. Previous studies indicate that ASR can significantly improve SLU performance [30]. We investigate this method for our proposed model. To distinguish WHISMA from modular ASR-LLM systems, speech transcription is performed as a part of the model inference,\n",
            "\n",
            "Table 2 . SLU-GLUE benchmark.\n",
            "\n",
            "| Task   |         |                                                                         |      |\n",
            "|--------|---------|-------------------------------------------------------------------------|------|\n",
            "|        | SST-2   | Classify the sentiment of [SPEECH] into positive or negative.           | 2790 |\n",
            "| SER    | QQP     | Identify if the question in is a paraphrase of the question in [TEXT] . |      |\n",
            "|        |         | Identify if the context in                                              |      |\n",
            "|        |         | Identify if the sentence in [SPEECH] entails the sentence in [TEXT] .   | 2088 |\n",
            "|        | SciTail | Identify if the premise in [SPEECH] supports the hypothesis in [TEXT] . | 2736 |\n",
            "\n",
            "Fig. 2 . E2E Inference strategies integrating ASR to SLU.\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "maintaining its E2E decoding property. We explore two strategies to integrate an auxiliary ASR step into SLU tasks:\n",
            "\n",
            "- · Speech chain-of-thought (SCoT) [10]: The system processes a concatenation of the ASR and SLU prompts, executing the tasks one-shot in sequential order. During inference, the SLU response is conditioned on the previously generated speech transcript.\n",
            "- · Multi-round (MR): Inference is conducted in multiple dialogue rounds. The system is first instructed to produce a speech transcript, then based on the speech embeddings and the transcript (in the dialogue history), we prompt the system to execute the SLU task.\n",
            "\n",
            "An illustration of the above inference strategies is depicted in Fig. 2. In SLU tasks (excluding SQIT), each example is randomly assigned to one of the following training configurations: SLU-alone, SCoT-ASR-SLU, and MR-ASR-SLU. Despite the fact that LLMs are inherently capable of executing CoT and MR inference, our observation suggests that speechLLMs do not develop such capabilities unless integrated into the fine-tuning process.\n",
            "\n",
            "## \n",
            "\n",
            "content_1:  1. INTRODUCTION\n",
            "\n",
            "Traditional speech processing techniques typically depend on specialised models tailored to individual tasks. These models, trained with limited data and constrained architectures, often face difficulties in generalising to new domains and applications. However, recent advancements in speech foundation models (SFMs) [1, 2, 3] and large language models (LLMs) [4, 5, 6] have fundamentally reshaped this approach, leading to the emergence of multi-functional end-to-end (E2E) speech-LLM systems [7, 8, 9, 10]. By utilising off-the-shelf SFMs and LLMs, the development of a speech-LLM system can be streamlined, often requiring only optimisation of adaptors and eliminating the need for extensive training data.\n",
            "\n",
            "Similar to their text-based counterparts, speech-LLMs perform speech tasks through instruction following. Each task is defined by a textual prompt, which the LLM decoder processes alongside the speech embeddings produced by the SFMencoder. Conditioned on the speech input, the LLM then generates a response to fulfill the given instruction. Leveraging the vast knowledge embedded in the LLM decoder,\n",
            "\n",
            "speech-LLMs demonstrate emergent abilities not explicitly imparted during training [8]. Additionally, the prompt-driven approach enables speech-LLMs to address various speech classification tasks in a zero-shot manner. In this paradigm, users maintain the flexibility to incorporate different class labels into the prompt, rather than relying on fixed classification heads as used in conventional methods. These attributes notably enhance the potential of speech-LLMs to serve as a universal solution in the speech industry.\n",
            "\n",
            "Previous studies have explored a wide range of speechcentric challenges within the speech-LLM framework. However, the evaluation of its spoken language understanding (SLU) capabilities, particularly in zero-shot scenarios, remains limited. While several existing speech-LLMs have showcased competence in common speech tasks like automatic speech recognition (ASR), speech-to-text translation (ST), and spoken question answering (SQA), their effectiveness in some key SLU tasks such as intent classification (IC) and slot filling (SF) has not been thoroughly examined.\n",
            "\n",
            "To address this gap, this paper introduces WHISMA, a speech-LLM system designed to enhance the zero-shot SLU performance across various domains. In this context, SLU refers to inferring semantics from spoken utterances. The proposed system employs Whisper [11] and Llama-3 [6] models as the speech encoder and text decoder, respectively. These components remain fixed during training and are connected by a trainable modality aligner. Low-rank adaptation (LoRA) [12] is implemented on Llama-3 to accommodate speech modality inputs. WHISMA is fine-tuned using approximately 2000 hours of speech data, covering tasks of ASR, IC, SF, SQA, and spoken (query) instruction tuning (SQIT/SIT). We adopt a training strategy that enables the system to perform an auxiliary ASR step before SLU through speech chain-of-thought (SCoT) [10] or multi-round (MR) inference, while maintaining the E2E property of WHISMA.\n",
            "\n",
            "To facilitate reproducing the proposed system, we ensure that all the training and test data used in this work is openly accessible. Specifically, we publish our self-developed Spoken-Alpaca dataset utilised for the SIT task. Derived from the text-based Alpaca instruction tuning corpus [13], we conduct pre-processing on the input and instruction fields, and synthesise speech for them using an in-house text-to-speech (TTS) model. Moreover, we curate a new SLU benchmark,\n",
            "\n",
            "named SLU-GLUE, to assess speech-LLMs on task-agnostic data beyond IC and SF. Tasks within SLU-GLUE include sentiment analysis (SA), semantic equivalence recognition (SER), and spoken-textual entailment recognition (STER). This benchmark is also made publicly available.\n",
            "\n",
            "The contributions of our work are summarised as follows:\n",
            "\n",
            "- · We introduce WHISMA, a speech-LLM that incorporates cutting-edge speech (Whisper) and language (Llama-3) foundation models to support a diverse range of SLU tasks in zero-shot settings.\n",
            "- · Through evaluations on the SLU tasks seen in training, WHISMA demonstrates superior zero-shot performance compared to existing baselines on the SLURP [14], FSC [15] and SmartLight [16] benchmarks.\n",
            "- · To illustrate the robustness of WHISMA on unseen SLU tasks, we further evaluate the system on the curated SLU-GLUE benchmark 1 , showing that it outperforms both the modular Whisper-Llama-3 system and an existing speech-LLM, Qwen-Audio [9].\n",
            "- · To mitigate over-fitting during WHISMA training, we develop the Spoken-Alpaca 2 dataset for speech-based instruction tuning, and show that it facilitates the model generalisation to unseen tasks.\n",
            "\n",
            "## \n",
            "\n",
            "content_2:  4.3. Main results\n",
            "\n",
            "The STSC evaluation results on the zero-shot test set of SLURP are presented in Table 3, focusing on ASR worderror-rate (WER) and SF SLU-F1 score. To ensure a fair com-\n",
            "\n",
            "Table 4 . STUC zero-shot evaluation. WER (%) and IC accuracy (IC Acc. %) on the test set of FSC.Table 5 . STUC zero-shot evaluation. WER (%), IC accuracy (IC Acc. %), SF SLU-F1 (%), and perfect parsing (PP, %) on the full set of SmartLight.\n",
            "\n",
            "| Model                     | WER ↓   | IC Acc. ↑   |\n",
            "|---------------------------|---------|-------------|\n",
            "| supervised                |         |             |\n",
            "| Finstreder [33]           | -       | 99.7        |\n",
            "| zero-shot                 |         |             |\n",
            "| BLSP [7]                  | -       | 77.5        |\n",
            "| ZS-Whisper-SLU [25]       | 0.8     | 95.0        |\n",
            "| Whisper-Llama-3 (modular) | 1.5     | 89.3        |\n",
            "| WHISMA (proposed)         | -       | 90.9        |\n",
            "| + SCoT                    | 1.7     | 97.3        |\n",
            "| + MR                      | 2.7     | 97.3        |\n",
            "\n",
            "| Model                     | WER ↓   | IC Acc. ↑   | SLU-F1 ↑   | PP ↑   |\n",
            "|---------------------------|---------|-------------|------------|--------|\n",
            "| supervised                |         |             |            |        |\n",
            "| Finstreder [33]           | 6.1     | -           | -          | 88.0   |\n",
            "| Whisper-TS [24]           | -       | 96.3        | -          | -      |\n",
            "| zero-shot                 |         |             |            |        |\n",
            "| BLSP [7]                  | -       | 78.8        | -          | -      |\n",
            "| ZS-Whisper-SLU [25]       | 2.7     | 91.6        | 90.9       | 82.5   |\n",
            "| UniverSLU-14 [24]         | -       | 44.6        | -          | -      |\n",
            "| Whisper-Llama-3 (modular) | 4.0     | 94.3        | 82.4       | 75.7   |\n",
            "| WHISMA (proposed)         | -       | 81.7        | 87.8       | 68.9   |\n",
            "| + SCoT                    | 4.5     | 95.9        | 90.2       | 82.5   |\n",
            "| + MR                      | 5.5     | 94.4        | 90.7       | 81.6   |\n",
            "\n",
            "parison, the modular Whisper-Llama-3 system is excluded as it has not been tuned on the SLURP dataset. For supervised model performance, we re-implement the spoken and natural language understanding (SNLU) system described in [32] and conduct the same evaluation. Given the rich knowledge embedded in Llama-3 and the wide array of SLU datasets used in training, our proposed WHISMA system with either SCoT or MR inference significantly outperforms the existing SOTA model, ZS-Whisper-SLU, achieving a 26.6% relative gain in SLU-F1. This comparison is equitable as ZS-whisper-SLU also conducts ASR prior to SF. The SCoT inference strategy achieves a lower WER compared to MR (11.9% vs. 13.8%), which we believe is because SCoT combines both tasks into a\n",
            "\n",
            "Table 6 . UTUC zero-shot evaluation. Accuracy (%) on SLU-GLUE sub-tasks.\n",
            "\n",
            "| Model                     |   SST-2 ↑ |   QQP ↑ |   QNLI ↑ |   RTE ↑ |   SciTail ↑ |   Avg. ↑ |\n",
            "|---------------------------|-----------|---------|----------|---------|-------------|----------|\n",
            "| Qwen-Audio [9]            |      53.4 |    62.4 |     58.8 |    65   |        54.5 |     58.8 |\n",
            "| + MR                      |      52.9 |    68.4 |     52.4 |    67.2 |        56.3 |     59.4 |\n",
            "| Whisper-Llama-3 (modular) |      88.3 |    74.9 |     81.6 |    76.7 |        68.3 |     78   |\n",
            "| WHISMA (proposed)         |      79   |    65.1 |     84   |    72.8 |        60.5 |     72.3 |\n",
            "| + SCoT                    |      88   |    65.1 |     78.9 |    73.7 |        71.7 |     75.5 |\n",
            "| + MR                      |      89.2 |    74.2 |     88   |    78   |        65.4 |     79   |\n",
            "| WHISMA w/o Spoken-Alpaca  |      78.6 |    45.9 |     50.9 |    61.2 |        52.6 |     57.8 |\n",
            "| + SCoT                    |      87.6 |    69   |     64.8 |    74.6 |        66.2 |     72.4 |\n",
            "| + MR                      |      86.6 |    52.6 |     73.8 |    70   |        57   |     68   |\n",
            "\n",
            "single prompt, enhancing ASR with the information in the SF instruction. WHISMA exhibits competitive SF performance (46.8%) to ZS-Whisper-SLU even without utilising speech transcripts. However, it lags behind cases with SCoT or MR inference, which emphasises the importance of performing ASR for SF tasks.\n",
            "\n",
            "The results of STUC evaluations on the FSC and SmartLight benchmarks are provided in Table 4 and 5, respectively. In the FSC benchmark, WHISMA+SCoT/MR obtains superior zero-shot performance compared to both the BLSP and ZS-Whisper-SLU baselines, showing relative improvements of 25.5% and 2.4% in IC accuracy. For the SmartLight benchmark, WHISMA+SCoT achieves SOTA performance in IC accuracy (95.9%) compared to all the zero-shot baselines. It also delivers SF SLU-F1 (90.2%) and perfect parsing (PP, 82.5%) results comparable to ZS-Whisper-SLU. Similar performance is observed with MR inference. On both benchmarks, WHISMA without ASR still surpasses the modular Whisper-Llama-3 model in various scenarios, highlighting the reduced complexity of E2E speech-LLMs over the modular approach. Regarding the SCoT and MR inferences, due to the limited ASR data used in fine-tuning, WHISMA generally displays higher WER relative to ZS-Whisper-SLU and Whisper-Llama-3. However, the joint modelling of speech and text modalities in WHISMA+SCoT/MR compensates for this shortcoming, leading to improved SLU performance.\n",
            "\n",
            "Table 6 presents the UTUC evaluation results on the SLU-GLUE benchmark. Alongside the modular system, we include Qwen-Audio, the most comprehensive open-sourced speech-LLM (available at the time of this study), for comparison. Qwen-Audio serves as a relevant baseline to WHISMA, since its training data includes SLU tasks similar to those used in our work. Furthermore, the system supports MR inference. As shown in the table, WHISMA demonstrates promising generalisation capabilities to unseen tasks during training. Specifically, WHISMA+MR achieves the highest performance among all the speech-LLMs, with an averaged\n",
            "\n",
            "accuracy of 79.0% across the five sub-tasks. This represents relative gains of 1.3% and 33.0% over Whisper-Llama-3 and Qwen-Audio+MR, respectively. Consistent with the observations in STSC and STUC settings, incorporating ASR into SLU substantially enhances WHISMA's performance on the challenging SLU-GLUE data, resulting in a relative gain of 9.3% when MR inference is performed. In contrast, QwenAudio+MR only provides minimal improvements over the SLU-alone inference fashion.\n",
            "\n",
            "In Table 6, we further illustrate the impact of utilising the Spoken-Alpaca dataset into the fine-tuning process of WHISMA. One can see that excluding this dataset results in a notable performance decline for WHISMA, with a 20.1% relative decrease in averaged accuracy when ASR is omitted. Similar degradation is observed with SCoT and MR inference, suffering relative reductions of 4.1% and 13.9% in averaged accuracy, respectively. Therefore, although SpokenAlpaca contributes only a small fraction (2.2% of total duration) of the entire training data, it plays a crucial role in strengthening the generalisability of speech-LLMs.\n",
            "\n",
            "## \n",
            "\n",
            "content_3:  4.2. Evaluation tasks\n",
            "\n",
            "We evaluate the proposed WHISMA model under three levels of zero-shot settings, ranging from easy to hard:\n",
            "\n",
            "- · seen-task-seen-corpus (STSC) , targeting the SF task using the zero-shot split of the SLURP dataset [14].\n",
            "- · seen-task-unseen-corpus (STUC) , focusing on IC and SF tasks within the test sets from FSC [15] and SmartLight [16] benchmarks.\n",
            "- · unseen-task-unseen-corpus (UTUC) , representing the most challenging scenario, utilising a new SLU benchmark named SLU-GLUE.\n",
            "\n",
            "For the STSC and STUC evaluations, we focus on data within the domain of in-home robot assistant. The same setups as\n",
            "\n",
            "Table 3 . STSC zero-shot evaluation. WER (%) and SF SLU-F1 (%) on the zero-shot test set of SLURP.\n",
            "\n",
            "| Model               | WER ↓   | SLU-F1 ↑   |\n",
            "|---------------------|---------|------------|\n",
            "| supervised          |         |            |\n",
            "| SNLU [32]           | 13.6    | 69.9       |\n",
            "| zero-shot           |         |            |\n",
            "| ZS-Whisper-SLU [25] | 8.3     | 50.0       |\n",
            "| WHISMA (proposed)   | -       | 46.8       |\n",
            "| + SCoT              | 11.9    | 63.1       |\n",
            "| + MR                | 13.8    | 63.3       |\n",
            "\n",
            "described in [25] are followed to structure the test data. In the case of SLURP, a zero-shot data split is created by holding out 5 slot types: { podcast name, artist name, audiobook name, business name, radio name } , which results in a test set containing 18k utterances. Regarding FSC, the original { action, object, location } labels are reorganised into 15 intents and 2 slots. As for SmartLight, we employ the full close-field subset as the test set, comprising 6 intents and 3 slot types.\n",
            "\n",
            "In addition to the task-oriented datasets mentioned above, we curate SLU-GLUE for the UTUC evaluation in our experiment. Derived from ASR-GLUE [31], SLU-GLUE comprises three primary tasks: sentiment analysis (SA), semantic equivalence recognition (SER), and spoken-textual entailment recognition (STER). These tasks encompass five sub-tasks, namely: Stanford Sentiment Treebank (SST-2), Quora Question Pairs (QQP), Question-answering NLI (QNLI), Recognizing Textual Entailment (RTE), and SciTail. We exclude Semantic Textual Similarity Benchmark (STS-B) as it is not suitable for zero-shot evaluation. The evaluation metric for all the tasks is binary accuracy. The data domains of SLUGLUE span from everyday language, such as movie reviews, to specialised scientific terminology. Speech samples in the dataset are recorded by six native speakers and are mixed with various levels of noise. Further detailed information of SLU-GLUE is provided in Table. 2.\n",
            "\n",
            "The performance of WHISMA is compared against multiple robust baselines, including state-of-the-art (SOTA) supervised models, SOTA zero-shot models, the modular ASRLLM counterpart to WHISMA, and a well-established opensourced speech-LLM called Qwen-Audio [9].\n",
            "\n",
            "## \n",
            "\n",
            "content_4:  3.1. Model architecture\n",
            "\n",
            "The model architecture of WHISMA is illustrated in Fig. 1. We adopt the encoder from Whisper-large-v2 , a 32layer Transformer model with two convolutional neural network (CNN) down-samplers, as the speech encoder in our system. The resulting speech embeddings are fed to a modality aligner, which converts these embeddings to align with the input space of the LLM decoder. Our modality aligner is structured similarly to a previous system [10], consisting\n",
            "\n",
            "Table 1 . Multi-task training dataset.\n",
            "\n",
            "| Task   | Data Source                              | #Hours   | #Samples   |\n",
            "|--------|------------------------------------------|----------|------------|\n",
            "| ASR    | GigaSpeech (M) [26]                      | 1000     | 910k       |\n",
            "| IC     | SLURP (zero-shot) [14]                   | 80       | 94k        |\n",
            "| SF     | SLURP (zero-shot) [14]                   | 80       | 94k        |\n",
            "| SQA    | LibriSQA-partI [27] LibriSQA-partII [27] | 360      | 104k 104k  |\n",
            "| SQIT   | Spoken-Alpaca                            | 360 30   | 29k        |\n",
            "| SIT    | Spoken-Alpaca                            | 14       | 14k        |\n",
            "\n",
            "of two CNN layers for further down-sampling, a bottleneck adaptor, and a linear output layer to match dimensions.\n",
            "\n",
            "WHISMA's decoder is implemented using Llama-38B-Instruct , one of the most promising open-sourced LLMs optimised for instruction following. This decoder processes both the aligned speech embeddings and a taskdescriptive text prompt, generating a response to execute the given instruction. We integrate low-rank adaptation (LoRA) weights into the attention module of all 32 Transformer layers in Llama-3, enabling it to effectively handle speech modality inputs. During training, only the modality aligner and LoRA parameters are optimised.\n",
            "\n",
            "## \n",
            "\n",
            "content_5:  6. REFERENCES\n",
            "\n",
            "- [1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli, 'wav2vec 2.0: A framework for self-supervised learning of speech representations,' Advances in neural information processing systems , vol. 33, pp. 12449-12460, 2020.\n",
            "- [2] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed, 'Hubert: Self-supervised speech representation learning by masked prediction of hidden units,' IEEE/ACM Transactions on Audio, Speech, and Language Processing , vol. 29, pp. 3451-3460, 2021.\n",
            "- [3] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al., 'WavLM: Large-scale self-supervised pre-training for full stack speech processing,' IEEE Journal of Selected Topics in Signal Processing , vol. 16, no. 6, pp. 1505-1518, 2022.\n",
            "- [4] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al., 'Mistral 7B,' arXiv preprint arXiv:2310.06825 , 2023.\n",
            "- [5] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al., 'GPT-4 technical report,' arXiv preprint arXiv:2303.08774 , 2023.\n",
            "- [6] AI@Meta, 'Llama 3 model card,' 2024.\n",
            "- [7] Chen Wang, Minpeng Liao, Zhongqiang Huang, Jinliang Lu, Junhong Wu, Yuchen Liu, Chengqing Zong, and Jiajun Zhang, 'BLSP: Bootstrapping languagespeech pre-training via behavior alignment of continuation writing,' arXiv preprint arXiv:2309.00916 , 2023.\n",
            "- [8] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, and Chao Zhang, 'SALMONN: Towards generic hearing abilities for large language models,' in The Twelfth International Conference on Learning Representations , 2024.\n",
            "- [9] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou, 'Qwen-Audio: Advancing universal audio understanding via unified large-scale audio-language models,' arXiv preprint arXiv:2311.07919 , 2023.\n",
            "- [10] Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, Linquan Liu, et al., 'WavLLM: Towards\n",
            "- robust and adaptive speech large language model,' arXiv preprint arXiv:2404.00656 , 2024.\n",
            "- [11] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever, 'Robust speech recognition via large-scale weak supervision,' in International conference on machine learning . PMLR, 2023, pp. 28492-28518.\n",
            "- [12] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen, 'LoRA: Low-rank adaptation of large language models,' in International Conference on Learning Representations , 2022.\n",
            "- [13] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto, 'Stanford alpaca: An instruction-following llama model,' https://gith ub.com/tatsu-lab/stanford\\_alpaca , 2023.\n",
            "- [14] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and Verena Rieser, 'SLURP: A spoken language understanding resource package,' in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , Online, Nov. 2020, pp. 7252-7262, Association for Computational Linguistics.\n",
            "- [15] Loren Lugosch, Mirco Ravanelli, Patrick Ignoto, Vikrant Singh Tomar, and Yoshua Bengio, 'Speech model pre-training for end-to-end spoken language understanding,' arXiv preprint arXiv:1904.03670 , 2019.\n",
            "- [16] Alaa Saade, Joseph Dureau, David Leroy, Francesco Caltagirone, Alice Coucke, Adrien Ball, Cl'ement Doumouro, Thibaut Lavril, Alexandre Caulier, Th'eodore Bluche, et al., 'Spoken language understanding on the edge,' in 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS) . IEEE, 2019, pp. 57-61.\n",
            "- [17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi, 'BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,' in International conference on machine learning . PMLR, 2023, pp. 19730-19742.\n",
            "- [18] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee, 'Improved baselines with visual instruction tuning,' 2023.\n",
            "- [19] Yuang Li, Yu Wu, Jinyu Li, and Shujie Liu, 'Prompting large language models for zero-shot domain adaptation in speech recognition,' in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1-8.\n",
            "- [20] Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, et al., 'Prompting large language models with speech recognition abilities,' in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2024, pp. 13351-13355.\n",
            "- [21] Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, et al., 'On decoder-only architecture for speech-to-text and large language model integration,' in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1-8.\n",
            "- [22] Mingqiu Wang, Wei Han, Izhak Shafran, Zelin Wu, Chung-Cheng Chiu, Yuan Cao, Nanxin Chen, Yu Zhang, Hagen Soltau, Paul K Rubenstein, et al., 'SLM: Bridge the thin gap between speech and text foundation models,' in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1-8.\n",
            "- [23] Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, and Yemin Shi, 'LLaSM: Large language and speech model,' arXiv preprint arXiv:2308.15930 , 2023.\n",
            "- [24] Siddhant Arora, Hayato Futami, Jee-weon Jung, Yifan Peng, Roshan Sharma, Yosuke Kashiwagi, Emiru Tsunoo, Karen Livescu, and Shinji Watanabe, 'UniverSLU: Universal spoken language understanding for diverse tasks with natural language instructions,' in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , 2024, pp. 2754-2774.\n",
            "- [25] Mohan Li, Simon Keizer, and Rama Doddipatla, 'Prompting whisper for qa-driven zero-shot end-toend spoken language understanding,' arXiv preprint arXiv:2406.15209 , 2024.\n",
            "- [26] Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, et al., 'GigaSpeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio,' arXiv preprint arXiv:2106.06909 , 2021.\n",
            "- [27] Zihan Zhao, Yiyang Jiang, Heyang Liu, Yanfeng Wang, and Yu Wang, 'Librisqa: Pioneering freeform and open-ended spoken question answering with a novel dataset and framework,' arXiv preprint arXiv:2308.10390 , 2023.\n",
            "- [28] Jaehyeon Kim, Jungil Kong, and Juhee Son, 'Conditional variational autoencoder with adversarial learning\n",
            "- for end-to-end text-to-speech,' in International Conference on Machine Learning . PMLR, 2021, pp. 55305540.\n",
            "- [29] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, 'Librispeech: an ASR corpus based on public domain audio books,' in 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP) . IEEE, 2015, pp. 5206-5210.\n",
            "- [30] Mohan Li and Rama Doddipatla, 'Non-autoregressive end-to-end approaches for joint automatic speech recognition and spoken language understanding,' in 2022 IEEE Spoken Language Technology Workshop (SLT) . IEEE, 2023, pp. 390-397.\n",
            "- [31] Lingyun Feng, Jianwei Yu, Deng Cai, Songxiang Liu, Haitao Zheng, and Yan Wang, 'ASR-GLUE: A new multi-task benchmark for asr-robust natural language understanding,' arXiv preprint arXiv:2108.13048 , 2021.\n",
            "- [32] Mohan Li, C˘at˘alin Zoril˘a, Cong-Thanh Do, and Rama Doddipatla, 'Towards a unified end-to-end language understanding system for speech and text inputs,' in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1-8.\n",
            "- [33] Daniel Bermuth, Alexander Poeppel, and Wolfgang Reif, 'Finstreder: Simple and fast spoken language understanding with finite state transducers using modern speech-to-text models,' arXiv preprint arXiv:2206.14589 , 2022. \n",
            "\n",
            "content_6: ## WHISMA: A SPEECH-LLM TO PERFORM ZERO-SHOT SPOKEN LANGUAGE UNDERSTANDING\n",
            "\n",
            "Mohan Li, Cong-Thanh Do, Simon Keizer, Youmna Farag, Svetlana Stoyanchev, Rama Doddipatla\n",
            "\n",
            "Cambridge Research Laboratory, Toshiba Europe Ltd, Cambridge, UK\n",
            "\n",
            "## ABSTRACT\n",
            "\n",
            "Speech large language models (speech-LLMs) integrate speech and text-based foundation models to provide a unified framework for handling a wide range of downstream tasks. In this paper, we introduce WHISMA, a speech-LLM tailored for spoken language understanding (SLU) that demonstrates robust performance in various zero-shot settings. WHISMA combines the speech encoder from Whisper with the Llama-3 LLM, and is fine-tuned in a parameter-efficient manner on a comprehensive collection of SLU-related datasets. Our experiments show that WHISMA significantly improves the zero-shot slot filling performance on the SLURP benchmark, achieving a relative gain of 26.6% compared to the current state-of-the-art model. Furthermore, to evaluate WHISMA's generalisation capabilities to unseen domains, we develop a new task-agnostic benchmark named SLU-GLUE. The evaluation results indicate that WHISMA outperforms an existing speech-LLM (Qwen-Audio) with a relative gain of 33.0%.\n",
            "\n",
            "Index Terms -spoken language understanding, speech large language model, zero-shot learning\n",
            "\n",
            "## \n",
            "\n",
            "content_7:  2. RELATED WORK\n",
            "\n",
            "Motivated by the success of vision language models (VLMs) [17, 18], there has been a growing interest in extending LLMs with auditory capabilities. Initial attempts to incorporate speech perception into LLMs involve LLM-based ASR models [19, 20]. Building upon this, Speech-Llama [21] integrates both ST and ASR to the framework through multi-task learning. Further, SLM [22] enhances the system with speech instruction tuning. However, these models typically undergo fine-tuning with a limited set of tasks and lack generalised understanding abilities.\n",
            "\n",
            "Recent advancements have introduced speech-LLMs with more diverse speech processing functions. Notable examples include BLSP [7], LLaSM [23], SALMONN [8], QwenAudio [9], and WavLLM [10]. Most of these systems utilise a variety of speech or audio resources to achieve cross-modal perception. Among them, zero-shot SLU evaluation results have been reported by BLSP for IC and SA tasks, SALMONN for a SF task, and WavLLM for a SQA task. In this work, we aim to conduct a comprehensive evaluation on these tasks using our proposed WHISMA system.\n",
            "\n",
            "UniverSLU [24] is the latest study that exclusively examines the SLU capabilities of the SFM, which fine-tunes the\n",
            "\n",
            "Fig. 1 . An overview of WHISMA model architecture.\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "Whisper model with up to 17 SLU datasets encompassing both acoustic and semantic domain tasks. The zero-shot performance of UniverSLU, as reported, reveals that the system struggles to generalise to unseen datasets and tasks, which is the issue we would like to address in this work.\n",
            "\n",
            "The most relevant research to this study is ZS-WhisperSLU proposed by [25], which investigates zero-shot IC and SF tasks within a QA-driven framework based on Whisper. Despite the promising performance achieved by the system, it is challenging to utilise ZS-Whisper-SLU for unseen tasks due to the limitation of the Whisper decoder. In this regard, we seek to bridge this gap by leveraging an LLM that possess broader knowledge than an SFM.\n",
            "\n",
            "## \n",
            "\n",
            "content_8:  3. METHOD\n",
            "\n",
            "In this section, we present the architecture of the proposed WHISMA system and describe the fine-tuning speech tasks along with their corresponding datasets. Additionally, we introduce the training strategy that integrates ASR auxiliary into SLU tasks to enhance the reliability of LLM inference, while preserving WHISMA's end-to-end (E2E) nature.\n",
            "\n",
            "## \n",
            "\n",
            "content_9:  3.2. Training data\n",
            "\n",
            "The fine-tuning of WHISMA is conducted with a multi-task learning approach. We compile a substantial training dataset that encompasses the following tasks: automatic speech recognition (ASR), intent classification (IC), slot filling (SF), spoken question answering (SQA), spoken query instruction tuning (SQIT), and spoken instruction tuning (SIT). Although ASR is not categorized as an SLU task, it significantly aids in aligning between the speech and text modalities. These tasks correspond to data from different sources, including the medium subset of GigaSpeech [26], the zero-shot data split of SLURP [14], LibriSQA-partI for open-ended QA, partII for multi-choice QA [27], and Spoken-Alpaca, developed as part of this research endeavor. The total duration of speech data amounts to approximately 2000 hours. Detailed information on this dataset is provided in Table 1.\n",
            "\n",
            "Although previous studies have employed Alpaca [13] with synthesised speech for instruction tuning [10, 22], the associated data remains unpublished, which hinders the replication of their findings. To remedy this, we introduce Spoken-Alpaca and release it for public access. The original text-based Alpaca dataset comprises two types of examples: i) containing fields { instruction, input, output } , where instruction describes the task, input provides context, and output is the expected system response; and ii) with fields\n",
            "\n",
            "{ instruction, output } , where the instruction stands alone as a query, not requiring contextual information. For the first type of examples, we generate speech for the input field while retaining instruction as the text prompt for the speechLLM system. This configuration defines the task referred to as spoken instruction tuning (SIT). Concerning the second type, speech synthesis is applied to instruction , which the system receives without a text prompt. This task category is denoted as spoken query instruction tuning (SQIT). We develop an in-house variational inference with adversarial learning for end-to-end text-to-speech (VITS) model [28] using the Librispeech [29] dataset featuring around 2200 speakers. Examples in Alpaca unsuitable for speech synthesis are filtered out, including those containing lengthy texts, mathematical equations, tables, etc. We also alter the wording of some instructions to make them speech-oriented. The diverse instructions within Spoken-Alpaca could significantly reduce the risk of over-fitting WHISMA to seen tasks with restricted prompt variations, such as ASR, IC and SF. Our experiments (Section 4.3) demonstrate that incorporating the dataset enables the proposed system to generalise more effectively to unseen tasks.\n",
            "\n",
            "## \n",
            "\n",
            "\n",
            "</ctx>\n",
            "\n",
            "--Response--\n",
            "1. Always and only respond in HTML tag formatted which should render on website smoothly. Do not use markdown, plain text, or any non-HTML formatting. Use semantic HTML tags like <h1> to <h6> for headings, <p> for paragraphs, <ul> and <li> for lists, <strong> for bold, <em> for italics, <mark> for highlights, <code> for inline code, <pre> for code blocks, <a> for links, and <table> for tabular data. Ensure the HTML is well-formed, valid, and self-contained (no external CSS or JavaScript)\n",
            "2. Cite relevant chunks from the context using ^[Context_n] notation. For multiple citations, use ^[Context_1] ^[Context_2], etc.\n",
            "3. Respond in customizable response lengths:\n",
            "   - Concise\n",
            "   - Medium\n",
            "   - Detailed\n",
            "4. Respond in customizable Format:\n",
            "  - use <ul> and <li> for bullets points and <p> for paragraph\n",
            "5. For questions unrelated to the given context, return : \"Sorry I dont have information about this,Please ask related to your research paper only.\" or politely deny and guide the user back to the given context.\n",
            "\n",
            "--Task--\n",
            "1. Answer questions using only the content from the provided context. Do not rely on external knowledge.\n",
            "2. Keep responses concise, crisp, and to the point unless the user requests a detailed answer.\n",
            "3. Always cite relevant chunks from the context using ^[Context_n] notation.\n",
            "4. If the user asks for a specific format (bullet points or paragraphs), adhere to their request.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await model.get_response(\n",
        "    system = system,\n",
        "    model_id = 'gemini-1.5-flash',\n",
        "    query = 'what is whisma, in detail,bullets points',\n",
        "    chat_history = [],\n",
        "    temperature = .5,\n",
        "    max_tokens = 2000\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5IGA7QXsrvQ",
        "outputId": "d8b0ac03-0388-4862-e04a-ab6348c3a317"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id=None, choices=[Choice(finish_reason=None, index=0, logprobs=None, message=ChatCompletionMessage(content='```html\\n<h1>WHISMA: A Detailed Overview</h1>\\n<ul>\\n  <li><strong>What it is:</strong> WHISMA is a speech-LLM (speech large language model) designed to enhance zero-shot spoken language understanding (SLU) performance across various domains.^[content_1] ^[content_6]</li>\\n  <li><strong>Architecture:</strong> It combines the Whisper large-v2 model as the speech encoder and the Llama-3 8B-Instruct model as the text decoder.  A trainable modality aligner connects these components. Low-rank adaptation (LoRA) is used on Llama-3 to handle speech modality inputs.^[content_4]</li>\\n  <li><strong>Training:</strong> WHISMA is fine-tuned using approximately 2000 hours of speech data covering ASR, IC, SF, SQA, and SQIT/SIT tasks.  A training strategy incorporates an auxiliary ASR step before SLU via speech chain-of-thought (SCoT) or multi-round (MR) inference, maintaining its E2E property.^[content_3] ^[content_8] ^[content_9]</li>\\n  <li><strong>Data Used:</strong> Training data includes GigaSpeech, SLURP (zero-shot split), LibriSQA, and a newly created dataset, Spoken-Alpaca.  Spoken-Alpaca is derived from the Alpaca instruction tuning corpus and includes synthesized speech.^[content_9]</li>\\n  <li><strong>Inference Strategies:</strong> WHISMA explores two inference strategies: SCoT (Speech chain-of-thought), where ASR and SLU prompts are concatenated for one-shot processing, and MR (Multi-round), where transcription is done first, followed by SLU based on speech embeddings and the transcript.^[content_0]</li>\\n  <li><strong>Evaluation:</strong> WHISMA is evaluated under three zero-shot settings: STSC (seen-task-seen-corpus), STUC (seen-task-unseen-corpus), and UTUC (unseen-task-unseen-corpus), using benchmarks like SLURP, FSC, SmartLight, and a new benchmark, SLU-GLUE.^[content_3]</li>\\n  <li><strong>Performance:</strong>  WHISMA shows superior zero-shot performance compared to baselines on various benchmarks, particularly in slot filling (SF) and intent classification (IC).  Incorporating ASR via SCoT or MR significantly improves performance.^[content_4] ^[content_3]</li>\\n  <li><strong>Contribution:</strong> WHISMA contributes by providing a robust speech-LLM for diverse SLU tasks in zero-shot settings, achieving state-of-the-art results and introducing a new SLU benchmark (SLU-GLUE).^[content_1]</li>\\n</ul>\\n```\\n', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), finishReason='stop')], created=1736165961, model='gemini-1.5-flash', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=None, prompt_tokens=None, total_tokens=None, completion_tokens_details=None, prompt_tokens_details=None, completionTokens=620, promptTokens=9697, totalTokens=10317))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenResponse(api_key=GEMINI_KEY,base_url = GEMINI_BASE_URL)\n",
        "chat_history = []\n",
        "\n",
        "async def chat_loop():\n",
        "    while True:\n",
        "        # Get user input\n",
        "        user_message = input(\"You: \")\n",
        "        if user_message.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Exiting chat...\")\n",
        "            break\n",
        "\n",
        "        # Add user message to chat history\n",
        "        chat_history.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "        # Run retrieval and query expansion pipeline\n",
        "        chunks, rephrased_query, system = await retrieval_with_query_expansion(\n",
        "            user_message=user_message,\n",
        "            prev_conversation=chat_history,\n",
        "            retrieval = retrieval,\n",
        "            llm_handler=model\n",
        "        )\n",
        "        print(f\"System Prompt: {system}\")\n",
        "\n",
        "        # Get response from the model\n",
        "        response = await model.get_response(\n",
        "            system=system,\n",
        "            model_id='gemini-1.5-flash',\n",
        "            query=user_message,\n",
        "            chat_history=chat_history,\n",
        "            temperature=0.5,\n",
        "            max_tokens=2000\n",
        "        )\n",
        "        if isinstance(response,Dict):\n",
        "          raise(response)\n",
        "        response = response.choices[0].message.content.replace(\"```html\", \"\").replace(\"```\", \"\")\n",
        "        # Add assistant response to chat history\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "        # Print the response\n",
        "        print(f\"You: {user_message}\")\n",
        "        print(f\"Assistant: {response}\")\n",
        "\n",
        "# Run the chat loop in the notebook\n",
        "await chat_loop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CBiCh3IMu6yI",
        "outputId": "ece0e75c-4a89-4762-e80b-9e2b4df64e4e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Explain Intro section in details list all the key points ,suggest something\n",
            "['Provide a detailed explanation of the introduction section, including key points and suggestions for improvement.', 'Explain the introduction section in detail, listing all key points and offering recommendations.']\n",
            "Query Expanded:\n",
            " Provide a detailed explanation of the introduction section, including key points and suggestions for improvement.\n",
            "System Prompt: -Role--\n",
            "You are a question-answering chatbot for research paper. Your job is to answer user queries strictly using the content provided in the delimited by <ctx></ctx>). You cannot use any external knowledge or information outside the provided context.\n",
            "\n",
            "<ctx>\n",
            "content_0:  1. INTRODUCTION\n",
            "\n",
            "Traditional speech processing techniques typically depend on specialised models tailored to individual tasks. These models, trained with limited data and constrained architectures, often face difficulties in generalising to new domains and applications. However, recent advancements in speech foundation models (SFMs) [1, 2, 3] and large language models (LLMs) [4, 5, 6] have fundamentally reshaped this approach, leading to the emergence of multi-functional end-to-end (E2E) speech-LLM systems [7, 8, 9, 10]. By utilising off-the-shelf SFMs and LLMs, the development of a speech-LLM system can be streamlined, often requiring only optimisation of adaptors and eliminating the need for extensive training data.\n",
            "\n",
            "Similar to their text-based counterparts, speech-LLMs perform speech tasks through instruction following. Each task is defined by a textual prompt, which the LLM decoder processes alongside the speech embeddings produced by the SFMencoder. Conditioned on the speech input, the LLM then generates a response to fulfill the given instruction. Leveraging the vast knowledge embedded in the LLM decoder,\n",
            "\n",
            "speech-LLMs demonstrate emergent abilities not explicitly imparted during training [8]. Additionally, the prompt-driven approach enables speech-LLMs to address various speech classification tasks in a zero-shot manner. In this paradigm, users maintain the flexibility to incorporate different class labels into the prompt, rather than relying on fixed classification heads as used in conventional methods. These attributes notably enhance the potential of speech-LLMs to serve as a universal solution in the speech industry.\n",
            "\n",
            "Previous studies have explored a wide range of speechcentric challenges within the speech-LLM framework. However, the evaluation of its spoken language understanding (SLU) capabilities, particularly in zero-shot scenarios, remains limited. While several existing speech-LLMs have showcased competence in common speech tasks like automatic speech recognition (ASR), speech-to-text translation (ST), and spoken question answering (SQA), their effectiveness in some key SLU tasks such as intent classification (IC) and slot filling (SF) has not been thoroughly examined.\n",
            "\n",
            "To address this gap, this paper introduces WHISMA, a speech-LLM system designed to enhance the zero-shot SLU performance across various domains. In this context, SLU refers to inferring semantics from spoken utterances. The proposed system employs Whisper [11] and Llama-3 [6] models as the speech encoder and text decoder, respectively. These components remain fixed during training and are connected by a trainable modality aligner. Low-rank adaptation (LoRA) [12] is implemented on Llama-3 to accommodate speech modality inputs. WHISMA is fine-tuned using approximately 2000 hours of speech data, covering tasks of ASR, IC, SF, SQA, and spoken (query) instruction tuning (SQIT/SIT). We adopt a training strategy that enables the system to perform an auxiliary ASR step before SLU through speech chain-of-thought (SCoT) [10] or multi-round (MR) inference, while maintaining the E2E property of WHISMA.\n",
            "\n",
            "To facilitate reproducing the proposed system, we ensure that all the training and test data used in this work is openly accessible. Specifically, we publish our self-developed Spoken-Alpaca dataset utilised for the SIT task. Derived from the text-based Alpaca instruction tuning corpus [13], we conduct pre-processing on the input and instruction fields, and synthesise speech for them using an in-house text-to-speech (TTS) model. Moreover, we curate a new SLU benchmark,\n",
            "\n",
            "named SLU-GLUE, to assess speech-LLMs on task-agnostic data beyond IC and SF. Tasks within SLU-GLUE include sentiment analysis (SA), semantic equivalence recognition (SER), and spoken-textual entailment recognition (STER). This benchmark is also made publicly available.\n",
            "\n",
            "The contributions of our work are summarised as follows:\n",
            "\n",
            "- · We introduce WHISMA, a speech-LLM that incorporates cutting-edge speech (Whisper) and language (Llama-3) foundation models to support a diverse range of SLU tasks in zero-shot settings.\n",
            "- · Through evaluations on the SLU tasks seen in training, WHISMA demonstrates superior zero-shot performance compared to existing baselines on the SLURP [14], FSC [15] and SmartLight [16] benchmarks.\n",
            "- · To illustrate the robustness of WHISMA on unseen SLU tasks, we further evaluate the system on the curated SLU-GLUE benchmark 1 , showing that it outperforms both the modular Whisper-Llama-3 system and an existing speech-LLM, Qwen-Audio [9].\n",
            "- · To mitigate over-fitting during WHISMA training, we develop the Spoken-Alpaca 2 dataset for speech-based instruction tuning, and show that it facilitates the model generalisation to unseen tasks.\n",
            "\n",
            "## \n",
            "\n",
            "content_1:  4.3. Main results\n",
            "\n",
            "The STSC evaluation results on the zero-shot test set of SLURP are presented in Table 3, focusing on ASR worderror-rate (WER) and SF SLU-F1 score. To ensure a fair com-\n",
            "\n",
            "Table 4 . STUC zero-shot evaluation. WER (%) and IC accuracy (IC Acc. %) on the test set of FSC.Table 5 . STUC zero-shot evaluation. WER (%), IC accuracy (IC Acc. %), SF SLU-F1 (%), and perfect parsing (PP, %) on the full set of SmartLight.\n",
            "\n",
            "| Model                     | WER ↓   | IC Acc. ↑   |\n",
            "|---------------------------|---------|-------------|\n",
            "| supervised                |         |             |\n",
            "| Finstreder [33]           | -       | 99.7        |\n",
            "| zero-shot                 |         |             |\n",
            "| BLSP [7]                  | -       | 77.5        |\n",
            "| ZS-Whisper-SLU [25]       | 0.8     | 95.0        |\n",
            "| Whisper-Llama-3 (modular) | 1.5     | 89.3        |\n",
            "| WHISMA (proposed)         | -       | 90.9        |\n",
            "| + SCoT                    | 1.7     | 97.3        |\n",
            "| + MR                      | 2.7     | 97.3        |\n",
            "\n",
            "| Model                     | WER ↓   | IC Acc. ↑   | SLU-F1 ↑   | PP ↑   |\n",
            "|---------------------------|---------|-------------|------------|--------|\n",
            "| supervised                |         |             |            |        |\n",
            "| Finstreder [33]           | 6.1     | -           | -          | 88.0   |\n",
            "| Whisper-TS [24]           | -       | 96.3        | -          | -      |\n",
            "| zero-shot                 |         |             |            |        |\n",
            "| BLSP [7]                  | -       | 78.8        | -          | -      |\n",
            "| ZS-Whisper-SLU [25]       | 2.7     | 91.6        | 90.9       | 82.5   |\n",
            "| UniverSLU-14 [24]         | -       | 44.6        | -          | -      |\n",
            "| Whisper-Llama-3 (modular) | 4.0     | 94.3        | 82.4       | 75.7   |\n",
            "| WHISMA (proposed)         | -       | 81.7        | 87.8       | 68.9   |\n",
            "| + SCoT                    | 4.5     | 95.9        | 90.2       | 82.5   |\n",
            "| + MR                      | 5.5     | 94.4        | 90.7       | 81.6   |\n",
            "\n",
            "parison, the modular Whisper-Llama-3 system is excluded as it has not been tuned on the SLURP dataset. For supervised model performance, we re-implement the spoken and natural language understanding (SNLU) system described in [32] and conduct the same evaluation. Given the rich knowledge embedded in Llama-3 and the wide array of SLU datasets used in training, our proposed WHISMA system with either SCoT or MR inference significantly outperforms the existing SOTA model, ZS-Whisper-SLU, achieving a 26.6% relative gain in SLU-F1. This comparison is equitable as ZS-whisper-SLU also conducts ASR prior to SF. The SCoT inference strategy achieves a lower WER compared to MR (11.9% vs. 13.8%), which we believe is because SCoT combines both tasks into a\n",
            "\n",
            "Table 6 . UTUC zero-shot evaluation. Accuracy (%) on SLU-GLUE sub-tasks.\n",
            "\n",
            "| Model                     |   SST-2 ↑ |   QQP ↑ |   QNLI ↑ |   RTE ↑ |   SciTail ↑ |   Avg. ↑ |\n",
            "|---------------------------|-----------|---------|----------|---------|-------------|----------|\n",
            "| Qwen-Audio [9]            |      53.4 |    62.4 |     58.8 |    65   |        54.5 |     58.8 |\n",
            "| + MR                      |      52.9 |    68.4 |     52.4 |    67.2 |        56.3 |     59.4 |\n",
            "| Whisper-Llama-3 (modular) |      88.3 |    74.9 |     81.6 |    76.7 |        68.3 |     78   |\n",
            "| WHISMA (proposed)         |      79   |    65.1 |     84   |    72.8 |        60.5 |     72.3 |\n",
            "| + SCoT                    |      88   |    65.1 |     78.9 |    73.7 |        71.7 |     75.5 |\n",
            "| + MR                      |      89.2 |    74.2 |     88   |    78   |        65.4 |     79   |\n",
            "| WHISMA w/o Spoken-Alpaca  |      78.6 |    45.9 |     50.9 |    61.2 |        52.6 |     57.8 |\n",
            "| + SCoT                    |      87.6 |    69   |     64.8 |    74.6 |        66.2 |     72.4 |\n",
            "| + MR                      |      86.6 |    52.6 |     73.8 |    70   |        57   |     68   |\n",
            "\n",
            "single prompt, enhancing ASR with the information in the SF instruction. WHISMA exhibits competitive SF performance (46.8%) to ZS-Whisper-SLU even without utilising speech transcripts. However, it lags behind cases with SCoT or MR inference, which emphasises the importance of performing ASR for SF tasks.\n",
            "\n",
            "The results of STUC evaluations on the FSC and SmartLight benchmarks are provided in Table 4 and 5, respectively. In the FSC benchmark, WHISMA+SCoT/MR obtains superior zero-shot performance compared to both the BLSP and ZS-Whisper-SLU baselines, showing relative improvements of 25.5% and 2.4% in IC accuracy. For the SmartLight benchmark, WHISMA+SCoT achieves SOTA performance in IC accuracy (95.9%) compared to all the zero-shot baselines. It also delivers SF SLU-F1 (90.2%) and perfect parsing (PP, 82.5%) results comparable to ZS-Whisper-SLU. Similar performance is observed with MR inference. On both benchmarks, WHISMA without ASR still surpasses the modular Whisper-Llama-3 model in various scenarios, highlighting the reduced complexity of E2E speech-LLMs over the modular approach. Regarding the SCoT and MR inferences, due to the limited ASR data used in fine-tuning, WHISMA generally displays higher WER relative to ZS-Whisper-SLU and Whisper-Llama-3. However, the joint modelling of speech and text modalities in WHISMA+SCoT/MR compensates for this shortcoming, leading to improved SLU performance.\n",
            "\n",
            "Table 6 presents the UTUC evaluation results on the SLU-GLUE benchmark. Alongside the modular system, we include Qwen-Audio, the most comprehensive open-sourced speech-LLM (available at the time of this study), for comparison. Qwen-Audio serves as a relevant baseline to WHISMA, since its training data includes SLU tasks similar to those used in our work. Furthermore, the system supports MR inference. As shown in the table, WHISMA demonstrates promising generalisation capabilities to unseen tasks during training. Specifically, WHISMA+MR achieves the highest performance among all the speech-LLMs, with an averaged\n",
            "\n",
            "accuracy of 79.0% across the five sub-tasks. This represents relative gains of 1.3% and 33.0% over Whisper-Llama-3 and Qwen-Audio+MR, respectively. Consistent with the observations in STSC and STUC settings, incorporating ASR into SLU substantially enhances WHISMA's performance on the challenging SLU-GLUE data, resulting in a relative gain of 9.3% when MR inference is performed. In contrast, QwenAudio+MR only provides minimal improvements over the SLU-alone inference fashion.\n",
            "\n",
            "In Table 6, we further illustrate the impact of utilising the Spoken-Alpaca dataset into the fine-tuning process of WHISMA. One can see that excluding this dataset results in a notable performance decline for WHISMA, with a 20.1% relative decrease in averaged accuracy when ASR is omitted. Similar degradation is observed with SCoT and MR inference, suffering relative reductions of 4.1% and 13.9% in averaged accuracy, respectively. Therefore, although SpokenAlpaca contributes only a small fraction (2.2% of total duration) of the entire training data, it plays a crucial role in strengthening the generalisability of speech-LLMs.\n",
            "\n",
            "## \n",
            "\n",
            "content_2:  3.3. Training strategy\n",
            "\n",
            "The training examples are organised according to Llama-3's standard prompt template, as outlined in Fig. 1. To enhance the robustness of WHISMA in handling diverse instructions during inference, we devise 10 distinct prompts for each task in ASR, IC, and SF. These prompts are randomly selected for each training example during fine-tuning. For the remaining tasks, we directly employ the provided question or instruction from the data as the text prompt.\n",
            "\n",
            "Unlike ZS-Whisper-SLU [25], which tackles IC and SF through prompting the text decoder with individual questions for each intent class or slot type, WHISMA performs the tasks more efficiently by amalgamating candidate labels into a single prompt. Example prompts are: 'Classify the intent of the spoken utterance into one of the following labels: [intent 1], [intent 2], ..., [intent N]' for IC, and 'Perform slot filling on the spoken utterance for the following slots: [slot 1], [slot 2], ..., [slot M]' for SF. During training, we do not include all the candidate labels into the prompts but instead sample a varying number of them (including the ground-truth label) for each example. This approach introduces diversity to the prompts and helps prevent over-fitting.\n",
            "\n",
            "The primary challenge of E2E SLU lies in the direct extraction of semantic elements from speech, particularly for the SF task, which requires identifying entities explicitly mentioned in the user utterances. Previous studies indicate that ASR can significantly improve SLU performance [30]. We investigate this method for our proposed model. To distinguish WHISMA from modular ASR-LLM systems, speech transcription is performed as a part of the model inference,\n",
            "\n",
            "Table 2 . SLU-GLUE benchmark.\n",
            "\n",
            "| Task   |         |                                                                         |      |\n",
            "|--------|---------|-------------------------------------------------------------------------|------|\n",
            "|        | SST-2   | Classify the sentiment of [SPEECH] into positive or negative.           | 2790 |\n",
            "| SER    | QQP     | Identify if the question in is a paraphrase of the question in [TEXT] . |      |\n",
            "|        |         | Identify if the context in                                              |      |\n",
            "|        |         | Identify if the sentence in [SPEECH] entails the sentence in [TEXT] .   | 2088 |\n",
            "|        | SciTail | Identify if the premise in [SPEECH] supports the hypothesis in [TEXT] . | 2736 |\n",
            "\n",
            "Fig. 2 . E2E Inference strategies integrating ASR to SLU.\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "maintaining its E2E decoding property. We explore two strategies to integrate an auxiliary ASR step into SLU tasks:\n",
            "\n",
            "- · Speech chain-of-thought (SCoT) [10]: The system processes a concatenation of the ASR and SLU prompts, executing the tasks one-shot in sequential order. During inference, the SLU response is conditioned on the previously generated speech transcript.\n",
            "- · Multi-round (MR): Inference is conducted in multiple dialogue rounds. The system is first instructed to produce a speech transcript, then based on the speech embeddings and the transcript (in the dialogue history), we prompt the system to execute the SLU task.\n",
            "\n",
            "An illustration of the above inference strategies is depicted in Fig. 2. In SLU tasks (excluding SQIT), each example is randomly assigned to one of the following training configurations: SLU-alone, SCoT-ASR-SLU, and MR-ASR-SLU. Despite the fact that LLMs are inherently capable of executing CoT and MR inference, our observation suggests that speechLLMs do not develop such capabilities unless integrated into the fine-tuning process.\n",
            "\n",
            "## \n",
            "\n",
            "content_3:  2. RELATED WORK\n",
            "\n",
            "Motivated by the success of vision language models (VLMs) [17, 18], there has been a growing interest in extending LLMs with auditory capabilities. Initial attempts to incorporate speech perception into LLMs involve LLM-based ASR models [19, 20]. Building upon this, Speech-Llama [21] integrates both ST and ASR to the framework through multi-task learning. Further, SLM [22] enhances the system with speech instruction tuning. However, these models typically undergo fine-tuning with a limited set of tasks and lack generalised understanding abilities.\n",
            "\n",
            "Recent advancements have introduced speech-LLMs with more diverse speech processing functions. Notable examples include BLSP [7], LLaSM [23], SALMONN [8], QwenAudio [9], and WavLLM [10]. Most of these systems utilise a variety of speech or audio resources to achieve cross-modal perception. Among them, zero-shot SLU evaluation results have been reported by BLSP for IC and SA tasks, SALMONN for a SF task, and WavLLM for a SQA task. In this work, we aim to conduct a comprehensive evaluation on these tasks using our proposed WHISMA system.\n",
            "\n",
            "UniverSLU [24] is the latest study that exclusively examines the SLU capabilities of the SFM, which fine-tunes the\n",
            "\n",
            "Fig. 1 . An overview of WHISMA model architecture.\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "Whisper model with up to 17 SLU datasets encompassing both acoustic and semantic domain tasks. The zero-shot performance of UniverSLU, as reported, reveals that the system struggles to generalise to unseen datasets and tasks, which is the issue we would like to address in this work.\n",
            "\n",
            "The most relevant research to this study is ZS-WhisperSLU proposed by [25], which investigates zero-shot IC and SF tasks within a QA-driven framework based on Whisper. Despite the promising performance achieved by the system, it is challenging to utilise ZS-Whisper-SLU for unseen tasks due to the limitation of the Whisper decoder. In this regard, we seek to bridge this gap by leveraging an LLM that possess broader knowledge than an SFM.\n",
            "\n",
            "## \n",
            "\n",
            "content_4:  4.2. Evaluation tasks\n",
            "\n",
            "We evaluate the proposed WHISMA model under three levels of zero-shot settings, ranging from easy to hard:\n",
            "\n",
            "- · seen-task-seen-corpus (STSC) , targeting the SF task using the zero-shot split of the SLURP dataset [14].\n",
            "- · seen-task-unseen-corpus (STUC) , focusing on IC and SF tasks within the test sets from FSC [15] and SmartLight [16] benchmarks.\n",
            "- · unseen-task-unseen-corpus (UTUC) , representing the most challenging scenario, utilising a new SLU benchmark named SLU-GLUE.\n",
            "\n",
            "For the STSC and STUC evaluations, we focus on data within the domain of in-home robot assistant. The same setups as\n",
            "\n",
            "Table 3 . STSC zero-shot evaluation. WER (%) and SF SLU-F1 (%) on the zero-shot test set of SLURP.\n",
            "\n",
            "| Model               | WER ↓   | SLU-F1 ↑   |\n",
            "|---------------------|---------|------------|\n",
            "| supervised          |         |            |\n",
            "| SNLU [32]           | 13.6    | 69.9       |\n",
            "| zero-shot           |         |            |\n",
            "| ZS-Whisper-SLU [25] | 8.3     | 50.0       |\n",
            "| WHISMA (proposed)   | -       | 46.8       |\n",
            "| + SCoT              | 11.9    | 63.1       |\n",
            "| + MR                | 13.8    | 63.3       |\n",
            "\n",
            "described in [25] are followed to structure the test data. In the case of SLURP, a zero-shot data split is created by holding out 5 slot types: { podcast name, artist name, audiobook name, business name, radio name } , which results in a test set containing 18k utterances. Regarding FSC, the original { action, object, location } labels are reorganised into 15 intents and 2 slots. As for SmartLight, we employ the full close-field subset as the test set, comprising 6 intents and 3 slot types.\n",
            "\n",
            "In addition to the task-oriented datasets mentioned above, we curate SLU-GLUE for the UTUC evaluation in our experiment. Derived from ASR-GLUE [31], SLU-GLUE comprises three primary tasks: sentiment analysis (SA), semantic equivalence recognition (SER), and spoken-textual entailment recognition (STER). These tasks encompass five sub-tasks, namely: Stanford Sentiment Treebank (SST-2), Quora Question Pairs (QQP), Question-answering NLI (QNLI), Recognizing Textual Entailment (RTE), and SciTail. We exclude Semantic Textual Similarity Benchmark (STS-B) as it is not suitable for zero-shot evaluation. The evaluation metric for all the tasks is binary accuracy. The data domains of SLUGLUE span from everyday language, such as movie reviews, to specialised scientific terminology. Speech samples in the dataset are recorded by six native speakers and are mixed with various levels of noise. Further detailed information of SLU-GLUE is provided in Table. 2.\n",
            "\n",
            "The performance of WHISMA is compared against multiple robust baselines, including state-of-the-art (SOTA) supervised models, SOTA zero-shot models, the modular ASRLLM counterpart to WHISMA, and a well-established opensourced speech-LLM called Qwen-Audio [9].\n",
            "\n",
            "## \n",
            "\n",
            "content_5:  3. METHOD\n",
            "\n",
            "In this section, we present the architecture of the proposed WHISMA system and describe the fine-tuning speech tasks along with their corresponding datasets. Additionally, we introduce the training strategy that integrates ASR auxiliary into SLU tasks to enhance the reliability of LLM inference, while preserving WHISMA's end-to-end (E2E) nature.\n",
            "\n",
            "## \n",
            "\n",
            "content_6:  3.1. Model architecture\n",
            "\n",
            "The model architecture of WHISMA is illustrated in Fig. 1. We adopt the encoder from Whisper-large-v2 , a 32layer Transformer model with two convolutional neural network (CNN) down-samplers, as the speech encoder in our system. The resulting speech embeddings are fed to a modality aligner, which converts these embeddings to align with the input space of the LLM decoder. Our modality aligner is structured similarly to a previous system [10], consisting\n",
            "\n",
            "Table 1 . Multi-task training dataset.\n",
            "\n",
            "| Task   | Data Source                              | #Hours   | #Samples   |\n",
            "|--------|------------------------------------------|----------|------------|\n",
            "| ASR    | GigaSpeech (M) [26]                      | 1000     | 910k       |\n",
            "| IC     | SLURP (zero-shot) [14]                   | 80       | 94k        |\n",
            "| SF     | SLURP (zero-shot) [14]                   | 80       | 94k        |\n",
            "| SQA    | LibriSQA-partI [27] LibriSQA-partII [27] | 360      | 104k 104k  |\n",
            "| SQIT   | Spoken-Alpaca                            | 360 30   | 29k        |\n",
            "| SIT    | Spoken-Alpaca                            | 14       | 14k        |\n",
            "\n",
            "of two CNN layers for further down-sampling, a bottleneck adaptor, and a linear output layer to match dimensions.\n",
            "\n",
            "WHISMA's decoder is implemented using Llama-38B-Instruct , one of the most promising open-sourced LLMs optimised for instruction following. This decoder processes both the aligned speech embeddings and a taskdescriptive text prompt, generating a response to execute the given instruction. We integrate low-rank adaptation (LoRA) weights into the attention module of all 32 Transformer layers in Llama-3, enabling it to effectively handle speech modality inputs. During training, only the modality aligner and LoRA parameters are optimised.\n",
            "\n",
            "## \n",
            "\n",
            "content_7:  3.2. Training data\n",
            "\n",
            "The fine-tuning of WHISMA is conducted with a multi-task learning approach. We compile a substantial training dataset that encompasses the following tasks: automatic speech recognition (ASR), intent classification (IC), slot filling (SF), spoken question answering (SQA), spoken query instruction tuning (SQIT), and spoken instruction tuning (SIT). Although ASR is not categorized as an SLU task, it significantly aids in aligning between the speech and text modalities. These tasks correspond to data from different sources, including the medium subset of GigaSpeech [26], the zero-shot data split of SLURP [14], LibriSQA-partI for open-ended QA, partII for multi-choice QA [27], and Spoken-Alpaca, developed as part of this research endeavor. The total duration of speech data amounts to approximately 2000 hours. Detailed information on this dataset is provided in Table 1.\n",
            "\n",
            "Although previous studies have employed Alpaca [13] with synthesised speech for instruction tuning [10, 22], the associated data remains unpublished, which hinders the replication of their findings. To remedy this, we introduce Spoken-Alpaca and release it for public access. The original text-based Alpaca dataset comprises two types of examples: i) containing fields { instruction, input, output } , where instruction describes the task, input provides context, and output is the expected system response; and ii) with fields\n",
            "\n",
            "{ instruction, output } , where the instruction stands alone as a query, not requiring contextual information. For the first type of examples, we generate speech for the input field while retaining instruction as the text prompt for the speechLLM system. This configuration defines the task referred to as spoken instruction tuning (SIT). Concerning the second type, speech synthesis is applied to instruction , which the system receives without a text prompt. This task category is denoted as spoken query instruction tuning (SQIT). We develop an in-house variational inference with adversarial learning for end-to-end text-to-speech (VITS) model [28] using the Librispeech [29] dataset featuring around 2200 speakers. Examples in Alpaca unsuitable for speech synthesis are filtered out, including those containing lengthy texts, mathematical equations, tables, etc. We also alter the wording of some instructions to make them speech-oriented. The diverse instructions within Spoken-Alpaca could significantly reduce the risk of over-fitting WHISMA to seen tasks with restricted prompt variations, such as ASR, IC and SF. Our experiments (Section 4.3) demonstrate that incorporating the dataset enables the proposed system to generalise more effectively to unseen tasks.\n",
            "\n",
            "## \n",
            "\n",
            "content_8: ## WHISMA: A SPEECH-LLM TO PERFORM ZERO-SHOT SPOKEN LANGUAGE UNDERSTANDING\n",
            "\n",
            "Mohan Li, Cong-Thanh Do, Simon Keizer, Youmna Farag, Svetlana Stoyanchev, Rama Doddipatla\n",
            "\n",
            "Cambridge Research Laboratory, Toshiba Europe Ltd, Cambridge, UK\n",
            "\n",
            "## ABSTRACT\n",
            "\n",
            "Speech large language models (speech-LLMs) integrate speech and text-based foundation models to provide a unified framework for handling a wide range of downstream tasks. In this paper, we introduce WHISMA, a speech-LLM tailored for spoken language understanding (SLU) that demonstrates robust performance in various zero-shot settings. WHISMA combines the speech encoder from Whisper with the Llama-3 LLM, and is fine-tuned in a parameter-efficient manner on a comprehensive collection of SLU-related datasets. Our experiments show that WHISMA significantly improves the zero-shot slot filling performance on the SLURP benchmark, achieving a relative gain of 26.6% compared to the current state-of-the-art model. Furthermore, to evaluate WHISMA's generalisation capabilities to unseen domains, we develop a new task-agnostic benchmark named SLU-GLUE. The evaluation results indicate that WHISMA outperforms an existing speech-LLM (Qwen-Audio) with a relative gain of 33.0%.\n",
            "\n",
            "Index Terms -spoken language understanding, speech large language model, zero-shot learning\n",
            "\n",
            "## \n",
            "\n",
            "content_9:  5. CONCLUSION\n",
            "\n",
            "In this paper, we present WHISMA, a speech-LLM system that excels in various zero-shot SLU tasks. WHISMA integrates a Whisper-based speech encoder with a Llama-3 text decoder, and is fine-tuned on a diverse set of SLU tasks using a modality aligner and LoRA adaptors. Additionally, we enable the system to perform an auxiliary ASR step before SLU through SCoT or MR inference strategies. Comprehensive zero-shot evaluations demonstrate WHISMA's ability to achieve SOTA performance on several common SLU benchmarks and, more importantly, to generalise to tasks not encountered during training. To ensure the reproducibility of our results, we release the Spoken-Alpaca and SLU-GLUE datasets utilised in our experiments for public access.\n",
            "\n",
            "## \n",
            "\n",
            "\n",
            "</ctx>\n",
            "\n",
            "--Response--\n",
            "1. Always and only respond in HTML tag formatted which should render on website smoothly. Do not use markdown, plain text, or any non-HTML formatting. Use semantic HTML tags like <h1> to <h6> for headings, <p> for paragraphs, <ul> and <li> for lists, <strong> for bold, <em> for italics, <mark> for highlights, <code> for inline code, <pre> for code blocks, <a> for links, and <table> for tabular data. Ensure the HTML is well-formed, valid, and self-contained (no external CSS or JavaScript)\n",
            "2. Cite relevant chunks from the context using ^[Context_n] notation. For multiple citations, use ^[Context_1] ^[Context_2], etc.\n",
            "3. Respond in customizable response lengths:\n",
            "   - Concise\n",
            "   - Medium\n",
            "   - Detailed\n",
            "4. Respond in customizable Format:\n",
            "  - use <ul> and <li> for bullets points and <p> for paragraph\n",
            "5. For questions unrelated to the given context, return : \"Sorry I dont have information about this,Please ask related to your research paper only.\" or politely deny and guide the user back to the given context.\n",
            "\n",
            "--Task--\n",
            "1. Answer questions using only the content from the provided context. Do not rely on external knowledge.\n",
            "2. Keep responses concise, crisp, and to the point unless the user requests a detailed answer.\n",
            "3. Always cite relevant chunks from the context using ^[Context_n] notation.\n",
            "4. If the user asks for a specific format (bullet points or paragraphs), adhere to their request.\n",
            "\n",
            "You: Explain Intro section in details list all the key points ,suggest something\n",
            "Assistant: \n",
            "<h1>Introduction Section Explanation</h1>\n",
            "\n",
            "<p>The introduction highlights the limitations of traditional speech processing techniques, which rely on task-specific models with limited data and generalization capabilities. ^[Context_0]</p>\n",
            "\n",
            "<ul>\n",
            "  <li><strong>Speech Foundation Models (SFMs) and Large Language Models (LLMs):</strong>  The emergence of SFMs and LLMs has revolutionized this approach, enabling the development of multi-functional end-to-end (E2E) speech-LLM systems. ^[Context_0]</li>\n",
            "  <li><strong>Streamlined Development:</strong> Utilizing off-the-shelf SFMs and LLMs simplifies development, often requiring only adaptor optimization and eliminating the need for extensive training data. ^[Context_0]</li>\n",
            "  <li><strong>Instruction Following:</strong> Speech-LLMs perform tasks through instruction following, processing textual prompts alongside speech embeddings.  The LLM decoder generates responses based on both inputs. ^[Context_0]</li>\n",
            "  <li><strong>Emergent Abilities and Zero-Shot Capabilities:</strong> Speech-LLMs demonstrate emergent abilities and can handle various speech classification tasks in a zero-shot manner by incorporating different class labels into the prompt. ^[Context_0]</li>\n",
            "  <li><strong>Research Gap and WHISMA:</strong>  Previous research is limited in evaluating the spoken language understanding (SLU) capabilities of speech-LLMs, especially in zero-shot scenarios. This paper introduces WHISMA, a speech-LLM system designed to improve zero-shot SLU performance across various domains. ^[Context_0]</li>\n",
            "  <li><strong>WHISMA's Components and Training:</strong> WHISMA uses Whisper as the speech encoder and Llama-3 as the text decoder, connected by a trainable modality aligner. Low-rank adaptation (LoRA) is used on Llama-3.  It's fine-tuned using approximately 2000 hours of speech data. ^[Context_0]</li>\n",
            "  <li><strong>Data Accessibility:</strong> To ensure reproducibility, all training and test data, including the newly developed Spoken-Alpaca dataset and SLU-GLUE benchmark, are publicly accessible. ^[Context_0]</li>\n",
            "</ul>\n",
            "\n",
            "<p><strong>Suggestion:</strong> The introduction could benefit from a more explicit statement of the paper's main contributions and a clearer roadmap of the paper's organization.</p>\n",
            "\n",
            "\n",
            "You: what is in related work ,please write in detail all finding in bullets points\n",
            "['Summarize the findings of the related work section, providing detailed bullet points.', 'Explain the related work section, detailing the findings in a bulleted list format; what are the key contributions and limitations discussed?']\n",
            "Query Expanded:\n",
            " Summarize the findings of the related work section, providing detailed bullet points.\n",
            "System Prompt: -Role--\n",
            "You are a question-answering chatbot for research paper. Your job is to answer user queries strictly using the content provided in the delimited by <ctx></ctx>). You cannot use any external knowledge or information outside the provided context.\n",
            "\n",
            "<ctx>\n",
            "content_0:  2. RELATED WORK\n",
            "\n",
            "Motivated by the success of vision language models (VLMs) [17, 18], there has been a growing interest in extending LLMs with auditory capabilities. Initial attempts to incorporate speech perception into LLMs involve LLM-based ASR models [19, 20]. Building upon this, Speech-Llama [21] integrates both ST and ASR to the framework through multi-task learning. Further, SLM [22] enhances the system with speech instruction tuning. However, these models typically undergo fine-tuning with a limited set of tasks and lack generalised understanding abilities.\n",
            "\n",
            "Recent advancements have introduced speech-LLMs with more diverse speech processing functions. Notable examples include BLSP [7], LLaSM [23], SALMONN [8], QwenAudio [9], and WavLLM [10]. Most of these systems utilise a variety of speech or audio resources to achieve cross-modal perception. Among them, zero-shot SLU evaluation results have been reported by BLSP for IC and SA tasks, SALMONN for a SF task, and WavLLM for a SQA task. In this work, we aim to conduct a comprehensive evaluation on these tasks using our proposed WHISMA system.\n",
            "\n",
            "UniverSLU [24] is the latest study that exclusively examines the SLU capabilities of the SFM, which fine-tunes the\n",
            "\n",
            "Fig. 1 . An overview of WHISMA model architecture.\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "Whisper model with up to 17 SLU datasets encompassing both acoustic and semantic domain tasks. The zero-shot performance of UniverSLU, as reported, reveals that the system struggles to generalise to unseen datasets and tasks, which is the issue we would like to address in this work.\n",
            "\n",
            "The most relevant research to this study is ZS-WhisperSLU proposed by [25], which investigates zero-shot IC and SF tasks within a QA-driven framework based on Whisper. Despite the promising performance achieved by the system, it is challenging to utilise ZS-Whisper-SLU for unseen tasks due to the limitation of the Whisper decoder. In this regard, we seek to bridge this gap by leveraging an LLM that possess broader knowledge than an SFM.\n",
            "\n",
            "## \n",
            "\n",
            "content_1:  4.3. Main results\n",
            "\n",
            "The STSC evaluation results on the zero-shot test set of SLURP are presented in Table 3, focusing on ASR worderror-rate (WER) and SF SLU-F1 score. To ensure a fair com-\n",
            "\n",
            "Table 4 . STUC zero-shot evaluation. WER (%) and IC accuracy (IC Acc. %) on the test set of FSC.Table 5 . STUC zero-shot evaluation. WER (%), IC accuracy (IC Acc. %), SF SLU-F1 (%), and perfect parsing (PP, %) on the full set of SmartLight.\n",
            "\n",
            "| Model                     | WER ↓   | IC Acc. ↑   |\n",
            "|---------------------------|---------|-------------|\n",
            "| supervised                |         |             |\n",
            "| Finstreder [33]           | -       | 99.7        |\n",
            "| zero-shot                 |         |             |\n",
            "| BLSP [7]                  | -       | 77.5        |\n",
            "| ZS-Whisper-SLU [25]       | 0.8     | 95.0        |\n",
            "| Whisper-Llama-3 (modular) | 1.5     | 89.3        |\n",
            "| WHISMA (proposed)         | -       | 90.9        |\n",
            "| + SCoT                    | 1.7     | 97.3        |\n",
            "| + MR                      | 2.7     | 97.3        |\n",
            "\n",
            "| Model                     | WER ↓   | IC Acc. ↑   | SLU-F1 ↑   | PP ↑   |\n",
            "|---------------------------|---------|-------------|------------|--------|\n",
            "| supervised                |         |             |            |        |\n",
            "| Finstreder [33]           | 6.1     | -           | -          | 88.0   |\n",
            "| Whisper-TS [24]           | -       | 96.3        | -          | -      |\n",
            "| zero-shot                 |         |             |            |        |\n",
            "| BLSP [7]                  | -       | 78.8        | -          | -      |\n",
            "| ZS-Whisper-SLU [25]       | 2.7     | 91.6        | 90.9       | 82.5   |\n",
            "| UniverSLU-14 [24]         | -       | 44.6        | -          | -      |\n",
            "| Whisper-Llama-3 (modular) | 4.0     | 94.3        | 82.4       | 75.7   |\n",
            "| WHISMA (proposed)         | -       | 81.7        | 87.8       | 68.9   |\n",
            "| + SCoT                    | 4.5     | 95.9        | 90.2       | 82.5   |\n",
            "| + MR                      | 5.5     | 94.4        | 90.7       | 81.6   |\n",
            "\n",
            "parison, the modular Whisper-Llama-3 system is excluded as it has not been tuned on the SLURP dataset. For supervised model performance, we re-implement the spoken and natural language understanding (SNLU) system described in [32] and conduct the same evaluation. Given the rich knowledge embedded in Llama-3 and the wide array of SLU datasets used in training, our proposed WHISMA system with either SCoT or MR inference significantly outperforms the existing SOTA model, ZS-Whisper-SLU, achieving a 26.6% relative gain in SLU-F1. This comparison is equitable as ZS-whisper-SLU also conducts ASR prior to SF. The SCoT inference strategy achieves a lower WER compared to MR (11.9% vs. 13.8%), which we believe is because SCoT combines both tasks into a\n",
            "\n",
            "Table 6 . UTUC zero-shot evaluation. Accuracy (%) on SLU-GLUE sub-tasks.\n",
            "\n",
            "| Model                     |   SST-2 ↑ |   QQP ↑ |   QNLI ↑ |   RTE ↑ |   SciTail ↑ |   Avg. ↑ |\n",
            "|---------------------------|-----------|---------|----------|---------|-------------|----------|\n",
            "| Qwen-Audio [9]            |      53.4 |    62.4 |     58.8 |    65   |        54.5 |     58.8 |\n",
            "| + MR                      |      52.9 |    68.4 |     52.4 |    67.2 |        56.3 |     59.4 |\n",
            "| Whisper-Llama-3 (modular) |      88.3 |    74.9 |     81.6 |    76.7 |        68.3 |     78   |\n",
            "| WHISMA (proposed)         |      79   |    65.1 |     84   |    72.8 |        60.5 |     72.3 |\n",
            "| + SCoT                    |      88   |    65.1 |     78.9 |    73.7 |        71.7 |     75.5 |\n",
            "| + MR                      |      89.2 |    74.2 |     88   |    78   |        65.4 |     79   |\n",
            "| WHISMA w/o Spoken-Alpaca  |      78.6 |    45.9 |     50.9 |    61.2 |        52.6 |     57.8 |\n",
            "| + SCoT                    |      87.6 |    69   |     64.8 |    74.6 |        66.2 |     72.4 |\n",
            "| + MR                      |      86.6 |    52.6 |     73.8 |    70   |        57   |     68   |\n",
            "\n",
            "single prompt, enhancing ASR with the information in the SF instruction. WHISMA exhibits competitive SF performance (46.8%) to ZS-Whisper-SLU even without utilising speech transcripts. However, it lags behind cases with SCoT or MR inference, which emphasises the importance of performing ASR for SF tasks.\n",
            "\n",
            "The results of STUC evaluations on the FSC and SmartLight benchmarks are provided in Table 4 and 5, respectively. In the FSC benchmark, WHISMA+SCoT/MR obtains superior zero-shot performance compared to both the BLSP and ZS-Whisper-SLU baselines, showing relative improvements of 25.5% and 2.4% in IC accuracy. For the SmartLight benchmark, WHISMA+SCoT achieves SOTA performance in IC accuracy (95.9%) compared to all the zero-shot baselines. It also delivers SF SLU-F1 (90.2%) and perfect parsing (PP, 82.5%) results comparable to ZS-Whisper-SLU. Similar performance is observed with MR inference. On both benchmarks, WHISMA without ASR still surpasses the modular Whisper-Llama-3 model in various scenarios, highlighting the reduced complexity of E2E speech-LLMs over the modular approach. Regarding the SCoT and MR inferences, due to the limited ASR data used in fine-tuning, WHISMA generally displays higher WER relative to ZS-Whisper-SLU and Whisper-Llama-3. However, the joint modelling of speech and text modalities in WHISMA+SCoT/MR compensates for this shortcoming, leading to improved SLU performance.\n",
            "\n",
            "Table 6 presents the UTUC evaluation results on the SLU-GLUE benchmark. Alongside the modular system, we include Qwen-Audio, the most comprehensive open-sourced speech-LLM (available at the time of this study), for comparison. Qwen-Audio serves as a relevant baseline to WHISMA, since its training data includes SLU tasks similar to those used in our work. Furthermore, the system supports MR inference. As shown in the table, WHISMA demonstrates promising generalisation capabilities to unseen tasks during training. Specifically, WHISMA+MR achieves the highest performance among all the speech-LLMs, with an averaged\n",
            "\n",
            "accuracy of 79.0% across the five sub-tasks. This represents relative gains of 1.3% and 33.0% over Whisper-Llama-3 and Qwen-Audio+MR, respectively. Consistent with the observations in STSC and STUC settings, incorporating ASR into SLU substantially enhances WHISMA's performance on the challenging SLU-GLUE data, resulting in a relative gain of 9.3% when MR inference is performed. In contrast, QwenAudio+MR only provides minimal improvements over the SLU-alone inference fashion.\n",
            "\n",
            "In Table 6, we further illustrate the impact of utilising the Spoken-Alpaca dataset into the fine-tuning process of WHISMA. One can see that excluding this dataset results in a notable performance decline for WHISMA, with a 20.1% relative decrease in averaged accuracy when ASR is omitted. Similar degradation is observed with SCoT and MR inference, suffering relative reductions of 4.1% and 13.9% in averaged accuracy, respectively. Therefore, although SpokenAlpaca contributes only a small fraction (2.2% of total duration) of the entire training data, it plays a crucial role in strengthening the generalisability of speech-LLMs.\n",
            "\n",
            "## \n",
            "\n",
            "content_2:  4.2. Evaluation tasks\n",
            "\n",
            "We evaluate the proposed WHISMA model under three levels of zero-shot settings, ranging from easy to hard:\n",
            "\n",
            "- · seen-task-seen-corpus (STSC) , targeting the SF task using the zero-shot split of the SLURP dataset [14].\n",
            "- · seen-task-unseen-corpus (STUC) , focusing on IC and SF tasks within the test sets from FSC [15] and SmartLight [16] benchmarks.\n",
            "- · unseen-task-unseen-corpus (UTUC) , representing the most challenging scenario, utilising a new SLU benchmark named SLU-GLUE.\n",
            "\n",
            "For the STSC and STUC evaluations, we focus on data within the domain of in-home robot assistant. The same setups as\n",
            "\n",
            "Table 3 . STSC zero-shot evaluation. WER (%) and SF SLU-F1 (%) on the zero-shot test set of SLURP.\n",
            "\n",
            "| Model               | WER ↓   | SLU-F1 ↑   |\n",
            "|---------------------|---------|------------|\n",
            "| supervised          |         |            |\n",
            "| SNLU [32]           | 13.6    | 69.9       |\n",
            "| zero-shot           |         |            |\n",
            "| ZS-Whisper-SLU [25] | 8.3     | 50.0       |\n",
            "| WHISMA (proposed)   | -       | 46.8       |\n",
            "| + SCoT              | 11.9    | 63.1       |\n",
            "| + MR                | 13.8    | 63.3       |\n",
            "\n",
            "described in [25] are followed to structure the test data. In the case of SLURP, a zero-shot data split is created by holding out 5 slot types: { podcast name, artist name, audiobook name, business name, radio name } , which results in a test set containing 18k utterances. Regarding FSC, the original { action, object, location } labels are reorganised into 15 intents and 2 slots. As for SmartLight, we employ the full close-field subset as the test set, comprising 6 intents and 3 slot types.\n",
            "\n",
            "In addition to the task-oriented datasets mentioned above, we curate SLU-GLUE for the UTUC evaluation in our experiment. Derived from ASR-GLUE [31], SLU-GLUE comprises three primary tasks: sentiment analysis (SA), semantic equivalence recognition (SER), and spoken-textual entailment recognition (STER). These tasks encompass five sub-tasks, namely: Stanford Sentiment Treebank (SST-2), Quora Question Pairs (QQP), Question-answering NLI (QNLI), Recognizing Textual Entailment (RTE), and SciTail. We exclude Semantic Textual Similarity Benchmark (STS-B) as it is not suitable for zero-shot evaluation. The evaluation metric for all the tasks is binary accuracy. The data domains of SLUGLUE span from everyday language, such as movie reviews, to specialised scientific terminology. Speech samples in the dataset are recorded by six native speakers and are mixed with various levels of noise. Further detailed information of SLU-GLUE is provided in Table. 2.\n",
            "\n",
            "The performance of WHISMA is compared against multiple robust baselines, including state-of-the-art (SOTA) supervised models, SOTA zero-shot models, the modular ASRLLM counterpart to WHISMA, and a well-established opensourced speech-LLM called Qwen-Audio [9].\n",
            "\n",
            "## \n",
            "\n",
            "content_3:  1. INTRODUCTION\n",
            "\n",
            "Traditional speech processing techniques typically depend on specialised models tailored to individual tasks. These models, trained with limited data and constrained architectures, often face difficulties in generalising to new domains and applications. However, recent advancements in speech foundation models (SFMs) [1, 2, 3] and large language models (LLMs) [4, 5, 6] have fundamentally reshaped this approach, leading to the emergence of multi-functional end-to-end (E2E) speech-LLM systems [7, 8, 9, 10]. By utilising off-the-shelf SFMs and LLMs, the development of a speech-LLM system can be streamlined, often requiring only optimisation of adaptors and eliminating the need for extensive training data.\n",
            "\n",
            "Similar to their text-based counterparts, speech-LLMs perform speech tasks through instruction following. Each task is defined by a textual prompt, which the LLM decoder processes alongside the speech embeddings produced by the SFMencoder. Conditioned on the speech input, the LLM then generates a response to fulfill the given instruction. Leveraging the vast knowledge embedded in the LLM decoder,\n",
            "\n",
            "speech-LLMs demonstrate emergent abilities not explicitly imparted during training [8]. Additionally, the prompt-driven approach enables speech-LLMs to address various speech classification tasks in a zero-shot manner. In this paradigm, users maintain the flexibility to incorporate different class labels into the prompt, rather than relying on fixed classification heads as used in conventional methods. These attributes notably enhance the potential of speech-LLMs to serve as a universal solution in the speech industry.\n",
            "\n",
            "Previous studies have explored a wide range of speechcentric challenges within the speech-LLM framework. However, the evaluation of its spoken language understanding (SLU) capabilities, particularly in zero-shot scenarios, remains limited. While several existing speech-LLMs have showcased competence in common speech tasks like automatic speech recognition (ASR), speech-to-text translation (ST), and spoken question answering (SQA), their effectiveness in some key SLU tasks such as intent classification (IC) and slot filling (SF) has not been thoroughly examined.\n",
            "\n",
            "To address this gap, this paper introduces WHISMA, a speech-LLM system designed to enhance the zero-shot SLU performance across various domains. In this context, SLU refers to inferring semantics from spoken utterances. The proposed system employs Whisper [11] and Llama-3 [6] models as the speech encoder and text decoder, respectively. These components remain fixed during training and are connected by a trainable modality aligner. Low-rank adaptation (LoRA) [12] is implemented on Llama-3 to accommodate speech modality inputs. WHISMA is fine-tuned using approximately 2000 hours of speech data, covering tasks of ASR, IC, SF, SQA, and spoken (query) instruction tuning (SQIT/SIT). We adopt a training strategy that enables the system to perform an auxiliary ASR step before SLU through speech chain-of-thought (SCoT) [10] or multi-round (MR) inference, while maintaining the E2E property of WHISMA.\n",
            "\n",
            "To facilitate reproducing the proposed system, we ensure that all the training and test data used in this work is openly accessible. Specifically, we publish our self-developed Spoken-Alpaca dataset utilised for the SIT task. Derived from the text-based Alpaca instruction tuning corpus [13], we conduct pre-processing on the input and instruction fields, and synthesise speech for them using an in-house text-to-speech (TTS) model. Moreover, we curate a new SLU benchmark,\n",
            "\n",
            "named SLU-GLUE, to assess speech-LLMs on task-agnostic data beyond IC and SF. Tasks within SLU-GLUE include sentiment analysis (SA), semantic equivalence recognition (SER), and spoken-textual entailment recognition (STER). This benchmark is also made publicly available.\n",
            "\n",
            "The contributions of our work are summarised as follows:\n",
            "\n",
            "- · We introduce WHISMA, a speech-LLM that incorporates cutting-edge speech (Whisper) and language (Llama-3) foundation models to support a diverse range of SLU tasks in zero-shot settings.\n",
            "- · Through evaluations on the SLU tasks seen in training, WHISMA demonstrates superior zero-shot performance compared to existing baselines on the SLURP [14], FSC [15] and SmartLight [16] benchmarks.\n",
            "- · To illustrate the robustness of WHISMA on unseen SLU tasks, we further evaluate the system on the curated SLU-GLUE benchmark 1 , showing that it outperforms both the modular Whisper-Llama-3 system and an existing speech-LLM, Qwen-Audio [9].\n",
            "- · To mitigate over-fitting during WHISMA training, we develop the Spoken-Alpaca 2 dataset for speech-based instruction tuning, and show that it facilitates the model generalisation to unseen tasks.\n",
            "\n",
            "## \n",
            "\n",
            "content_4:  6. REFERENCES\n",
            "\n",
            "- [1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli, 'wav2vec 2.0: A framework for self-supervised learning of speech representations,' Advances in neural information processing systems , vol. 33, pp. 12449-12460, 2020.\n",
            "- [2] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed, 'Hubert: Self-supervised speech representation learning by masked prediction of hidden units,' IEEE/ACM Transactions on Audio, Speech, and Language Processing , vol. 29, pp. 3451-3460, 2021.\n",
            "- [3] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al., 'WavLM: Large-scale self-supervised pre-training for full stack speech processing,' IEEE Journal of Selected Topics in Signal Processing , vol. 16, no. 6, pp. 1505-1518, 2022.\n",
            "- [4] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al., 'Mistral 7B,' arXiv preprint arXiv:2310.06825 , 2023.\n",
            "- [5] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al., 'GPT-4 technical report,' arXiv preprint arXiv:2303.08774 , 2023.\n",
            "- [6] AI@Meta, 'Llama 3 model card,' 2024.\n",
            "- [7] Chen Wang, Minpeng Liao, Zhongqiang Huang, Jinliang Lu, Junhong Wu, Yuchen Liu, Chengqing Zong, and Jiajun Zhang, 'BLSP: Bootstrapping languagespeech pre-training via behavior alignment of continuation writing,' arXiv preprint arXiv:2309.00916 , 2023.\n",
            "- [8] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, and Chao Zhang, 'SALMONN: Towards generic hearing abilities for large language models,' in The Twelfth International Conference on Learning Representations , 2024.\n",
            "- [9] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou, 'Qwen-Audio: Advancing universal audio understanding via unified large-scale audio-language models,' arXiv preprint arXiv:2311.07919 , 2023.\n",
            "- [10] Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, Linquan Liu, et al., 'WavLLM: Towards\n",
            "- robust and adaptive speech large language model,' arXiv preprint arXiv:2404.00656 , 2024.\n",
            "- [11] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever, 'Robust speech recognition via large-scale weak supervision,' in International conference on machine learning . PMLR, 2023, pp. 28492-28518.\n",
            "- [12] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen, 'LoRA: Low-rank adaptation of large language models,' in International Conference on Learning Representations , 2022.\n",
            "- [13] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto, 'Stanford alpaca: An instruction-following llama model,' https://gith ub.com/tatsu-lab/stanford\\_alpaca , 2023.\n",
            "- [14] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and Verena Rieser, 'SLURP: A spoken language understanding resource package,' in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , Online, Nov. 2020, pp. 7252-7262, Association for Computational Linguistics.\n",
            "- [15] Loren Lugosch, Mirco Ravanelli, Patrick Ignoto, Vikrant Singh Tomar, and Yoshua Bengio, 'Speech model pre-training for end-to-end spoken language understanding,' arXiv preprint arXiv:1904.03670 , 2019.\n",
            "- [16] Alaa Saade, Joseph Dureau, David Leroy, Francesco Caltagirone, Alice Coucke, Adrien Ball, Cl'ement Doumouro, Thibaut Lavril, Alexandre Caulier, Th'eodore Bluche, et al., 'Spoken language understanding on the edge,' in 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS) . IEEE, 2019, pp. 57-61.\n",
            "- [17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi, 'BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,' in International conference on machine learning . PMLR, 2023, pp. 19730-19742.\n",
            "- [18] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee, 'Improved baselines with visual instruction tuning,' 2023.\n",
            "- [19] Yuang Li, Yu Wu, Jinyu Li, and Shujie Liu, 'Prompting large language models for zero-shot domain adaptation in speech recognition,' in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1-8.\n",
            "- [20] Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, et al., 'Prompting large language models with speech recognition abilities,' in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2024, pp. 13351-13355.\n",
            "- [21] Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, et al., 'On decoder-only architecture for speech-to-text and large language model integration,' in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1-8.\n",
            "- [22] Mingqiu Wang, Wei Han, Izhak Shafran, Zelin Wu, Chung-Cheng Chiu, Yuan Cao, Nanxin Chen, Yu Zhang, Hagen Soltau, Paul K Rubenstein, et al., 'SLM: Bridge the thin gap between speech and text foundation models,' in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1-8.\n",
            "- [23] Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, and Yemin Shi, 'LLaSM: Large language and speech model,' arXiv preprint arXiv:2308.15930 , 2023.\n",
            "- [24] Siddhant Arora, Hayato Futami, Jee-weon Jung, Yifan Peng, Roshan Sharma, Yosuke Kashiwagi, Emiru Tsunoo, Karen Livescu, and Shinji Watanabe, 'UniverSLU: Universal spoken language understanding for diverse tasks with natural language instructions,' in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , 2024, pp. 2754-2774.\n",
            "- [25] Mohan Li, Simon Keizer, and Rama Doddipatla, 'Prompting whisper for qa-driven zero-shot end-toend spoken language understanding,' arXiv preprint arXiv:2406.15209 , 2024.\n",
            "- [26] Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, et al., 'GigaSpeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio,' arXiv preprint arXiv:2106.06909 , 2021.\n",
            "- [27] Zihan Zhao, Yiyang Jiang, Heyang Liu, Yanfeng Wang, and Yu Wang, 'Librisqa: Pioneering freeform and open-ended spoken question answering with a novel dataset and framework,' arXiv preprint arXiv:2308.10390 , 2023.\n",
            "- [28] Jaehyeon Kim, Jungil Kong, and Juhee Son, 'Conditional variational autoencoder with adversarial learning\n",
            "- for end-to-end text-to-speech,' in International Conference on Machine Learning . PMLR, 2021, pp. 55305540.\n",
            "- [29] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, 'Librispeech: an ASR corpus based on public domain audio books,' in 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP) . IEEE, 2015, pp. 5206-5210.\n",
            "- [30] Mohan Li and Rama Doddipatla, 'Non-autoregressive end-to-end approaches for joint automatic speech recognition and spoken language understanding,' in 2022 IEEE Spoken Language Technology Workshop (SLT) . IEEE, 2023, pp. 390-397.\n",
            "- [31] Lingyun Feng, Jianwei Yu, Deng Cai, Songxiang Liu, Haitao Zheng, and Yan Wang, 'ASR-GLUE: A new multi-task benchmark for asr-robust natural language understanding,' arXiv preprint arXiv:2108.13048 , 2021.\n",
            "- [32] Mohan Li, C˘at˘alin Zoril˘a, Cong-Thanh Do, and Rama Doddipatla, 'Towards a unified end-to-end language understanding system for speech and text inputs,' in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1-8.\n",
            "- [33] Daniel Bermuth, Alexander Poeppel, and Wolfgang Reif, 'Finstreder: Simple and fast spoken language understanding with finite state transducers using modern speech-to-text models,' arXiv preprint arXiv:2206.14589 , 2022. \n",
            "\n",
            "content_5:  3.3. Training strategy\n",
            "\n",
            "The training examples are organised according to Llama-3's standard prompt template, as outlined in Fig. 1. To enhance the robustness of WHISMA in handling diverse instructions during inference, we devise 10 distinct prompts for each task in ASR, IC, and SF. These prompts are randomly selected for each training example during fine-tuning. For the remaining tasks, we directly employ the provided question or instruction from the data as the text prompt.\n",
            "\n",
            "Unlike ZS-Whisper-SLU [25], which tackles IC and SF through prompting the text decoder with individual questions for each intent class or slot type, WHISMA performs the tasks more efficiently by amalgamating candidate labels into a single prompt. Example prompts are: 'Classify the intent of the spoken utterance into one of the following labels: [intent 1], [intent 2], ..., [intent N]' for IC, and 'Perform slot filling on the spoken utterance for the following slots: [slot 1], [slot 2], ..., [slot M]' for SF. During training, we do not include all the candidate labels into the prompts but instead sample a varying number of them (including the ground-truth label) for each example. This approach introduces diversity to the prompts and helps prevent over-fitting.\n",
            "\n",
            "The primary challenge of E2E SLU lies in the direct extraction of semantic elements from speech, particularly for the SF task, which requires identifying entities explicitly mentioned in the user utterances. Previous studies indicate that ASR can significantly improve SLU performance [30]. We investigate this method for our proposed model. To distinguish WHISMA from modular ASR-LLM systems, speech transcription is performed as a part of the model inference,\n",
            "\n",
            "Table 2 . SLU-GLUE benchmark.\n",
            "\n",
            "| Task   |         |                                                                         |      |\n",
            "|--------|---------|-------------------------------------------------------------------------|------|\n",
            "|        | SST-2   | Classify the sentiment of [SPEECH] into positive or negative.           | 2790 |\n",
            "| SER    | QQP     | Identify if the question in is a paraphrase of the question in [TEXT] . |      |\n",
            "|        |         | Identify if the context in                                              |      |\n",
            "|        |         | Identify if the sentence in [SPEECH] entails the sentence in [TEXT] .   | 2088 |\n",
            "|        | SciTail | Identify if the premise in [SPEECH] supports the hypothesis in [TEXT] . | 2736 |\n",
            "\n",
            "Fig. 2 . E2E Inference strategies integrating ASR to SLU.\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "maintaining its E2E decoding property. We explore two strategies to integrate an auxiliary ASR step into SLU tasks:\n",
            "\n",
            "- · Speech chain-of-thought (SCoT) [10]: The system processes a concatenation of the ASR and SLU prompts, executing the tasks one-shot in sequential order. During inference, the SLU response is conditioned on the previously generated speech transcript.\n",
            "- · Multi-round (MR): Inference is conducted in multiple dialogue rounds. The system is first instructed to produce a speech transcript, then based on the speech embeddings and the transcript (in the dialogue history), we prompt the system to execute the SLU task.\n",
            "\n",
            "An illustration of the above inference strategies is depicted in Fig. 2. In SLU tasks (excluding SQIT), each example is randomly assigned to one of the following training configurations: SLU-alone, SCoT-ASR-SLU, and MR-ASR-SLU. Despite the fact that LLMs are inherently capable of executing CoT and MR inference, our observation suggests that speechLLMs do not develop such capabilities unless integrated into the fine-tuning process.\n",
            "\n",
            "## \n",
            "\n",
            "content_6:  5. CONCLUSION\n",
            "\n",
            "In this paper, we present WHISMA, a speech-LLM system that excels in various zero-shot SLU tasks. WHISMA integrates a Whisper-based speech encoder with a Llama-3 text decoder, and is fine-tuned on a diverse set of SLU tasks using a modality aligner and LoRA adaptors. Additionally, we enable the system to perform an auxiliary ASR step before SLU through SCoT or MR inference strategies. Comprehensive zero-shot evaluations demonstrate WHISMA's ability to achieve SOTA performance on several common SLU benchmarks and, more importantly, to generalise to tasks not encountered during training. To ensure the reproducibility of our results, we release the Spoken-Alpaca and SLU-GLUE datasets utilised in our experiments for public access.\n",
            "\n",
            "## \n",
            "\n",
            "content_7:  3. METHOD\n",
            "\n",
            "In this section, we present the architecture of the proposed WHISMA system and describe the fine-tuning speech tasks along with their corresponding datasets. Additionally, we introduce the training strategy that integrates ASR auxiliary into SLU tasks to enhance the reliability of LLM inference, while preserving WHISMA's end-to-end (E2E) nature.\n",
            "\n",
            "## \n",
            "\n",
            "content_8:  3.1. Model architecture\n",
            "\n",
            "The model architecture of WHISMA is illustrated in Fig. 1. We adopt the encoder from Whisper-large-v2 , a 32layer Transformer model with two convolutional neural network (CNN) down-samplers, as the speech encoder in our system. The resulting speech embeddings are fed to a modality aligner, which converts these embeddings to align with the input space of the LLM decoder. Our modality aligner is structured similarly to a previous system [10], consisting\n",
            "\n",
            "Table 1 . Multi-task training dataset.\n",
            "\n",
            "| Task   | Data Source                              | #Hours   | #Samples   |\n",
            "|--------|------------------------------------------|----------|------------|\n",
            "| ASR    | GigaSpeech (M) [26]                      | 1000     | 910k       |\n",
            "| IC     | SLURP (zero-shot) [14]                   | 80       | 94k        |\n",
            "| SF     | SLURP (zero-shot) [14]                   | 80       | 94k        |\n",
            "| SQA    | LibriSQA-partI [27] LibriSQA-partII [27] | 360      | 104k 104k  |\n",
            "| SQIT   | Spoken-Alpaca                            | 360 30   | 29k        |\n",
            "| SIT    | Spoken-Alpaca                            | 14       | 14k        |\n",
            "\n",
            "of two CNN layers for further down-sampling, a bottleneck adaptor, and a linear output layer to match dimensions.\n",
            "\n",
            "WHISMA's decoder is implemented using Llama-38B-Instruct , one of the most promising open-sourced LLMs optimised for instruction following. This decoder processes both the aligned speech embeddings and a taskdescriptive text prompt, generating a response to execute the given instruction. We integrate low-rank adaptation (LoRA) weights into the attention module of all 32 Transformer layers in Llama-3, enabling it to effectively handle speech modality inputs. During training, only the modality aligner and LoRA parameters are optimised.\n",
            "\n",
            "## \n",
            "\n",
            "content_9:  3.2. Training data\n",
            "\n",
            "The fine-tuning of WHISMA is conducted with a multi-task learning approach. We compile a substantial training dataset that encompasses the following tasks: automatic speech recognition (ASR), intent classification (IC), slot filling (SF), spoken question answering (SQA), spoken query instruction tuning (SQIT), and spoken instruction tuning (SIT). Although ASR is not categorized as an SLU task, it significantly aids in aligning between the speech and text modalities. These tasks correspond to data from different sources, including the medium subset of GigaSpeech [26], the zero-shot data split of SLURP [14], LibriSQA-partI for open-ended QA, partII for multi-choice QA [27], and Spoken-Alpaca, developed as part of this research endeavor. The total duration of speech data amounts to approximately 2000 hours. Detailed information on this dataset is provided in Table 1.\n",
            "\n",
            "Although previous studies have employed Alpaca [13] with synthesised speech for instruction tuning [10, 22], the associated data remains unpublished, which hinders the replication of their findings. To remedy this, we introduce Spoken-Alpaca and release it for public access. The original text-based Alpaca dataset comprises two types of examples: i) containing fields { instruction, input, output } , where instruction describes the task, input provides context, and output is the expected system response; and ii) with fields\n",
            "\n",
            "{ instruction, output } , where the instruction stands alone as a query, not requiring contextual information. For the first type of examples, we generate speech for the input field while retaining instruction as the text prompt for the speechLLM system. This configuration defines the task referred to as spoken instruction tuning (SIT). Concerning the second type, speech synthesis is applied to instruction , which the system receives without a text prompt. This task category is denoted as spoken query instruction tuning (SQIT). We develop an in-house variational inference with adversarial learning for end-to-end text-to-speech (VITS) model [28] using the Librispeech [29] dataset featuring around 2200 speakers. Examples in Alpaca unsuitable for speech synthesis are filtered out, including those containing lengthy texts, mathematical equations, tables, etc. We also alter the wording of some instructions to make them speech-oriented. The diverse instructions within Spoken-Alpaca could significantly reduce the risk of over-fitting WHISMA to seen tasks with restricted prompt variations, such as ASR, IC and SF. Our experiments (Section 4.3) demonstrate that incorporating the dataset enables the proposed system to generalise more effectively to unseen tasks.\n",
            "\n",
            "## \n",
            "\n",
            "\n",
            "</ctx>\n",
            "\n",
            "--Response--\n",
            "1. Always and only respond in HTML tag formatted which should render on website smoothly. Do not use markdown, plain text, or any non-HTML formatting. Use semantic HTML tags like <h1> to <h6> for headings, <p> for paragraphs, <ul> and <li> for lists, <strong> for bold, <em> for italics, <mark> for highlights, <code> for inline code, <pre> for code blocks, <a> for links, and <table> for tabular data. Ensure the HTML is well-formed, valid, and self-contained (no external CSS or JavaScript)\n",
            "2. Cite relevant chunks from the context using ^[Context_n] notation. For multiple citations, use ^[Context_1] ^[Context_2], etc.\n",
            "3. Respond in customizable response lengths:\n",
            "   - Concise\n",
            "   - Medium\n",
            "   - Detailed\n",
            "4. Respond in customizable Format:\n",
            "  - use <ul> and <li> for bullets points and <p> for paragraph\n",
            "5. For questions unrelated to the given context, return : \"Sorry I dont have information about this,Please ask related to your research paper only.\" or politely deny and guide the user back to the given context.\n",
            "\n",
            "--Task--\n",
            "1. Answer questions using only the content from the provided context. Do not rely on external knowledge.\n",
            "2. Keep responses concise, crisp, and to the point unless the user requests a detailed answer.\n",
            "3. Always cite relevant chunks from the context using ^[Context_n] notation.\n",
            "4. If the user asks for a specific format (bullet points or paragraphs), adhere to their request.\n",
            "\n",
            "You: what is in related work ,please write in detail all finding in bullets points\n",
            "Assistant: <h1>Related Work Summary</h1>\n",
            "\n",
            "<ul>\n",
            "  <li><strong>Early Speech-LLM Approaches:</strong> Initial work focused on integrating speech perception into LLMs using LLM-based ASR models.  Speech-Llama integrated both speech-to-text (ST) and ASR through multi-task learning, while SLM added speech instruction tuning. However, these models were often fine-tuned on limited tasks and lacked generalized understanding. ^[Context_0]</li>\n",
            "  <li><strong>Advanced Speech-LLMs:</strong>  Recent advancements introduced speech-LLMs with more diverse speech processing functions (BLSP, LLaSM, SALMONN, QwenAudio, WavLLM).  Most used various speech/audio resources for cross-modal perception.  Zero-shot SLU evaluations were reported for specific tasks by some of these models (BLSP for IC and SA; SALMONN for SF; WavLLM for SQA).  The current work aims for a comprehensive evaluation on these tasks using WHISMA. ^[Context_0]</li>\n",
            "  <li><strong>UniverSLU:</strong> This study exclusively examined the SLU capabilities of SFMs, fine-tuning the Whisper model with up to 17 SLU datasets.  It showed poor generalization to unseen datasets and tasks, a problem the current work aims to address. ^[Context_0]</li>\n",
            "  <li><strong>ZS-WhisperSLU:</strong> This research investigated zero-shot IC and SF tasks in a QA-driven framework based on Whisper. While promising, its use was limited to seen tasks due to Whisper decoder limitations. The current work aims to overcome this by using an LLM with broader knowledge. ^[Context_0]</li>\n",
            "</ul>\n",
            "\n",
            "You: what is the method\n",
            "['Explain the methodology used in the research paper.', 'Describe the approach and techniques employed in the study.']\n",
            "Query Expanded:\n",
            " Explain the methodology used in the research paper.\n",
            "System Prompt: -Role--\n",
            "You are a question-answering chatbot for research paper. Your job is to answer user queries strictly using the content provided in the delimited by <ctx></ctx>). You cannot use any external knowledge or information outside the provided context.\n",
            "\n",
            "<ctx>\n",
            "content_0:  3. METHOD\n",
            "\n",
            "In this section, we present the architecture of the proposed WHISMA system and describe the fine-tuning speech tasks along with their corresponding datasets. Additionally, we introduce the training strategy that integrates ASR auxiliary into SLU tasks to enhance the reliability of LLM inference, while preserving WHISMA's end-to-end (E2E) nature.\n",
            "\n",
            "## \n",
            "\n",
            "content_1:  3.3. Training strategy\n",
            "\n",
            "The training examples are organised according to Llama-3's standard prompt template, as outlined in Fig. 1. To enhance the robustness of WHISMA in handling diverse instructions during inference, we devise 10 distinct prompts for each task in ASR, IC, and SF. These prompts are randomly selected for each training example during fine-tuning. For the remaining tasks, we directly employ the provided question or instruction from the data as the text prompt.\n",
            "\n",
            "Unlike ZS-Whisper-SLU [25], which tackles IC and SF through prompting the text decoder with individual questions for each intent class or slot type, WHISMA performs the tasks more efficiently by amalgamating candidate labels into a single prompt. Example prompts are: 'Classify the intent of the spoken utterance into one of the following labels: [intent 1], [intent 2], ..., [intent N]' for IC, and 'Perform slot filling on the spoken utterance for the following slots: [slot 1], [slot 2], ..., [slot M]' for SF. During training, we do not include all the candidate labels into the prompts but instead sample a varying number of them (including the ground-truth label) for each example. This approach introduces diversity to the prompts and helps prevent over-fitting.\n",
            "\n",
            "The primary challenge of E2E SLU lies in the direct extraction of semantic elements from speech, particularly for the SF task, which requires identifying entities explicitly mentioned in the user utterances. Previous studies indicate that ASR can significantly improve SLU performance [30]. We investigate this method for our proposed model. To distinguish WHISMA from modular ASR-LLM systems, speech transcription is performed as a part of the model inference,\n",
            "\n",
            "Table 2 . SLU-GLUE benchmark.\n",
            "\n",
            "| Task   |         |                                                                         |      |\n",
            "|--------|---------|-------------------------------------------------------------------------|------|\n",
            "|        | SST-2   | Classify the sentiment of [SPEECH] into positive or negative.           | 2790 |\n",
            "| SER    | QQP     | Identify if the question in is a paraphrase of the question in [TEXT] . |      |\n",
            "|        |         | Identify if the context in                                              |      |\n",
            "|        |         | Identify if the sentence in [SPEECH] entails the sentence in [TEXT] .   | 2088 |\n",
            "|        | SciTail | Identify if the premise in [SPEECH] supports the hypothesis in [TEXT] . | 2736 |\n",
            "\n",
            "Fig. 2 . E2E Inference strategies integrating ASR to SLU.\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "maintaining its E2E decoding property. We explore two strategies to integrate an auxiliary ASR step into SLU tasks:\n",
            "\n",
            "- · Speech chain-of-thought (SCoT) [10]: The system processes a concatenation of the ASR and SLU prompts, executing the tasks one-shot in sequential order. During inference, the SLU response is conditioned on the previously generated speech transcript.\n",
            "- · Multi-round (MR): Inference is conducted in multiple dialogue rounds. The system is first instructed to produce a speech transcript, then based on the speech embeddings and the transcript (in the dialogue history), we prompt the system to execute the SLU task.\n",
            "\n",
            "An illustration of the above inference strategies is depicted in Fig. 2. In SLU tasks (excluding SQIT), each example is randomly assigned to one of the following training configurations: SLU-alone, SCoT-ASR-SLU, and MR-ASR-SLU. Despite the fact that LLMs are inherently capable of executing CoT and MR inference, our observation suggests that speechLLMs do not develop such capabilities unless integrated into the fine-tuning process.\n",
            "\n",
            "## \n",
            "\n",
            "content_2:  1. INTRODUCTION\n",
            "\n",
            "Traditional speech processing techniques typically depend on specialised models tailored to individual tasks. These models, trained with limited data and constrained architectures, often face difficulties in generalising to new domains and applications. However, recent advancements in speech foundation models (SFMs) [1, 2, 3] and large language models (LLMs) [4, 5, 6] have fundamentally reshaped this approach, leading to the emergence of multi-functional end-to-end (E2E) speech-LLM systems [7, 8, 9, 10]. By utilising off-the-shelf SFMs and LLMs, the development of a speech-LLM system can be streamlined, often requiring only optimisation of adaptors and eliminating the need for extensive training data.\n",
            "\n",
            "Similar to their text-based counterparts, speech-LLMs perform speech tasks through instruction following. Each task is defined by a textual prompt, which the LLM decoder processes alongside the speech embeddings produced by the SFMencoder. Conditioned on the speech input, the LLM then generates a response to fulfill the given instruction. Leveraging the vast knowledge embedded in the LLM decoder,\n",
            "\n",
            "speech-LLMs demonstrate emergent abilities not explicitly imparted during training [8]. Additionally, the prompt-driven approach enables speech-LLMs to address various speech classification tasks in a zero-shot manner. In this paradigm, users maintain the flexibility to incorporate different class labels into the prompt, rather than relying on fixed classification heads as used in conventional methods. These attributes notably enhance the potential of speech-LLMs to serve as a universal solution in the speech industry.\n",
            "\n",
            "Previous studies have explored a wide range of speechcentric challenges within the speech-LLM framework. However, the evaluation of its spoken language understanding (SLU) capabilities, particularly in zero-shot scenarios, remains limited. While several existing speech-LLMs have showcased competence in common speech tasks like automatic speech recognition (ASR), speech-to-text translation (ST), and spoken question answering (SQA), their effectiveness in some key SLU tasks such as intent classification (IC) and slot filling (SF) has not been thoroughly examined.\n",
            "\n",
            "To address this gap, this paper introduces WHISMA, a speech-LLM system designed to enhance the zero-shot SLU performance across various domains. In this context, SLU refers to inferring semantics from spoken utterances. The proposed system employs Whisper [11] and Llama-3 [6] models as the speech encoder and text decoder, respectively. These components remain fixed during training and are connected by a trainable modality aligner. Low-rank adaptation (LoRA) [12] is implemented on Llama-3 to accommodate speech modality inputs. WHISMA is fine-tuned using approximately 2000 hours of speech data, covering tasks of ASR, IC, SF, SQA, and spoken (query) instruction tuning (SQIT/SIT). We adopt a training strategy that enables the system to perform an auxiliary ASR step before SLU through speech chain-of-thought (SCoT) [10] or multi-round (MR) inference, while maintaining the E2E property of WHISMA.\n",
            "\n",
            "To facilitate reproducing the proposed system, we ensure that all the training and test data used in this work is openly accessible. Specifically, we publish our self-developed Spoken-Alpaca dataset utilised for the SIT task. Derived from the text-based Alpaca instruction tuning corpus [13], we conduct pre-processing on the input and instruction fields, and synthesise speech for them using an in-house text-to-speech (TTS) model. Moreover, we curate a new SLU benchmark,\n",
            "\n",
            "named SLU-GLUE, to assess speech-LLMs on task-agnostic data beyond IC and SF. Tasks within SLU-GLUE include sentiment analysis (SA), semantic equivalence recognition (SER), and spoken-textual entailment recognition (STER). This benchmark is also made publicly available.\n",
            "\n",
            "The contributions of our work are summarised as follows:\n",
            "\n",
            "- · We introduce WHISMA, a speech-LLM that incorporates cutting-edge speech (Whisper) and language (Llama-3) foundation models to support a diverse range of SLU tasks in zero-shot settings.\n",
            "- · Through evaluations on the SLU tasks seen in training, WHISMA demonstrates superior zero-shot performance compared to existing baselines on the SLURP [14], FSC [15] and SmartLight [16] benchmarks.\n",
            "- · To illustrate the robustness of WHISMA on unseen SLU tasks, we further evaluate the system on the curated SLU-GLUE benchmark 1 , showing that it outperforms both the modular Whisper-Llama-3 system and an existing speech-LLM, Qwen-Audio [9].\n",
            "- · To mitigate over-fitting during WHISMA training, we develop the Spoken-Alpaca 2 dataset for speech-based instruction tuning, and show that it facilitates the model generalisation to unseen tasks.\n",
            "\n",
            "## \n",
            "\n",
            "content_3:  6. REFERENCES\n",
            "\n",
            "- [1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli, 'wav2vec 2.0: A framework for self-supervised learning of speech representations,' Advances in neural information processing systems , vol. 33, pp. 12449-12460, 2020.\n",
            "- [2] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed, 'Hubert: Self-supervised speech representation learning by masked prediction of hidden units,' IEEE/ACM Transactions on Audio, Speech, and Language Processing , vol. 29, pp. 3451-3460, 2021.\n",
            "- [3] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al., 'WavLM: Large-scale self-supervised pre-training for full stack speech processing,' IEEE Journal of Selected Topics in Signal Processing , vol. 16, no. 6, pp. 1505-1518, 2022.\n",
            "- [4] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al., 'Mistral 7B,' arXiv preprint arXiv:2310.06825 , 2023.\n",
            "- [5] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al., 'GPT-4 technical report,' arXiv preprint arXiv:2303.08774 , 2023.\n",
            "- [6] AI@Meta, 'Llama 3 model card,' 2024.\n",
            "- [7] Chen Wang, Minpeng Liao, Zhongqiang Huang, Jinliang Lu, Junhong Wu, Yuchen Liu, Chengqing Zong, and Jiajun Zhang, 'BLSP: Bootstrapping languagespeech pre-training via behavior alignment of continuation writing,' arXiv preprint arXiv:2309.00916 , 2023.\n",
            "- [8] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, and Chao Zhang, 'SALMONN: Towards generic hearing abilities for large language models,' in The Twelfth International Conference on Learning Representations , 2024.\n",
            "- [9] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou, 'Qwen-Audio: Advancing universal audio understanding via unified large-scale audio-language models,' arXiv preprint arXiv:2311.07919 , 2023.\n",
            "- [10] Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, Linquan Liu, et al., 'WavLLM: Towards\n",
            "- robust and adaptive speech large language model,' arXiv preprint arXiv:2404.00656 , 2024.\n",
            "- [11] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever, 'Robust speech recognition via large-scale weak supervision,' in International conference on machine learning . PMLR, 2023, pp. 28492-28518.\n",
            "- [12] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen, 'LoRA: Low-rank adaptation of large language models,' in International Conference on Learning Representations , 2022.\n",
            "- [13] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto, 'Stanford alpaca: An instruction-following llama model,' https://gith ub.com/tatsu-lab/stanford\\_alpaca , 2023.\n",
            "- [14] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and Verena Rieser, 'SLURP: A spoken language understanding resource package,' in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , Online, Nov. 2020, pp. 7252-7262, Association for Computational Linguistics.\n",
            "- [15] Loren Lugosch, Mirco Ravanelli, Patrick Ignoto, Vikrant Singh Tomar, and Yoshua Bengio, 'Speech model pre-training for end-to-end spoken language understanding,' arXiv preprint arXiv:1904.03670 , 2019.\n",
            "- [16] Alaa Saade, Joseph Dureau, David Leroy, Francesco Caltagirone, Alice Coucke, Adrien Ball, Cl'ement Doumouro, Thibaut Lavril, Alexandre Caulier, Th'eodore Bluche, et al., 'Spoken language understanding on the edge,' in 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS) . IEEE, 2019, pp. 57-61.\n",
            "- [17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi, 'BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,' in International conference on machine learning . PMLR, 2023, pp. 19730-19742.\n",
            "- [18] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee, 'Improved baselines with visual instruction tuning,' 2023.\n",
            "- [19] Yuang Li, Yu Wu, Jinyu Li, and Shujie Liu, 'Prompting large language models for zero-shot domain adaptation in speech recognition,' in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1-8.\n",
            "- [20] Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, et al., 'Prompting large language models with speech recognition abilities,' in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2024, pp. 13351-13355.\n",
            "- [21] Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, et al., 'On decoder-only architecture for speech-to-text and large language model integration,' in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1-8.\n",
            "- [22] Mingqiu Wang, Wei Han, Izhak Shafran, Zelin Wu, Chung-Cheng Chiu, Yuan Cao, Nanxin Chen, Yu Zhang, Hagen Soltau, Paul K Rubenstein, et al., 'SLM: Bridge the thin gap between speech and text foundation models,' in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1-8.\n",
            "- [23] Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, and Yemin Shi, 'LLaSM: Large language and speech model,' arXiv preprint arXiv:2308.15930 , 2023.\n",
            "- [24] Siddhant Arora, Hayato Futami, Jee-weon Jung, Yifan Peng, Roshan Sharma, Yosuke Kashiwagi, Emiru Tsunoo, Karen Livescu, and Shinji Watanabe, 'UniverSLU: Universal spoken language understanding for diverse tasks with natural language instructions,' in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , 2024, pp. 2754-2774.\n",
            "- [25] Mohan Li, Simon Keizer, and Rama Doddipatla, 'Prompting whisper for qa-driven zero-shot end-toend spoken language understanding,' arXiv preprint arXiv:2406.15209 , 2024.\n",
            "- [26] Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, et al., 'GigaSpeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio,' arXiv preprint arXiv:2106.06909 , 2021.\n",
            "- [27] Zihan Zhao, Yiyang Jiang, Heyang Liu, Yanfeng Wang, and Yu Wang, 'Librisqa: Pioneering freeform and open-ended spoken question answering with a novel dataset and framework,' arXiv preprint arXiv:2308.10390 , 2023.\n",
            "- [28] Jaehyeon Kim, Jungil Kong, and Juhee Son, 'Conditional variational autoencoder with adversarial learning\n",
            "- for end-to-end text-to-speech,' in International Conference on Machine Learning . PMLR, 2021, pp. 55305540.\n",
            "- [29] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, 'Librispeech: an ASR corpus based on public domain audio books,' in 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP) . IEEE, 2015, pp. 5206-5210.\n",
            "- [30] Mohan Li and Rama Doddipatla, 'Non-autoregressive end-to-end approaches for joint automatic speech recognition and spoken language understanding,' in 2022 IEEE Spoken Language Technology Workshop (SLT) . IEEE, 2023, pp. 390-397.\n",
            "- [31] Lingyun Feng, Jianwei Yu, Deng Cai, Songxiang Liu, Haitao Zheng, and Yan Wang, 'ASR-GLUE: A new multi-task benchmark for asr-robust natural language understanding,' arXiv preprint arXiv:2108.13048 , 2021.\n",
            "- [32] Mohan Li, C˘at˘alin Zoril˘a, Cong-Thanh Do, and Rama Doddipatla, 'Towards a unified end-to-end language understanding system for speech and text inputs,' in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1-8.\n",
            "- [33] Daniel Bermuth, Alexander Poeppel, and Wolfgang Reif, 'Finstreder: Simple and fast spoken language understanding with finite state transducers using modern speech-to-text models,' arXiv preprint arXiv:2206.14589 , 2022. \n",
            "\n",
            "content_4:  4.3. Main results\n",
            "\n",
            "The STSC evaluation results on the zero-shot test set of SLURP are presented in Table 3, focusing on ASR worderror-rate (WER) and SF SLU-F1 score. To ensure a fair com-\n",
            "\n",
            "Table 4 . STUC zero-shot evaluation. WER (%) and IC accuracy (IC Acc. %) on the test set of FSC.Table 5 . STUC zero-shot evaluation. WER (%), IC accuracy (IC Acc. %), SF SLU-F1 (%), and perfect parsing (PP, %) on the full set of SmartLight.\n",
            "\n",
            "| Model                     | WER ↓   | IC Acc. ↑   |\n",
            "|---------------------------|---------|-------------|\n",
            "| supervised                |         |             |\n",
            "| Finstreder [33]           | -       | 99.7        |\n",
            "| zero-shot                 |         |             |\n",
            "| BLSP [7]                  | -       | 77.5        |\n",
            "| ZS-Whisper-SLU [25]       | 0.8     | 95.0        |\n",
            "| Whisper-Llama-3 (modular) | 1.5     | 89.3        |\n",
            "| WHISMA (proposed)         | -       | 90.9        |\n",
            "| + SCoT                    | 1.7     | 97.3        |\n",
            "| + MR                      | 2.7     | 97.3        |\n",
            "\n",
            "| Model                     | WER ↓   | IC Acc. ↑   | SLU-F1 ↑   | PP ↑   |\n",
            "|---------------------------|---------|-------------|------------|--------|\n",
            "| supervised                |         |             |            |        |\n",
            "| Finstreder [33]           | 6.1     | -           | -          | 88.0   |\n",
            "| Whisper-TS [24]           | -       | 96.3        | -          | -      |\n",
            "| zero-shot                 |         |             |            |        |\n",
            "| BLSP [7]                  | -       | 78.8        | -          | -      |\n",
            "| ZS-Whisper-SLU [25]       | 2.7     | 91.6        | 90.9       | 82.5   |\n",
            "| UniverSLU-14 [24]         | -       | 44.6        | -          | -      |\n",
            "| Whisper-Llama-3 (modular) | 4.0     | 94.3        | 82.4       | 75.7   |\n",
            "| WHISMA (proposed)         | -       | 81.7        | 87.8       | 68.9   |\n",
            "| + SCoT                    | 4.5     | 95.9        | 90.2       | 82.5   |\n",
            "| + MR                      | 5.5     | 94.4        | 90.7       | 81.6   |\n",
            "\n",
            "parison, the modular Whisper-Llama-3 system is excluded as it has not been tuned on the SLURP dataset. For supervised model performance, we re-implement the spoken and natural language understanding (SNLU) system described in [32] and conduct the same evaluation. Given the rich knowledge embedded in Llama-3 and the wide array of SLU datasets used in training, our proposed WHISMA system with either SCoT or MR inference significantly outperforms the existing SOTA model, ZS-Whisper-SLU, achieving a 26.6% relative gain in SLU-F1. This comparison is equitable as ZS-whisper-SLU also conducts ASR prior to SF. The SCoT inference strategy achieves a lower WER compared to MR (11.9% vs. 13.8%), which we believe is because SCoT combines both tasks into a\n",
            "\n",
            "Table 6 . UTUC zero-shot evaluation. Accuracy (%) on SLU-GLUE sub-tasks.\n",
            "\n",
            "| Model                     |   SST-2 ↑ |   QQP ↑ |   QNLI ↑ |   RTE ↑ |   SciTail ↑ |   Avg. ↑ |\n",
            "|---------------------------|-----------|---------|----------|---------|-------------|----------|\n",
            "| Qwen-Audio [9]            |      53.4 |    62.4 |     58.8 |    65   |        54.5 |     58.8 |\n",
            "| + MR                      |      52.9 |    68.4 |     52.4 |    67.2 |        56.3 |     59.4 |\n",
            "| Whisper-Llama-3 (modular) |      88.3 |    74.9 |     81.6 |    76.7 |        68.3 |     78   |\n",
            "| WHISMA (proposed)         |      79   |    65.1 |     84   |    72.8 |        60.5 |     72.3 |\n",
            "| + SCoT                    |      88   |    65.1 |     78.9 |    73.7 |        71.7 |     75.5 |\n",
            "| + MR                      |      89.2 |    74.2 |     88   |    78   |        65.4 |     79   |\n",
            "| WHISMA w/o Spoken-Alpaca  |      78.6 |    45.9 |     50.9 |    61.2 |        52.6 |     57.8 |\n",
            "| + SCoT                    |      87.6 |    69   |     64.8 |    74.6 |        66.2 |     72.4 |\n",
            "| + MR                      |      86.6 |    52.6 |     73.8 |    70   |        57   |     68   |\n",
            "\n",
            "single prompt, enhancing ASR with the information in the SF instruction. WHISMA exhibits competitive SF performance (46.8%) to ZS-Whisper-SLU even without utilising speech transcripts. However, it lags behind cases with SCoT or MR inference, which emphasises the importance of performing ASR for SF tasks.\n",
            "\n",
            "The results of STUC evaluations on the FSC and SmartLight benchmarks are provided in Table 4 and 5, respectively. In the FSC benchmark, WHISMA+SCoT/MR obtains superior zero-shot performance compared to both the BLSP and ZS-Whisper-SLU baselines, showing relative improvements of 25.5% and 2.4% in IC accuracy. For the SmartLight benchmark, WHISMA+SCoT achieves SOTA performance in IC accuracy (95.9%) compared to all the zero-shot baselines. It also delivers SF SLU-F1 (90.2%) and perfect parsing (PP, 82.5%) results comparable to ZS-Whisper-SLU. Similar performance is observed with MR inference. On both benchmarks, WHISMA without ASR still surpasses the modular Whisper-Llama-3 model in various scenarios, highlighting the reduced complexity of E2E speech-LLMs over the modular approach. Regarding the SCoT and MR inferences, due to the limited ASR data used in fine-tuning, WHISMA generally displays higher WER relative to ZS-Whisper-SLU and Whisper-Llama-3. However, the joint modelling of speech and text modalities in WHISMA+SCoT/MR compensates for this shortcoming, leading to improved SLU performance.\n",
            "\n",
            "Table 6 presents the UTUC evaluation results on the SLU-GLUE benchmark. Alongside the modular system, we include Qwen-Audio, the most comprehensive open-sourced speech-LLM (available at the time of this study), for comparison. Qwen-Audio serves as a relevant baseline to WHISMA, since its training data includes SLU tasks similar to those used in our work. Furthermore, the system supports MR inference. As shown in the table, WHISMA demonstrates promising generalisation capabilities to unseen tasks during training. Specifically, WHISMA+MR achieves the highest performance among all the speech-LLMs, with an averaged\n",
            "\n",
            "accuracy of 79.0% across the five sub-tasks. This represents relative gains of 1.3% and 33.0% over Whisper-Llama-3 and Qwen-Audio+MR, respectively. Consistent with the observations in STSC and STUC settings, incorporating ASR into SLU substantially enhances WHISMA's performance on the challenging SLU-GLUE data, resulting in a relative gain of 9.3% when MR inference is performed. In contrast, QwenAudio+MR only provides minimal improvements over the SLU-alone inference fashion.\n",
            "\n",
            "In Table 6, we further illustrate the impact of utilising the Spoken-Alpaca dataset into the fine-tuning process of WHISMA. One can see that excluding this dataset results in a notable performance decline for WHISMA, with a 20.1% relative decrease in averaged accuracy when ASR is omitted. Similar degradation is observed with SCoT and MR inference, suffering relative reductions of 4.1% and 13.9% in averaged accuracy, respectively. Therefore, although SpokenAlpaca contributes only a small fraction (2.2% of total duration) of the entire training data, it plays a crucial role in strengthening the generalisability of speech-LLMs.\n",
            "\n",
            "## \n",
            "\n",
            "content_5:  3.2. Training data\n",
            "\n",
            "The fine-tuning of WHISMA is conducted with a multi-task learning approach. We compile a substantial training dataset that encompasses the following tasks: automatic speech recognition (ASR), intent classification (IC), slot filling (SF), spoken question answering (SQA), spoken query instruction tuning (SQIT), and spoken instruction tuning (SIT). Although ASR is not categorized as an SLU task, it significantly aids in aligning between the speech and text modalities. These tasks correspond to data from different sources, including the medium subset of GigaSpeech [26], the zero-shot data split of SLURP [14], LibriSQA-partI for open-ended QA, partII for multi-choice QA [27], and Spoken-Alpaca, developed as part of this research endeavor. The total duration of speech data amounts to approximately 2000 hours. Detailed information on this dataset is provided in Table 1.\n",
            "\n",
            "Although previous studies have employed Alpaca [13] with synthesised speech for instruction tuning [10, 22], the associated data remains unpublished, which hinders the replication of their findings. To remedy this, we introduce Spoken-Alpaca and release it for public access. The original text-based Alpaca dataset comprises two types of examples: i) containing fields { instruction, input, output } , where instruction describes the task, input provides context, and output is the expected system response; and ii) with fields\n",
            "\n",
            "{ instruction, output } , where the instruction stands alone as a query, not requiring contextual information. For the first type of examples, we generate speech for the input field while retaining instruction as the text prompt for the speechLLM system. This configuration defines the task referred to as spoken instruction tuning (SIT). Concerning the second type, speech synthesis is applied to instruction , which the system receives without a text prompt. This task category is denoted as spoken query instruction tuning (SQIT). We develop an in-house variational inference with adversarial learning for end-to-end text-to-speech (VITS) model [28] using the Librispeech [29] dataset featuring around 2200 speakers. Examples in Alpaca unsuitable for speech synthesis are filtered out, including those containing lengthy texts, mathematical equations, tables, etc. We also alter the wording of some instructions to make them speech-oriented. The diverse instructions within Spoken-Alpaca could significantly reduce the risk of over-fitting WHISMA to seen tasks with restricted prompt variations, such as ASR, IC and SF. Our experiments (Section 4.3) demonstrate that incorporating the dataset enables the proposed system to generalise more effectively to unseen tasks.\n",
            "\n",
            "## \n",
            "\n",
            "content_6:  4.2. Evaluation tasks\n",
            "\n",
            "We evaluate the proposed WHISMA model under three levels of zero-shot settings, ranging from easy to hard:\n",
            "\n",
            "- · seen-task-seen-corpus (STSC) , targeting the SF task using the zero-shot split of the SLURP dataset [14].\n",
            "- · seen-task-unseen-corpus (STUC) , focusing on IC and SF tasks within the test sets from FSC [15] and SmartLight [16] benchmarks.\n",
            "- · unseen-task-unseen-corpus (UTUC) , representing the most challenging scenario, utilising a new SLU benchmark named SLU-GLUE.\n",
            "\n",
            "For the STSC and STUC evaluations, we focus on data within the domain of in-home robot assistant. The same setups as\n",
            "\n",
            "Table 3 . STSC zero-shot evaluation. WER (%) and SF SLU-F1 (%) on the zero-shot test set of SLURP.\n",
            "\n",
            "| Model               | WER ↓   | SLU-F1 ↑   |\n",
            "|---------------------|---------|------------|\n",
            "| supervised          |         |            |\n",
            "| SNLU [32]           | 13.6    | 69.9       |\n",
            "| zero-shot           |         |            |\n",
            "| ZS-Whisper-SLU [25] | 8.3     | 50.0       |\n",
            "| WHISMA (proposed)   | -       | 46.8       |\n",
            "| + SCoT              | 11.9    | 63.1       |\n",
            "| + MR                | 13.8    | 63.3       |\n",
            "\n",
            "described in [25] are followed to structure the test data. In the case of SLURP, a zero-shot data split is created by holding out 5 slot types: { podcast name, artist name, audiobook name, business name, radio name } , which results in a test set containing 18k utterances. Regarding FSC, the original { action, object, location } labels are reorganised into 15 intents and 2 slots. As for SmartLight, we employ the full close-field subset as the test set, comprising 6 intents and 3 slot types.\n",
            "\n",
            "In addition to the task-oriented datasets mentioned above, we curate SLU-GLUE for the UTUC evaluation in our experiment. Derived from ASR-GLUE [31], SLU-GLUE comprises three primary tasks: sentiment analysis (SA), semantic equivalence recognition (SER), and spoken-textual entailment recognition (STER). These tasks encompass five sub-tasks, namely: Stanford Sentiment Treebank (SST-2), Quora Question Pairs (QQP), Question-answering NLI (QNLI), Recognizing Textual Entailment (RTE), and SciTail. We exclude Semantic Textual Similarity Benchmark (STS-B) as it is not suitable for zero-shot evaluation. The evaluation metric for all the tasks is binary accuracy. The data domains of SLUGLUE span from everyday language, such as movie reviews, to specialised scientific terminology. Speech samples in the dataset are recorded by six native speakers and are mixed with various levels of noise. Further detailed information of SLU-GLUE is provided in Table. 2.\n",
            "\n",
            "The performance of WHISMA is compared against multiple robust baselines, including state-of-the-art (SOTA) supervised models, SOTA zero-shot models, the modular ASRLLM counterpart to WHISMA, and a well-established opensourced speech-LLM called Qwen-Audio [9].\n",
            "\n",
            "## \n",
            "\n",
            "content_7:  3.1. Model architecture\n",
            "\n",
            "The model architecture of WHISMA is illustrated in Fig. 1. We adopt the encoder from Whisper-large-v2 , a 32layer Transformer model with two convolutional neural network (CNN) down-samplers, as the speech encoder in our system. The resulting speech embeddings are fed to a modality aligner, which converts these embeddings to align with the input space of the LLM decoder. Our modality aligner is structured similarly to a previous system [10], consisting\n",
            "\n",
            "Table 1 . Multi-task training dataset.\n",
            "\n",
            "| Task   | Data Source                              | #Hours   | #Samples   |\n",
            "|--------|------------------------------------------|----------|------------|\n",
            "| ASR    | GigaSpeech (M) [26]                      | 1000     | 910k       |\n",
            "| IC     | SLURP (zero-shot) [14]                   | 80       | 94k        |\n",
            "| SF     | SLURP (zero-shot) [14]                   | 80       | 94k        |\n",
            "| SQA    | LibriSQA-partI [27] LibriSQA-partII [27] | 360      | 104k 104k  |\n",
            "| SQIT   | Spoken-Alpaca                            | 360 30   | 29k        |\n",
            "| SIT    | Spoken-Alpaca                            | 14       | 14k        |\n",
            "\n",
            "of two CNN layers for further down-sampling, a bottleneck adaptor, and a linear output layer to match dimensions.\n",
            "\n",
            "WHISMA's decoder is implemented using Llama-38B-Instruct , one of the most promising open-sourced LLMs optimised for instruction following. This decoder processes both the aligned speech embeddings and a taskdescriptive text prompt, generating a response to execute the given instruction. We integrate low-rank adaptation (LoRA) weights into the attention module of all 32 Transformer layers in Llama-3, enabling it to effectively handle speech modality inputs. During training, only the modality aligner and LoRA parameters are optimised.\n",
            "\n",
            "## \n",
            "\n",
            "content_8:  2. RELATED WORK\n",
            "\n",
            "Motivated by the success of vision language models (VLMs) [17, 18], there has been a growing interest in extending LLMs with auditory capabilities. Initial attempts to incorporate speech perception into LLMs involve LLM-based ASR models [19, 20]. Building upon this, Speech-Llama [21] integrates both ST and ASR to the framework through multi-task learning. Further, SLM [22] enhances the system with speech instruction tuning. However, these models typically undergo fine-tuning with a limited set of tasks and lack generalised understanding abilities.\n",
            "\n",
            "Recent advancements have introduced speech-LLMs with more diverse speech processing functions. Notable examples include BLSP [7], LLaSM [23], SALMONN [8], QwenAudio [9], and WavLLM [10]. Most of these systems utilise a variety of speech or audio resources to achieve cross-modal perception. Among them, zero-shot SLU evaluation results have been reported by BLSP for IC and SA tasks, SALMONN for a SF task, and WavLLM for a SQA task. In this work, we aim to conduct a comprehensive evaluation on these tasks using our proposed WHISMA system.\n",
            "\n",
            "UniverSLU [24] is the latest study that exclusively examines the SLU capabilities of the SFM, which fine-tunes the\n",
            "\n",
            "Fig. 1 . An overview of WHISMA model architecture.\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "Whisper model with up to 17 SLU datasets encompassing both acoustic and semantic domain tasks. The zero-shot performance of UniverSLU, as reported, reveals that the system struggles to generalise to unseen datasets and tasks, which is the issue we would like to address in this work.\n",
            "\n",
            "The most relevant research to this study is ZS-WhisperSLU proposed by [25], which investigates zero-shot IC and SF tasks within a QA-driven framework based on Whisper. Despite the promising performance achieved by the system, it is challenging to utilise ZS-Whisper-SLU for unseen tasks due to the limitation of the Whisper decoder. In this regard, we seek to bridge this gap by leveraging an LLM that possess broader knowledge than an SFM.\n",
            "\n",
            "## \n",
            "\n",
            "content_9:  4. EXPERIMENTS\n",
            "\n",
            "## 4.1. Experimental setup\n",
            "\n",
            "The speech encoder and text decoder of WHISMA are based on Whisper-large-v2 and Llama-3-8B-Instruct models, respectively. The modality aligner adopts two 1-D CNN layers with a stride of 2, down-sampling the speech by a factor of 8 along with the Whisper encoder. This configuration produces 375 speech embeddings per input. The bottleneck dimension of the adaptor is set to 320. Within Llama-3, a light-weight LoRA scheme with a rank of 8 and an alpha of 16 is implemented. WHISMA is fine-tuned on 6 V6000 GPUs using a batch size of 12. To prevent over-fitting, the system undergoes only one epoch of training, using the AdamW optimiser with a constant learning rate of 0.0001.\n",
            "\n",
            "## \n",
            "\n",
            "\n",
            "</ctx>\n",
            "\n",
            "--Response--\n",
            "1. Always and only respond in HTML tag formatted which should render on website smoothly. Do not use markdown, plain text, or any non-HTML formatting. Use semantic HTML tags like <h1> to <h6> for headings, <p> for paragraphs, <ul> and <li> for lists, <strong> for bold, <em> for italics, <mark> for highlights, <code> for inline code, <pre> for code blocks, <a> for links, and <table> for tabular data. Ensure the HTML is well-formed, valid, and self-contained (no external CSS or JavaScript)\n",
            "2. Cite relevant chunks from the context using ^[Context_n] notation. For multiple citations, use ^[Context_1] ^[Context_2], etc.\n",
            "3. Respond in customizable response lengths:\n",
            "   - Concise\n",
            "   - Medium\n",
            "   - Detailed\n",
            "4. Respond in customizable Format:\n",
            "  - use <ul> and <li> for bullets points and <p> for paragraph\n",
            "5. For questions unrelated to the given context, return : \"Sorry I dont have information about this,Please ask related to your research paper only.\" or politely deny and guide the user back to the given context.\n",
            "\n",
            "--Task--\n",
            "1. Answer questions using only the content from the provided context. Do not rely on external knowledge.\n",
            "2. Keep responses concise, crisp, and to the point unless the user requests a detailed answer.\n",
            "3. Always cite relevant chunks from the context using ^[Context_n] notation.\n",
            "4. If the user asks for a specific format (bullet points or paragraphs), adhere to their request.\n",
            "\n",
            "You: what is the method\n",
            "Assistant: <h1>Method Overview</h1>\n",
            "\n",
            "<p>The method section details the architecture of the proposed WHISMA system, the fine-tuning speech tasks and datasets, and the training strategy.  ^[Context_0]</p>\n",
            "\n",
            "<ul>\n",
            "<li><strong>System Architecture:</strong> WHISMA uses Whisper-large-v2 as the speech encoder and Llama-3-8B-Instruct as the text decoder, connected by a modality aligner (two CNN layers, bottleneck adaptor, linear output layer). Low-rank adaptation (LoRA) is applied to Llama-3. ^[Context_7]</li>\n",
            "<li><strong>Fine-tuning Tasks and Datasets:</strong>  Fine-tuning involves multiple tasks: automatic speech recognition (ASR), intent classification (IC), slot filling (SF), spoken question answering (SQA), spoken query instruction tuning (SQIT), and spoken instruction tuning (SIT). Datasets include GigaSpeech, SLURP, LibriSQA, and the newly developed Spoken-Alpaca (from the Alpaca instruction tuning corpus, with speech synthesized using a VITS model).  ^[Context_2] ^[Context_5]</li>\n",
            "<li><strong>Training Strategy:</strong> Training examples follow Llama-3's prompt template.  To enhance robustness, 10 distinct prompts per task (ASR, IC, SF) are randomly selected. For other tasks, the provided question/instruction is used.  Candidate labels for IC and SF are amalgamated into a single prompt, sampling a varying number of labels per example to introduce diversity and prevent overfitting. ^[Context_1]</li>\n",
            "<li><strong>ASR Integration:</strong> To improve SLU performance, two ASR integration strategies are explored: Speech chain-of-thought (SCoT) – concatenating ASR and SLU prompts for sequential processing; and Multi-round (MR) – performing ASR first, then SLU based on speech embeddings and the transcript. Each example is randomly assigned to one of: SLU-alone, SCoT-ASR-SLU, and MR-ASR-SLU. ^[Context_1]</li>\n",
            "</ul>\n",
            "\n",
            "\n",
            "You: what is the dataset and benchmark for this\n",
            "['What datasets and benchmarks were used to evaluate the WHISMA model?', 'Can you detail the datasets and benchmark used in the experiments for this research?']\n",
            "Query Expanded:\n",
            " What datasets and benchmarks were used to evaluate the WHISMA model?\n",
            "System Prompt: -Role--\n",
            "You are a question-answering chatbot for research paper. Your job is to answer user queries strictly using the content provided in the delimited by <ctx></ctx>). You cannot use any external knowledge or information outside the provided context.\n",
            "\n",
            "<ctx>\n",
            "content_0:  4.2. Evaluation tasks\n",
            "\n",
            "We evaluate the proposed WHISMA model under three levels of zero-shot settings, ranging from easy to hard:\n",
            "\n",
            "- · seen-task-seen-corpus (STSC) , targeting the SF task using the zero-shot split of the SLURP dataset [14].\n",
            "- · seen-task-unseen-corpus (STUC) , focusing on IC and SF tasks within the test sets from FSC [15] and SmartLight [16] benchmarks.\n",
            "- · unseen-task-unseen-corpus (UTUC) , representing the most challenging scenario, utilising a new SLU benchmark named SLU-GLUE.\n",
            "\n",
            "For the STSC and STUC evaluations, we focus on data within the domain of in-home robot assistant. The same setups as\n",
            "\n",
            "Table 3 . STSC zero-shot evaluation. WER (%) and SF SLU-F1 (%) on the zero-shot test set of SLURP.\n",
            "\n",
            "| Model               | WER ↓   | SLU-F1 ↑   |\n",
            "|---------------------|---------|------------|\n",
            "| supervised          |         |            |\n",
            "| SNLU [32]           | 13.6    | 69.9       |\n",
            "| zero-shot           |         |            |\n",
            "| ZS-Whisper-SLU [25] | 8.3     | 50.0       |\n",
            "| WHISMA (proposed)   | -       | 46.8       |\n",
            "| + SCoT              | 11.9    | 63.1       |\n",
            "| + MR                | 13.8    | 63.3       |\n",
            "\n",
            "described in [25] are followed to structure the test data. In the case of SLURP, a zero-shot data split is created by holding out 5 slot types: { podcast name, artist name, audiobook name, business name, radio name } , which results in a test set containing 18k utterances. Regarding FSC, the original { action, object, location } labels are reorganised into 15 intents and 2 slots. As for SmartLight, we employ the full close-field subset as the test set, comprising 6 intents and 3 slot types.\n",
            "\n",
            "In addition to the task-oriented datasets mentioned above, we curate SLU-GLUE for the UTUC evaluation in our experiment. Derived from ASR-GLUE [31], SLU-GLUE comprises three primary tasks: sentiment analysis (SA), semantic equivalence recognition (SER), and spoken-textual entailment recognition (STER). These tasks encompass five sub-tasks, namely: Stanford Sentiment Treebank (SST-2), Quora Question Pairs (QQP), Question-answering NLI (QNLI), Recognizing Textual Entailment (RTE), and SciTail. We exclude Semantic Textual Similarity Benchmark (STS-B) as it is not suitable for zero-shot evaluation. The evaluation metric for all the tasks is binary accuracy. The data domains of SLUGLUE span from everyday language, such as movie reviews, to specialised scientific terminology. Speech samples in the dataset are recorded by six native speakers and are mixed with various levels of noise. Further detailed information of SLU-GLUE is provided in Table. 2.\n",
            "\n",
            "The performance of WHISMA is compared against multiple robust baselines, including state-of-the-art (SOTA) supervised models, SOTA zero-shot models, the modular ASRLLM counterpart to WHISMA, and a well-established opensourced speech-LLM called Qwen-Audio [9].\n",
            "\n",
            "## \n",
            "\n",
            "content_1:  1. INTRODUCTION\n",
            "\n",
            "Traditional speech processing techniques typically depend on specialised models tailored to individual tasks. These models, trained with limited data and constrained architectures, often face difficulties in generalising to new domains and applications. However, recent advancements in speech foundation models (SFMs) [1, 2, 3] and large language models (LLMs) [4, 5, 6] have fundamentally reshaped this approach, leading to the emergence of multi-functional end-to-end (E2E) speech-LLM systems [7, 8, 9, 10]. By utilising off-the-shelf SFMs and LLMs, the development of a speech-LLM system can be streamlined, often requiring only optimisation of adaptors and eliminating the need for extensive training data.\n",
            "\n",
            "Similar to their text-based counterparts, speech-LLMs perform speech tasks through instruction following. Each task is defined by a textual prompt, which the LLM decoder processes alongside the speech embeddings produced by the SFMencoder. Conditioned on the speech input, the LLM then generates a response to fulfill the given instruction. Leveraging the vast knowledge embedded in the LLM decoder,\n",
            "\n",
            "speech-LLMs demonstrate emergent abilities not explicitly imparted during training [8]. Additionally, the prompt-driven approach enables speech-LLMs to address various speech classification tasks in a zero-shot manner. In this paradigm, users maintain the flexibility to incorporate different class labels into the prompt, rather than relying on fixed classification heads as used in conventional methods. These attributes notably enhance the potential of speech-LLMs to serve as a universal solution in the speech industry.\n",
            "\n",
            "Previous studies have explored a wide range of speechcentric challenges within the speech-LLM framework. However, the evaluation of its spoken language understanding (SLU) capabilities, particularly in zero-shot scenarios, remains limited. While several existing speech-LLMs have showcased competence in common speech tasks like automatic speech recognition (ASR), speech-to-text translation (ST), and spoken question answering (SQA), their effectiveness in some key SLU tasks such as intent classification (IC) and slot filling (SF) has not been thoroughly examined.\n",
            "\n",
            "To address this gap, this paper introduces WHISMA, a speech-LLM system designed to enhance the zero-shot SLU performance across various domains. In this context, SLU refers to inferring semantics from spoken utterances. The proposed system employs Whisper [11] and Llama-3 [6] models as the speech encoder and text decoder, respectively. These components remain fixed during training and are connected by a trainable modality aligner. Low-rank adaptation (LoRA) [12] is implemented on Llama-3 to accommodate speech modality inputs. WHISMA is fine-tuned using approximately 2000 hours of speech data, covering tasks of ASR, IC, SF, SQA, and spoken (query) instruction tuning (SQIT/SIT). We adopt a training strategy that enables the system to perform an auxiliary ASR step before SLU through speech chain-of-thought (SCoT) [10] or multi-round (MR) inference, while maintaining the E2E property of WHISMA.\n",
            "\n",
            "To facilitate reproducing the proposed system, we ensure that all the training and test data used in this work is openly accessible. Specifically, we publish our self-developed Spoken-Alpaca dataset utilised for the SIT task. Derived from the text-based Alpaca instruction tuning corpus [13], we conduct pre-processing on the input and instruction fields, and synthesise speech for them using an in-house text-to-speech (TTS) model. Moreover, we curate a new SLU benchmark,\n",
            "\n",
            "named SLU-GLUE, to assess speech-LLMs on task-agnostic data beyond IC and SF. Tasks within SLU-GLUE include sentiment analysis (SA), semantic equivalence recognition (SER), and spoken-textual entailment recognition (STER). This benchmark is also made publicly available.\n",
            "\n",
            "The contributions of our work are summarised as follows:\n",
            "\n",
            "- · We introduce WHISMA, a speech-LLM that incorporates cutting-edge speech (Whisper) and language (Llama-3) foundation models to support a diverse range of SLU tasks in zero-shot settings.\n",
            "- · Through evaluations on the SLU tasks seen in training, WHISMA demonstrates superior zero-shot performance compared to existing baselines on the SLURP [14], FSC [15] and SmartLight [16] benchmarks.\n",
            "- · To illustrate the robustness of WHISMA on unseen SLU tasks, we further evaluate the system on the curated SLU-GLUE benchmark 1 , showing that it outperforms both the modular Whisper-Llama-3 system and an existing speech-LLM, Qwen-Audio [9].\n",
            "- · To mitigate over-fitting during WHISMA training, we develop the Spoken-Alpaca 2 dataset for speech-based instruction tuning, and show that it facilitates the model generalisation to unseen tasks.\n",
            "\n",
            "## \n",
            "\n",
            "content_2:  5. CONCLUSION\n",
            "\n",
            "In this paper, we present WHISMA, a speech-LLM system that excels in various zero-shot SLU tasks. WHISMA integrates a Whisper-based speech encoder with a Llama-3 text decoder, and is fine-tuned on a diverse set of SLU tasks using a modality aligner and LoRA adaptors. Additionally, we enable the system to perform an auxiliary ASR step before SLU through SCoT or MR inference strategies. Comprehensive zero-shot evaluations demonstrate WHISMA's ability to achieve SOTA performance on several common SLU benchmarks and, more importantly, to generalise to tasks not encountered during training. To ensure the reproducibility of our results, we release the Spoken-Alpaca and SLU-GLUE datasets utilised in our experiments for public access.\n",
            "\n",
            "## \n",
            "\n",
            "content_3:  3.2. Training data\n",
            "\n",
            "The fine-tuning of WHISMA is conducted with a multi-task learning approach. We compile a substantial training dataset that encompasses the following tasks: automatic speech recognition (ASR), intent classification (IC), slot filling (SF), spoken question answering (SQA), spoken query instruction tuning (SQIT), and spoken instruction tuning (SIT). Although ASR is not categorized as an SLU task, it significantly aids in aligning between the speech and text modalities. These tasks correspond to data from different sources, including the medium subset of GigaSpeech [26], the zero-shot data split of SLURP [14], LibriSQA-partI for open-ended QA, partII for multi-choice QA [27], and Spoken-Alpaca, developed as part of this research endeavor. The total duration of speech data amounts to approximately 2000 hours. Detailed information on this dataset is provided in Table 1.\n",
            "\n",
            "Although previous studies have employed Alpaca [13] with synthesised speech for instruction tuning [10, 22], the associated data remains unpublished, which hinders the replication of their findings. To remedy this, we introduce Spoken-Alpaca and release it for public access. The original text-based Alpaca dataset comprises two types of examples: i) containing fields { instruction, input, output } , where instruction describes the task, input provides context, and output is the expected system response; and ii) with fields\n",
            "\n",
            "{ instruction, output } , where the instruction stands alone as a query, not requiring contextual information. For the first type of examples, we generate speech for the input field while retaining instruction as the text prompt for the speechLLM system. This configuration defines the task referred to as spoken instruction tuning (SIT). Concerning the second type, speech synthesis is applied to instruction , which the system receives without a text prompt. This task category is denoted as spoken query instruction tuning (SQIT). We develop an in-house variational inference with adversarial learning for end-to-end text-to-speech (VITS) model [28] using the Librispeech [29] dataset featuring around 2200 speakers. Examples in Alpaca unsuitable for speech synthesis are filtered out, including those containing lengthy texts, mathematical equations, tables, etc. We also alter the wording of some instructions to make them speech-oriented. The diverse instructions within Spoken-Alpaca could significantly reduce the risk of over-fitting WHISMA to seen tasks with restricted prompt variations, such as ASR, IC and SF. Our experiments (Section 4.3) demonstrate that incorporating the dataset enables the proposed system to generalise more effectively to unseen tasks.\n",
            "\n",
            "## \n",
            "\n",
            "content_4:  3. METHOD\n",
            "\n",
            "In this section, we present the architecture of the proposed WHISMA system and describe the fine-tuning speech tasks along with their corresponding datasets. Additionally, we introduce the training strategy that integrates ASR auxiliary into SLU tasks to enhance the reliability of LLM inference, while preserving WHISMA's end-to-end (E2E) nature.\n",
            "\n",
            "## \n",
            "\n",
            "content_5:  2. RELATED WORK\n",
            "\n",
            "Motivated by the success of vision language models (VLMs) [17, 18], there has been a growing interest in extending LLMs with auditory capabilities. Initial attempts to incorporate speech perception into LLMs involve LLM-based ASR models [19, 20]. Building upon this, Speech-Llama [21] integrates both ST and ASR to the framework through multi-task learning. Further, SLM [22] enhances the system with speech instruction tuning. However, these models typically undergo fine-tuning with a limited set of tasks and lack generalised understanding abilities.\n",
            "\n",
            "Recent advancements have introduced speech-LLMs with more diverse speech processing functions. Notable examples include BLSP [7], LLaSM [23], SALMONN [8], QwenAudio [9], and WavLLM [10]. Most of these systems utilise a variety of speech or audio resources to achieve cross-modal perception. Among them, zero-shot SLU evaluation results have been reported by BLSP for IC and SA tasks, SALMONN for a SF task, and WavLLM for a SQA task. In this work, we aim to conduct a comprehensive evaluation on these tasks using our proposed WHISMA system.\n",
            "\n",
            "UniverSLU [24] is the latest study that exclusively examines the SLU capabilities of the SFM, which fine-tunes the\n",
            "\n",
            "Fig. 1 . An overview of WHISMA model architecture.\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "Whisper model with up to 17 SLU datasets encompassing both acoustic and semantic domain tasks. The zero-shot performance of UniverSLU, as reported, reveals that the system struggles to generalise to unseen datasets and tasks, which is the issue we would like to address in this work.\n",
            "\n",
            "The most relevant research to this study is ZS-WhisperSLU proposed by [25], which investigates zero-shot IC and SF tasks within a QA-driven framework based on Whisper. Despite the promising performance achieved by the system, it is challenging to utilise ZS-Whisper-SLU for unseen tasks due to the limitation of the Whisper decoder. In this regard, we seek to bridge this gap by leveraging an LLM that possess broader knowledge than an SFM.\n",
            "\n",
            "## \n",
            "\n",
            "content_6:  4. EXPERIMENTS\n",
            "\n",
            "## 4.1. Experimental setup\n",
            "\n",
            "The speech encoder and text decoder of WHISMA are based on Whisper-large-v2 and Llama-3-8B-Instruct models, respectively. The modality aligner adopts two 1-D CNN layers with a stride of 2, down-sampling the speech by a factor of 8 along with the Whisper encoder. This configuration produces 375 speech embeddings per input. The bottleneck dimension of the adaptor is set to 320. Within Llama-3, a light-weight LoRA scheme with a rank of 8 and an alpha of 16 is implemented. WHISMA is fine-tuned on 6 V6000 GPUs using a batch size of 12. To prevent over-fitting, the system undergoes only one epoch of training, using the AdamW optimiser with a constant learning rate of 0.0001.\n",
            "\n",
            "## \n",
            "\n",
            "content_7:  3.1. Model architecture\n",
            "\n",
            "The model architecture of WHISMA is illustrated in Fig. 1. We adopt the encoder from Whisper-large-v2 , a 32layer Transformer model with two convolutional neural network (CNN) down-samplers, as the speech encoder in our system. The resulting speech embeddings are fed to a modality aligner, which converts these embeddings to align with the input space of the LLM decoder. Our modality aligner is structured similarly to a previous system [10], consisting\n",
            "\n",
            "Table 1 . Multi-task training dataset.\n",
            "\n",
            "| Task   | Data Source                              | #Hours   | #Samples   |\n",
            "|--------|------------------------------------------|----------|------------|\n",
            "| ASR    | GigaSpeech (M) [26]                      | 1000     | 910k       |\n",
            "| IC     | SLURP (zero-shot) [14]                   | 80       | 94k        |\n",
            "| SF     | SLURP (zero-shot) [14]                   | 80       | 94k        |\n",
            "| SQA    | LibriSQA-partI [27] LibriSQA-partII [27] | 360      | 104k 104k  |\n",
            "| SQIT   | Spoken-Alpaca                            | 360 30   | 29k        |\n",
            "| SIT    | Spoken-Alpaca                            | 14       | 14k        |\n",
            "\n",
            "of two CNN layers for further down-sampling, a bottleneck adaptor, and a linear output layer to match dimensions.\n",
            "\n",
            "WHISMA's decoder is implemented using Llama-38B-Instruct , one of the most promising open-sourced LLMs optimised for instruction following. This decoder processes both the aligned speech embeddings and a taskdescriptive text prompt, generating a response to execute the given instruction. We integrate low-rank adaptation (LoRA) weights into the attention module of all 32 Transformer layers in Llama-3, enabling it to effectively handle speech modality inputs. During training, only the modality aligner and LoRA parameters are optimised.\n",
            "\n",
            "## \n",
            "\n",
            "content_8: ## WHISMA: A SPEECH-LLM TO PERFORM ZERO-SHOT SPOKEN LANGUAGE UNDERSTANDING\n",
            "\n",
            "Mohan Li, Cong-Thanh Do, Simon Keizer, Youmna Farag, Svetlana Stoyanchev, Rama Doddipatla\n",
            "\n",
            "Cambridge Research Laboratory, Toshiba Europe Ltd, Cambridge, UK\n",
            "\n",
            "## ABSTRACT\n",
            "\n",
            "Speech large language models (speech-LLMs) integrate speech and text-based foundation models to provide a unified framework for handling a wide range of downstream tasks. In this paper, we introduce WHISMA, a speech-LLM tailored for spoken language understanding (SLU) that demonstrates robust performance in various zero-shot settings. WHISMA combines the speech encoder from Whisper with the Llama-3 LLM, and is fine-tuned in a parameter-efficient manner on a comprehensive collection of SLU-related datasets. Our experiments show that WHISMA significantly improves the zero-shot slot filling performance on the SLURP benchmark, achieving a relative gain of 26.6% compared to the current state-of-the-art model. Furthermore, to evaluate WHISMA's generalisation capabilities to unseen domains, we develop a new task-agnostic benchmark named SLU-GLUE. The evaluation results indicate that WHISMA outperforms an existing speech-LLM (Qwen-Audio) with a relative gain of 33.0%.\n",
            "\n",
            "Index Terms -spoken language understanding, speech large language model, zero-shot learning\n",
            "\n",
            "## \n",
            "\n",
            "content_9:  4.3. Main results\n",
            "\n",
            "The STSC evaluation results on the zero-shot test set of SLURP are presented in Table 3, focusing on ASR worderror-rate (WER) and SF SLU-F1 score. To ensure a fair com-\n",
            "\n",
            "Table 4 . STUC zero-shot evaluation. WER (%) and IC accuracy (IC Acc. %) on the test set of FSC.Table 5 . STUC zero-shot evaluation. WER (%), IC accuracy (IC Acc. %), SF SLU-F1 (%), and perfect parsing (PP, %) on the full set of SmartLight.\n",
            "\n",
            "| Model                     | WER ↓   | IC Acc. ↑   |\n",
            "|---------------------------|---------|-------------|\n",
            "| supervised                |         |             |\n",
            "| Finstreder [33]           | -       | 99.7        |\n",
            "| zero-shot                 |         |             |\n",
            "| BLSP [7]                  | -       | 77.5        |\n",
            "| ZS-Whisper-SLU [25]       | 0.8     | 95.0        |\n",
            "| Whisper-Llama-3 (modular) | 1.5     | 89.3        |\n",
            "| WHISMA (proposed)         | -       | 90.9        |\n",
            "| + SCoT                    | 1.7     | 97.3        |\n",
            "| + MR                      | 2.7     | 97.3        |\n",
            "\n",
            "| Model                     | WER ↓   | IC Acc. ↑   | SLU-F1 ↑   | PP ↑   |\n",
            "|---------------------------|---------|-------------|------------|--------|\n",
            "| supervised                |         |             |            |        |\n",
            "| Finstreder [33]           | 6.1     | -           | -          | 88.0   |\n",
            "| Whisper-TS [24]           | -       | 96.3        | -          | -      |\n",
            "| zero-shot                 |         |             |            |        |\n",
            "| BLSP [7]                  | -       | 78.8        | -          | -      |\n",
            "| ZS-Whisper-SLU [25]       | 2.7     | 91.6        | 90.9       | 82.5   |\n",
            "| UniverSLU-14 [24]         | -       | 44.6        | -          | -      |\n",
            "| Whisper-Llama-3 (modular) | 4.0     | 94.3        | 82.4       | 75.7   |\n",
            "| WHISMA (proposed)         | -       | 81.7        | 87.8       | 68.9   |\n",
            "| + SCoT                    | 4.5     | 95.9        | 90.2       | 82.5   |\n",
            "| + MR                      | 5.5     | 94.4        | 90.7       | 81.6   |\n",
            "\n",
            "parison, the modular Whisper-Llama-3 system is excluded as it has not been tuned on the SLURP dataset. For supervised model performance, we re-implement the spoken and natural language understanding (SNLU) system described in [32] and conduct the same evaluation. Given the rich knowledge embedded in Llama-3 and the wide array of SLU datasets used in training, our proposed WHISMA system with either SCoT or MR inference significantly outperforms the existing SOTA model, ZS-Whisper-SLU, achieving a 26.6% relative gain in SLU-F1. This comparison is equitable as ZS-whisper-SLU also conducts ASR prior to SF. The SCoT inference strategy achieves a lower WER compared to MR (11.9% vs. 13.8%), which we believe is because SCoT combines both tasks into a\n",
            "\n",
            "Table 6 . UTUC zero-shot evaluation. Accuracy (%) on SLU-GLUE sub-tasks.\n",
            "\n",
            "| Model                     |   SST-2 ↑ |   QQP ↑ |   QNLI ↑ |   RTE ↑ |   SciTail ↑ |   Avg. ↑ |\n",
            "|---------------------------|-----------|---------|----------|---------|-------------|----------|\n",
            "| Qwen-Audio [9]            |      53.4 |    62.4 |     58.8 |    65   |        54.5 |     58.8 |\n",
            "| + MR                      |      52.9 |    68.4 |     52.4 |    67.2 |        56.3 |     59.4 |\n",
            "| Whisper-Llama-3 (modular) |      88.3 |    74.9 |     81.6 |    76.7 |        68.3 |     78   |\n",
            "| WHISMA (proposed)         |      79   |    65.1 |     84   |    72.8 |        60.5 |     72.3 |\n",
            "| + SCoT                    |      88   |    65.1 |     78.9 |    73.7 |        71.7 |     75.5 |\n",
            "| + MR                      |      89.2 |    74.2 |     88   |    78   |        65.4 |     79   |\n",
            "| WHISMA w/o Spoken-Alpaca  |      78.6 |    45.9 |     50.9 |    61.2 |        52.6 |     57.8 |\n",
            "| + SCoT                    |      87.6 |    69   |     64.8 |    74.6 |        66.2 |     72.4 |\n",
            "| + MR                      |      86.6 |    52.6 |     73.8 |    70   |        57   |     68   |\n",
            "\n",
            "single prompt, enhancing ASR with the information in the SF instruction. WHISMA exhibits competitive SF performance (46.8%) to ZS-Whisper-SLU even without utilising speech transcripts. However, it lags behind cases with SCoT or MR inference, which emphasises the importance of performing ASR for SF tasks.\n",
            "\n",
            "The results of STUC evaluations on the FSC and SmartLight benchmarks are provided in Table 4 and 5, respectively. In the FSC benchmark, WHISMA+SCoT/MR obtains superior zero-shot performance compared to both the BLSP and ZS-Whisper-SLU baselines, showing relative improvements of 25.5% and 2.4% in IC accuracy. For the SmartLight benchmark, WHISMA+SCoT achieves SOTA performance in IC accuracy (95.9%) compared to all the zero-shot baselines. It also delivers SF SLU-F1 (90.2%) and perfect parsing (PP, 82.5%) results comparable to ZS-Whisper-SLU. Similar performance is observed with MR inference. On both benchmarks, WHISMA without ASR still surpasses the modular Whisper-Llama-3 model in various scenarios, highlighting the reduced complexity of E2E speech-LLMs over the modular approach. Regarding the SCoT and MR inferences, due to the limited ASR data used in fine-tuning, WHISMA generally displays higher WER relative to ZS-Whisper-SLU and Whisper-Llama-3. However, the joint modelling of speech and text modalities in WHISMA+SCoT/MR compensates for this shortcoming, leading to improved SLU performance.\n",
            "\n",
            "Table 6 presents the UTUC evaluation results on the SLU-GLUE benchmark. Alongside the modular system, we include Qwen-Audio, the most comprehensive open-sourced speech-LLM (available at the time of this study), for comparison. Qwen-Audio serves as a relevant baseline to WHISMA, since its training data includes SLU tasks similar to those used in our work. Furthermore, the system supports MR inference. As shown in the table, WHISMA demonstrates promising generalisation capabilities to unseen tasks during training. Specifically, WHISMA+MR achieves the highest performance among all the speech-LLMs, with an averaged\n",
            "\n",
            "accuracy of 79.0% across the five sub-tasks. This represents relative gains of 1.3% and 33.0% over Whisper-Llama-3 and Qwen-Audio+MR, respectively. Consistent with the observations in STSC and STUC settings, incorporating ASR into SLU substantially enhances WHISMA's performance on the challenging SLU-GLUE data, resulting in a relative gain of 9.3% when MR inference is performed. In contrast, QwenAudio+MR only provides minimal improvements over the SLU-alone inference fashion.\n",
            "\n",
            "In Table 6, we further illustrate the impact of utilising the Spoken-Alpaca dataset into the fine-tuning process of WHISMA. One can see that excluding this dataset results in a notable performance decline for WHISMA, with a 20.1% relative decrease in averaged accuracy when ASR is omitted. Similar degradation is observed with SCoT and MR inference, suffering relative reductions of 4.1% and 13.9% in averaged accuracy, respectively. Therefore, although SpokenAlpaca contributes only a small fraction (2.2% of total duration) of the entire training data, it plays a crucial role in strengthening the generalisability of speech-LLMs.\n",
            "\n",
            "## \n",
            "\n",
            "\n",
            "</ctx>\n",
            "\n",
            "--Response--\n",
            "1. Always and only respond in HTML tag formatted which should render on website smoothly. Do not use markdown, plain text, or any non-HTML formatting. Use semantic HTML tags like <h1> to <h6> for headings, <p> for paragraphs, <ul> and <li> for lists, <strong> for bold, <em> for italics, <mark> for highlights, <code> for inline code, <pre> for code blocks, <a> for links, and <table> for tabular data. Ensure the HTML is well-formed, valid, and self-contained (no external CSS or JavaScript)\n",
            "2. Cite relevant chunks from the context using ^[Context_n] notation. For multiple citations, use ^[Context_1] ^[Context_2], etc.\n",
            "3. Respond in customizable response lengths:\n",
            "   - Concise\n",
            "   - Medium\n",
            "   - Detailed\n",
            "4. Respond in customizable Format:\n",
            "  - use <ul> and <li> for bullets points and <p> for paragraph\n",
            "5. For questions unrelated to the given context, return : \"Sorry I dont have information about this,Please ask related to your research paper only.\" or politely deny and guide the user back to the given context.\n",
            "\n",
            "--Task--\n",
            "1. Answer questions using only the content from the provided context. Do not rely on external knowledge.\n",
            "2. Keep responses concise, crisp, and to the point unless the user requests a detailed answer.\n",
            "3. Always cite relevant chunks from the context using ^[Context_n] notation.\n",
            "4. If the user asks for a specific format (bullet points or paragraphs), adhere to their request.\n",
            "\n",
            "You: what is the dataset and benchmark for this\n",
            "Assistant: <h1>Datasets and Benchmarks</h1>\n",
            "\n",
            "<p>WHISMA uses several datasets for training and various benchmarks for evaluation.  </p>\n",
            "\n",
            "<h2>Training Datasets:</h2>\n",
            "<ul>\n",
            "  <li><strong>GigaSpeech (medium subset):</strong> Used for automatic speech recognition (ASR). ^[Context_2]</li>\n",
            "  <li><strong>SLURP (zero-shot split):</strong> Used for intent classification (IC) and slot filling (SF). ^[Context_2]</li>\n",
            "  <li><strong>LibriSQA (part I and II):</strong> Used for spoken question answering (SQA). ^[Context_2]</li>\n",
            "  <li><strong>Spoken-Alpaca (newly developed):</strong>  Derived from the text-based Alpaca corpus, with speech synthesized for instruction tuning (SIT and SQIT).  This dataset is publicly available. ^[Context_2]</li>\n",
            "</ul>\n",
            "<p>The total training data amounts to approximately 2000 hours of speech. ^[Context_2]</p>\n",
            "\n",
            "\n",
            "<h2>Evaluation Benchmarks:</h2>\n",
            "<ul>\n",
            "  <li><strong>SLURP (zero-shot split):</strong> Used for seen-task-seen-corpus (STSC) evaluation, focusing on slot filling.  A zero-shot split is created by holding out 5 slot types. ^[Context_0]</li>\n",
            "  <li><strong>FSC and SmartLight:</strong> Used for seen-task-unseen-corpus (STUC) evaluation, focusing on IC and SF.  FSC labels are reorganized into 15 intents and 2 slots; SmartLight uses the full close-field subset. ^[Context_0]</li>\n",
            "  <li><strong>SLU-GLUE (newly developed):</strong> A task-agnostic benchmark for unseen-task-unseen-corpus (UTUC) evaluation.  It includes sentiment analysis (SA), semantic equivalence recognition (SER), and spoken-textual entailment recognition (STER) tasks, derived from ASR-GLUE.  This benchmark is also publicly available. ^[Context_0]</li>\n",
            "</ul>\n",
            "\n",
            "\n",
            "You: return their experimentys ,with all subsection any table result\n",
            "['Describe the experimental setup, including subsections on data, methods, and evaluation metrics, and provide any tabular results.', 'Detail the experiments conducted, outlining the data used, methodology, evaluation benchmarks, and presenting all results in tables if available.']\n",
            "Query Expanded:\n",
            " Describe the experimental setup, including subsections on data, methods, and evaluation metrics, and provide any tabular results.\n",
            "System Prompt: -Role--\n",
            "You are a question-answering chatbot for research paper. Your job is to answer user queries strictly using the content provided in the delimited by <ctx></ctx>). You cannot use any external knowledge or information outside the provided context.\n",
            "\n",
            "<ctx>\n",
            "content_0:  4. EXPERIMENTS\n",
            "\n",
            "## 4.1. Experimental setup\n",
            "\n",
            "The speech encoder and text decoder of WHISMA are based on Whisper-large-v2 and Llama-3-8B-Instruct models, respectively. The modality aligner adopts two 1-D CNN layers with a stride of 2, down-sampling the speech by a factor of 8 along with the Whisper encoder. This configuration produces 375 speech embeddings per input. The bottleneck dimension of the adaptor is set to 320. Within Llama-3, a light-weight LoRA scheme with a rank of 8 and an alpha of 16 is implemented. WHISMA is fine-tuned on 6 V6000 GPUs using a batch size of 12. To prevent over-fitting, the system undergoes only one epoch of training, using the AdamW optimiser with a constant learning rate of 0.0001.\n",
            "\n",
            "## \n",
            "\n",
            "content_1:  4.2. Evaluation tasks\n",
            "\n",
            "We evaluate the proposed WHISMA model under three levels of zero-shot settings, ranging from easy to hard:\n",
            "\n",
            "- · seen-task-seen-corpus (STSC) , targeting the SF task using the zero-shot split of the SLURP dataset [14].\n",
            "- · seen-task-unseen-corpus (STUC) , focusing on IC and SF tasks within the test sets from FSC [15] and SmartLight [16] benchmarks.\n",
            "- · unseen-task-unseen-corpus (UTUC) , representing the most challenging scenario, utilising a new SLU benchmark named SLU-GLUE.\n",
            "\n",
            "For the STSC and STUC evaluations, we focus on data within the domain of in-home robot assistant. The same setups as\n",
            "\n",
            "Table 3 . STSC zero-shot evaluation. WER (%) and SF SLU-F1 (%) on the zero-shot test set of SLURP.\n",
            "\n",
            "| Model               | WER ↓   | SLU-F1 ↑   |\n",
            "|---------------------|---------|------------|\n",
            "| supervised          |         |            |\n",
            "| SNLU [32]           | 13.6    | 69.9       |\n",
            "| zero-shot           |         |            |\n",
            "| ZS-Whisper-SLU [25] | 8.3     | 50.0       |\n",
            "| WHISMA (proposed)   | -       | 46.8       |\n",
            "| + SCoT              | 11.9    | 63.1       |\n",
            "| + MR                | 13.8    | 63.3       |\n",
            "\n",
            "described in [25] are followed to structure the test data. In the case of SLURP, a zero-shot data split is created by holding out 5 slot types: { podcast name, artist name, audiobook name, business name, radio name } , which results in a test set containing 18k utterances. Regarding FSC, the original { action, object, location } labels are reorganised into 15 intents and 2 slots. As for SmartLight, we employ the full close-field subset as the test set, comprising 6 intents and 3 slot types.\n",
            "\n",
            "In addition to the task-oriented datasets mentioned above, we curate SLU-GLUE for the UTUC evaluation in our experiment. Derived from ASR-GLUE [31], SLU-GLUE comprises three primary tasks: sentiment analysis (SA), semantic equivalence recognition (SER), and spoken-textual entailment recognition (STER). These tasks encompass five sub-tasks, namely: Stanford Sentiment Treebank (SST-2), Quora Question Pairs (QQP), Question-answering NLI (QNLI), Recognizing Textual Entailment (RTE), and SciTail. We exclude Semantic Textual Similarity Benchmark (STS-B) as it is not suitable for zero-shot evaluation. The evaluation metric for all the tasks is binary accuracy. The data domains of SLUGLUE span from everyday language, such as movie reviews, to specialised scientific terminology. Speech samples in the dataset are recorded by six native speakers and are mixed with various levels of noise. Further detailed information of SLU-GLUE is provided in Table. 2.\n",
            "\n",
            "The performance of WHISMA is compared against multiple robust baselines, including state-of-the-art (SOTA) supervised models, SOTA zero-shot models, the modular ASRLLM counterpart to WHISMA, and a well-established opensourced speech-LLM called Qwen-Audio [9].\n",
            "\n",
            "## \n",
            "\n",
            "content_2:  3. METHOD\n",
            "\n",
            "In this section, we present the architecture of the proposed WHISMA system and describe the fine-tuning speech tasks along with their corresponding datasets. Additionally, we introduce the training strategy that integrates ASR auxiliary into SLU tasks to enhance the reliability of LLM inference, while preserving WHISMA's end-to-end (E2E) nature.\n",
            "\n",
            "## \n",
            "\n",
            "content_3:  1. INTRODUCTION\n",
            "\n",
            "Traditional speech processing techniques typically depend on specialised models tailored to individual tasks. These models, trained with limited data and constrained architectures, often face difficulties in generalising to new domains and applications. However, recent advancements in speech foundation models (SFMs) [1, 2, 3] and large language models (LLMs) [4, 5, 6] have fundamentally reshaped this approach, leading to the emergence of multi-functional end-to-end (E2E) speech-LLM systems [7, 8, 9, 10]. By utilising off-the-shelf SFMs and LLMs, the development of a speech-LLM system can be streamlined, often requiring only optimisation of adaptors and eliminating the need for extensive training data.\n",
            "\n",
            "Similar to their text-based counterparts, speech-LLMs perform speech tasks through instruction following. Each task is defined by a textual prompt, which the LLM decoder processes alongside the speech embeddings produced by the SFMencoder. Conditioned on the speech input, the LLM then generates a response to fulfill the given instruction. Leveraging the vast knowledge embedded in the LLM decoder,\n",
            "\n",
            "speech-LLMs demonstrate emergent abilities not explicitly imparted during training [8]. Additionally, the prompt-driven approach enables speech-LLMs to address various speech classification tasks in a zero-shot manner. In this paradigm, users maintain the flexibility to incorporate different class labels into the prompt, rather than relying on fixed classification heads as used in conventional methods. These attributes notably enhance the potential of speech-LLMs to serve as a universal solution in the speech industry.\n",
            "\n",
            "Previous studies have explored a wide range of speechcentric challenges within the speech-LLM framework. However, the evaluation of its spoken language understanding (SLU) capabilities, particularly in zero-shot scenarios, remains limited. While several existing speech-LLMs have showcased competence in common speech tasks like automatic speech recognition (ASR), speech-to-text translation (ST), and spoken question answering (SQA), their effectiveness in some key SLU tasks such as intent classification (IC) and slot filling (SF) has not been thoroughly examined.\n",
            "\n",
            "To address this gap, this paper introduces WHISMA, a speech-LLM system designed to enhance the zero-shot SLU performance across various domains. In this context, SLU refers to inferring semantics from spoken utterances. The proposed system employs Whisper [11] and Llama-3 [6] models as the speech encoder and text decoder, respectively. These components remain fixed during training and are connected by a trainable modality aligner. Low-rank adaptation (LoRA) [12] is implemented on Llama-3 to accommodate speech modality inputs. WHISMA is fine-tuned using approximately 2000 hours of speech data, covering tasks of ASR, IC, SF, SQA, and spoken (query) instruction tuning (SQIT/SIT). We adopt a training strategy that enables the system to perform an auxiliary ASR step before SLU through speech chain-of-thought (SCoT) [10] or multi-round (MR) inference, while maintaining the E2E property of WHISMA.\n",
            "\n",
            "To facilitate reproducing the proposed system, we ensure that all the training and test data used in this work is openly accessible. Specifically, we publish our self-developed Spoken-Alpaca dataset utilised for the SIT task. Derived from the text-based Alpaca instruction tuning corpus [13], we conduct pre-processing on the input and instruction fields, and synthesise speech for them using an in-house text-to-speech (TTS) model. Moreover, we curate a new SLU benchmark,\n",
            "\n",
            "named SLU-GLUE, to assess speech-LLMs on task-agnostic data beyond IC and SF. Tasks within SLU-GLUE include sentiment analysis (SA), semantic equivalence recognition (SER), and spoken-textual entailment recognition (STER). This benchmark is also made publicly available.\n",
            "\n",
            "The contributions of our work are summarised as follows:\n",
            "\n",
            "- · We introduce WHISMA, a speech-LLM that incorporates cutting-edge speech (Whisper) and language (Llama-3) foundation models to support a diverse range of SLU tasks in zero-shot settings.\n",
            "- · Through evaluations on the SLU tasks seen in training, WHISMA demonstrates superior zero-shot performance compared to existing baselines on the SLURP [14], FSC [15] and SmartLight [16] benchmarks.\n",
            "- · To illustrate the robustness of WHISMA on unseen SLU tasks, we further evaluate the system on the curated SLU-GLUE benchmark 1 , showing that it outperforms both the modular Whisper-Llama-3 system and an existing speech-LLM, Qwen-Audio [9].\n",
            "- · To mitigate over-fitting during WHISMA training, we develop the Spoken-Alpaca 2 dataset for speech-based instruction tuning, and show that it facilitates the model generalisation to unseen tasks.\n",
            "\n",
            "## \n",
            "\n",
            "content_4:  4.3. Main results\n",
            "\n",
            "The STSC evaluation results on the zero-shot test set of SLURP are presented in Table 3, focusing on ASR worderror-rate (WER) and SF SLU-F1 score. To ensure a fair com-\n",
            "\n",
            "Table 4 . STUC zero-shot evaluation. WER (%) and IC accuracy (IC Acc. %) on the test set of FSC.Table 5 . STUC zero-shot evaluation. WER (%), IC accuracy (IC Acc. %), SF SLU-F1 (%), and perfect parsing (PP, %) on the full set of SmartLight.\n",
            "\n",
            "| Model                     | WER ↓   | IC Acc. ↑   |\n",
            "|---------------------------|---------|-------------|\n",
            "| supervised                |         |             |\n",
            "| Finstreder [33]           | -       | 99.7        |\n",
            "| zero-shot                 |         |             |\n",
            "| BLSP [7]                  | -       | 77.5        |\n",
            "| ZS-Whisper-SLU [25]       | 0.8     | 95.0        |\n",
            "| Whisper-Llama-3 (modular) | 1.5     | 89.3        |\n",
            "| WHISMA (proposed)         | -       | 90.9        |\n",
            "| + SCoT                    | 1.7     | 97.3        |\n",
            "| + MR                      | 2.7     | 97.3        |\n",
            "\n",
            "| Model                     | WER ↓   | IC Acc. ↑   | SLU-F1 ↑   | PP ↑   |\n",
            "|---------------------------|---------|-------------|------------|--------|\n",
            "| supervised                |         |             |            |        |\n",
            "| Finstreder [33]           | 6.1     | -           | -          | 88.0   |\n",
            "| Whisper-TS [24]           | -       | 96.3        | -          | -      |\n",
            "| zero-shot                 |         |             |            |        |\n",
            "| BLSP [7]                  | -       | 78.8        | -          | -      |\n",
            "| ZS-Whisper-SLU [25]       | 2.7     | 91.6        | 90.9       | 82.5   |\n",
            "| UniverSLU-14 [24]         | -       | 44.6        | -          | -      |\n",
            "| Whisper-Llama-3 (modular) | 4.0     | 94.3        | 82.4       | 75.7   |\n",
            "| WHISMA (proposed)         | -       | 81.7        | 87.8       | 68.9   |\n",
            "| + SCoT                    | 4.5     | 95.9        | 90.2       | 82.5   |\n",
            "| + MR                      | 5.5     | 94.4        | 90.7       | 81.6   |\n",
            "\n",
            "parison, the modular Whisper-Llama-3 system is excluded as it has not been tuned on the SLURP dataset. For supervised model performance, we re-implement the spoken and natural language understanding (SNLU) system described in [32] and conduct the same evaluation. Given the rich knowledge embedded in Llama-3 and the wide array of SLU datasets used in training, our proposed WHISMA system with either SCoT or MR inference significantly outperforms the existing SOTA model, ZS-Whisper-SLU, achieving a 26.6% relative gain in SLU-F1. This comparison is equitable as ZS-whisper-SLU also conducts ASR prior to SF. The SCoT inference strategy achieves a lower WER compared to MR (11.9% vs. 13.8%), which we believe is because SCoT combines both tasks into a\n",
            "\n",
            "Table 6 . UTUC zero-shot evaluation. Accuracy (%) on SLU-GLUE sub-tasks.\n",
            "\n",
            "| Model                     |   SST-2 ↑ |   QQP ↑ |   QNLI ↑ |   RTE ↑ |   SciTail ↑ |   Avg. ↑ |\n",
            "|---------------------------|-----------|---------|----------|---------|-------------|----------|\n",
            "| Qwen-Audio [9]            |      53.4 |    62.4 |     58.8 |    65   |        54.5 |     58.8 |\n",
            "| + MR                      |      52.9 |    68.4 |     52.4 |    67.2 |        56.3 |     59.4 |\n",
            "| Whisper-Llama-3 (modular) |      88.3 |    74.9 |     81.6 |    76.7 |        68.3 |     78   |\n",
            "| WHISMA (proposed)         |      79   |    65.1 |     84   |    72.8 |        60.5 |     72.3 |\n",
            "| + SCoT                    |      88   |    65.1 |     78.9 |    73.7 |        71.7 |     75.5 |\n",
            "| + MR                      |      89.2 |    74.2 |     88   |    78   |        65.4 |     79   |\n",
            "| WHISMA w/o Spoken-Alpaca  |      78.6 |    45.9 |     50.9 |    61.2 |        52.6 |     57.8 |\n",
            "| + SCoT                    |      87.6 |    69   |     64.8 |    74.6 |        66.2 |     72.4 |\n",
            "| + MR                      |      86.6 |    52.6 |     73.8 |    70   |        57   |     68   |\n",
            "\n",
            "single prompt, enhancing ASR with the information in the SF instruction. WHISMA exhibits competitive SF performance (46.8%) to ZS-Whisper-SLU even without utilising speech transcripts. However, it lags behind cases with SCoT or MR inference, which emphasises the importance of performing ASR for SF tasks.\n",
            "\n",
            "The results of STUC evaluations on the FSC and SmartLight benchmarks are provided in Table 4 and 5, respectively. In the FSC benchmark, WHISMA+SCoT/MR obtains superior zero-shot performance compared to both the BLSP and ZS-Whisper-SLU baselines, showing relative improvements of 25.5% and 2.4% in IC accuracy. For the SmartLight benchmark, WHISMA+SCoT achieves SOTA performance in IC accuracy (95.9%) compared to all the zero-shot baselines. It also delivers SF SLU-F1 (90.2%) and perfect parsing (PP, 82.5%) results comparable to ZS-Whisper-SLU. Similar performance is observed with MR inference. On both benchmarks, WHISMA without ASR still surpasses the modular Whisper-Llama-3 model in various scenarios, highlighting the reduced complexity of E2E speech-LLMs over the modular approach. Regarding the SCoT and MR inferences, due to the limited ASR data used in fine-tuning, WHISMA generally displays higher WER relative to ZS-Whisper-SLU and Whisper-Llama-3. However, the joint modelling of speech and text modalities in WHISMA+SCoT/MR compensates for this shortcoming, leading to improved SLU performance.\n",
            "\n",
            "Table 6 presents the UTUC evaluation results on the SLU-GLUE benchmark. Alongside the modular system, we include Qwen-Audio, the most comprehensive open-sourced speech-LLM (available at the time of this study), for comparison. Qwen-Audio serves as a relevant baseline to WHISMA, since its training data includes SLU tasks similar to those used in our work. Furthermore, the system supports MR inference. As shown in the table, WHISMA demonstrates promising generalisation capabilities to unseen tasks during training. Specifically, WHISMA+MR achieves the highest performance among all the speech-LLMs, with an averaged\n",
            "\n",
            "accuracy of 79.0% across the five sub-tasks. This represents relative gains of 1.3% and 33.0% over Whisper-Llama-3 and Qwen-Audio+MR, respectively. Consistent with the observations in STSC and STUC settings, incorporating ASR into SLU substantially enhances WHISMA's performance on the challenging SLU-GLUE data, resulting in a relative gain of 9.3% when MR inference is performed. In contrast, QwenAudio+MR only provides minimal improvements over the SLU-alone inference fashion.\n",
            "\n",
            "In Table 6, we further illustrate the impact of utilising the Spoken-Alpaca dataset into the fine-tuning process of WHISMA. One can see that excluding this dataset results in a notable performance decline for WHISMA, with a 20.1% relative decrease in averaged accuracy when ASR is omitted. Similar degradation is observed with SCoT and MR inference, suffering relative reductions of 4.1% and 13.9% in averaged accuracy, respectively. Therefore, although SpokenAlpaca contributes only a small fraction (2.2% of total duration) of the entire training data, it plays a crucial role in strengthening the generalisability of speech-LLMs.\n",
            "\n",
            "## \n",
            "\n",
            "content_5:  3.2. Training data\n",
            "\n",
            "The fine-tuning of WHISMA is conducted with a multi-task learning approach. We compile a substantial training dataset that encompasses the following tasks: automatic speech recognition (ASR), intent classification (IC), slot filling (SF), spoken question answering (SQA), spoken query instruction tuning (SQIT), and spoken instruction tuning (SIT). Although ASR is not categorized as an SLU task, it significantly aids in aligning between the speech and text modalities. These tasks correspond to data from different sources, including the medium subset of GigaSpeech [26], the zero-shot data split of SLURP [14], LibriSQA-partI for open-ended QA, partII for multi-choice QA [27], and Spoken-Alpaca, developed as part of this research endeavor. The total duration of speech data amounts to approximately 2000 hours. Detailed information on this dataset is provided in Table 1.\n",
            "\n",
            "Although previous studies have employed Alpaca [13] with synthesised speech for instruction tuning [10, 22], the associated data remains unpublished, which hinders the replication of their findings. To remedy this, we introduce Spoken-Alpaca and release it for public access. The original text-based Alpaca dataset comprises two types of examples: i) containing fields { instruction, input, output } , where instruction describes the task, input provides context, and output is the expected system response; and ii) with fields\n",
            "\n",
            "{ instruction, output } , where the instruction stands alone as a query, not requiring contextual information. For the first type of examples, we generate speech for the input field while retaining instruction as the text prompt for the speechLLM system. This configuration defines the task referred to as spoken instruction tuning (SIT). Concerning the second type, speech synthesis is applied to instruction , which the system receives without a text prompt. This task category is denoted as spoken query instruction tuning (SQIT). We develop an in-house variational inference with adversarial learning for end-to-end text-to-speech (VITS) model [28] using the Librispeech [29] dataset featuring around 2200 speakers. Examples in Alpaca unsuitable for speech synthesis are filtered out, including those containing lengthy texts, mathematical equations, tables, etc. We also alter the wording of some instructions to make them speech-oriented. The diverse instructions within Spoken-Alpaca could significantly reduce the risk of over-fitting WHISMA to seen tasks with restricted prompt variations, such as ASR, IC and SF. Our experiments (Section 4.3) demonstrate that incorporating the dataset enables the proposed system to generalise more effectively to unseen tasks.\n",
            "\n",
            "## \n",
            "\n",
            "content_6:  2. RELATED WORK\n",
            "\n",
            "Motivated by the success of vision language models (VLMs) [17, 18], there has been a growing interest in extending LLMs with auditory capabilities. Initial attempts to incorporate speech perception into LLMs involve LLM-based ASR models [19, 20]. Building upon this, Speech-Llama [21] integrates both ST and ASR to the framework through multi-task learning. Further, SLM [22] enhances the system with speech instruction tuning. However, these models typically undergo fine-tuning with a limited set of tasks and lack generalised understanding abilities.\n",
            "\n",
            "Recent advancements have introduced speech-LLMs with more diverse speech processing functions. Notable examples include BLSP [7], LLaSM [23], SALMONN [8], QwenAudio [9], and WavLLM [10]. Most of these systems utilise a variety of speech or audio resources to achieve cross-modal perception. Among them, zero-shot SLU evaluation results have been reported by BLSP for IC and SA tasks, SALMONN for a SF task, and WavLLM for a SQA task. In this work, we aim to conduct a comprehensive evaluation on these tasks using our proposed WHISMA system.\n",
            "\n",
            "UniverSLU [24] is the latest study that exclusively examines the SLU capabilities of the SFM, which fine-tunes the\n",
            "\n",
            "Fig. 1 . An overview of WHISMA model architecture.\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "Whisper model with up to 17 SLU datasets encompassing both acoustic and semantic domain tasks. The zero-shot performance of UniverSLU, as reported, reveals that the system struggles to generalise to unseen datasets and tasks, which is the issue we would like to address in this work.\n",
            "\n",
            "The most relevant research to this study is ZS-WhisperSLU proposed by [25], which investigates zero-shot IC and SF tasks within a QA-driven framework based on Whisper. Despite the promising performance achieved by the system, it is challenging to utilise ZS-Whisper-SLU for unseen tasks due to the limitation of the Whisper decoder. In this regard, we seek to bridge this gap by leveraging an LLM that possess broader knowledge than an SFM.\n",
            "\n",
            "## \n",
            "\n",
            "content_7:  3.3. Training strategy\n",
            "\n",
            "The training examples are organised according to Llama-3's standard prompt template, as outlined in Fig. 1. To enhance the robustness of WHISMA in handling diverse instructions during inference, we devise 10 distinct prompts for each task in ASR, IC, and SF. These prompts are randomly selected for each training example during fine-tuning. For the remaining tasks, we directly employ the provided question or instruction from the data as the text prompt.\n",
            "\n",
            "Unlike ZS-Whisper-SLU [25], which tackles IC and SF through prompting the text decoder with individual questions for each intent class or slot type, WHISMA performs the tasks more efficiently by amalgamating candidate labels into a single prompt. Example prompts are: 'Classify the intent of the spoken utterance into one of the following labels: [intent 1], [intent 2], ..., [intent N]' for IC, and 'Perform slot filling on the spoken utterance for the following slots: [slot 1], [slot 2], ..., [slot M]' for SF. During training, we do not include all the candidate labels into the prompts but instead sample a varying number of them (including the ground-truth label) for each example. This approach introduces diversity to the prompts and helps prevent over-fitting.\n",
            "\n",
            "The primary challenge of E2E SLU lies in the direct extraction of semantic elements from speech, particularly for the SF task, which requires identifying entities explicitly mentioned in the user utterances. Previous studies indicate that ASR can significantly improve SLU performance [30]. We investigate this method for our proposed model. To distinguish WHISMA from modular ASR-LLM systems, speech transcription is performed as a part of the model inference,\n",
            "\n",
            "Table 2 . SLU-GLUE benchmark.\n",
            "\n",
            "| Task   |         |                                                                         |      |\n",
            "|--------|---------|-------------------------------------------------------------------------|------|\n",
            "|        | SST-2   | Classify the sentiment of [SPEECH] into positive or negative.           | 2790 |\n",
            "| SER    | QQP     | Identify if the question in is a paraphrase of the question in [TEXT] . |      |\n",
            "|        |         | Identify if the context in                                              |      |\n",
            "|        |         | Identify if the sentence in [SPEECH] entails the sentence in [TEXT] .   | 2088 |\n",
            "|        | SciTail | Identify if the premise in [SPEECH] supports the hypothesis in [TEXT] . | 2736 |\n",
            "\n",
            "Fig. 2 . E2E Inference strategies integrating ASR to SLU.\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "maintaining its E2E decoding property. We explore two strategies to integrate an auxiliary ASR step into SLU tasks:\n",
            "\n",
            "- · Speech chain-of-thought (SCoT) [10]: The system processes a concatenation of the ASR and SLU prompts, executing the tasks one-shot in sequential order. During inference, the SLU response is conditioned on the previously generated speech transcript.\n",
            "- · Multi-round (MR): Inference is conducted in multiple dialogue rounds. The system is first instructed to produce a speech transcript, then based on the speech embeddings and the transcript (in the dialogue history), we prompt the system to execute the SLU task.\n",
            "\n",
            "An illustration of the above inference strategies is depicted in Fig. 2. In SLU tasks (excluding SQIT), each example is randomly assigned to one of the following training configurations: SLU-alone, SCoT-ASR-SLU, and MR-ASR-SLU. Despite the fact that LLMs are inherently capable of executing CoT and MR inference, our observation suggests that speechLLMs do not develop such capabilities unless integrated into the fine-tuning process.\n",
            "\n",
            "## \n",
            "\n",
            "content_8:  3.1. Model architecture\n",
            "\n",
            "The model architecture of WHISMA is illustrated in Fig. 1. We adopt the encoder from Whisper-large-v2 , a 32layer Transformer model with two convolutional neural network (CNN) down-samplers, as the speech encoder in our system. The resulting speech embeddings are fed to a modality aligner, which converts these embeddings to align with the input space of the LLM decoder. Our modality aligner is structured similarly to a previous system [10], consisting\n",
            "\n",
            "Table 1 . Multi-task training dataset.\n",
            "\n",
            "| Task   | Data Source                              | #Hours   | #Samples   |\n",
            "|--------|------------------------------------------|----------|------------|\n",
            "| ASR    | GigaSpeech (M) [26]                      | 1000     | 910k       |\n",
            "| IC     | SLURP (zero-shot) [14]                   | 80       | 94k        |\n",
            "| SF     | SLURP (zero-shot) [14]                   | 80       | 94k        |\n",
            "| SQA    | LibriSQA-partI [27] LibriSQA-partII [27] | 360      | 104k 104k  |\n",
            "| SQIT   | Spoken-Alpaca                            | 360 30   | 29k        |\n",
            "| SIT    | Spoken-Alpaca                            | 14       | 14k        |\n",
            "\n",
            "of two CNN layers for further down-sampling, a bottleneck adaptor, and a linear output layer to match dimensions.\n",
            "\n",
            "WHISMA's decoder is implemented using Llama-38B-Instruct , one of the most promising open-sourced LLMs optimised for instruction following. This decoder processes both the aligned speech embeddings and a taskdescriptive text prompt, generating a response to execute the given instruction. We integrate low-rank adaptation (LoRA) weights into the attention module of all 32 Transformer layers in Llama-3, enabling it to effectively handle speech modality inputs. During training, only the modality aligner and LoRA parameters are optimised.\n",
            "\n",
            "## \n",
            "\n",
            "content_9:  5. CONCLUSION\n",
            "\n",
            "In this paper, we present WHISMA, a speech-LLM system that excels in various zero-shot SLU tasks. WHISMA integrates a Whisper-based speech encoder with a Llama-3 text decoder, and is fine-tuned on a diverse set of SLU tasks using a modality aligner and LoRA adaptors. Additionally, we enable the system to perform an auxiliary ASR step before SLU through SCoT or MR inference strategies. Comprehensive zero-shot evaluations demonstrate WHISMA's ability to achieve SOTA performance on several common SLU benchmarks and, more importantly, to generalise to tasks not encountered during training. To ensure the reproducibility of our results, we release the Spoken-Alpaca and SLU-GLUE datasets utilised in our experiments for public access.\n",
            "\n",
            "## \n",
            "\n",
            "\n",
            "</ctx>\n",
            "\n",
            "--Response--\n",
            "1. Always and only respond in HTML tag formatted which should render on website smoothly. Do not use markdown, plain text, or any non-HTML formatting. Use semantic HTML tags like <h1> to <h6> for headings, <p> for paragraphs, <ul> and <li> for lists, <strong> for bold, <em> for italics, <mark> for highlights, <code> for inline code, <pre> for code blocks, <a> for links, and <table> for tabular data. Ensure the HTML is well-formed, valid, and self-contained (no external CSS or JavaScript)\n",
            "2. Cite relevant chunks from the context using ^[Context_n] notation. For multiple citations, use ^[Context_1] ^[Context_2], etc.\n",
            "3. Respond in customizable response lengths:\n",
            "   - Concise\n",
            "   - Medium\n",
            "   - Detailed\n",
            "4. Respond in customizable Format:\n",
            "  - use <ul> and <li> for bullets points and <p> for paragraph\n",
            "5. For questions unrelated to the given context, return : \"Sorry I dont have information about this,Please ask related to your research paper only.\" or politely deny and guide the user back to the given context.\n",
            "\n",
            "--Task--\n",
            "1. Answer questions using only the content from the provided context. Do not rely on external knowledge.\n",
            "2. Keep responses concise, crisp, and to the point unless the user requests a detailed answer.\n",
            "3. Always cite relevant chunks from the context using ^[Context_n] notation.\n",
            "4. If the user asks for a specific format (bullet points or paragraphs), adhere to their request.\n",
            "\n",
            "You: return their experimentys ,with all subsection any table result\n",
            "Assistant: <h1>Experiments</h1>\n",
            "\n",
            "<h2>4.1 Experimental Setup</h2>\n",
            "<p>WHISMA uses Whisper-large-v2 for the speech encoder and Llama-3-8B-Instruct for the text decoder. A modality aligner (two 1-D CNN layers) downsamples speech by a factor of 8, producing 375 speech embeddings. The adaptor bottleneck dimension is 320.  A lightweight LoRA scheme (rank 8, alpha 16) is used in Llama-3. Fine-tuning is done on 6 V6000 GPUs with a batch size of 12 for one epoch using the AdamW optimizer and a learning rate of 0.0001. ^[Context_0]</p>\n",
            "\n",
            "\n",
            "<h2>4.2 Evaluation Tasks</h2>\n",
            "<p>Three zero-shot evaluation settings are used:</p>\n",
            "<ul>\n",
            "  <li><strong>Seen-task-seen-corpus (STSC):</strong> Uses the zero-shot split of the SLURP dataset for slot filling (SF), holding out 5 slot types. ^[Context_0]</li>\n",
            "  <li><strong>Seen-task-unseen-corpus (STUC):</strong> Uses FSC and SmartLight test sets for IC and SF. FSC labels are reorganized; SmartLight uses the full close-field subset. ^[Context_0]</li>\n",
            "  <li><strong>Unseen-task-unseen-corpus (UTUC):</strong> Uses the newly created SLU-GLUE benchmark, including SA, SER, and STER tasks. ^[Context_0]</li>\n",
            "</ul>\n",
            "<p>WHISMA's performance is compared against supervised models, zero-shot models, a modular ASR-LLM counterpart, and Qwen-Audio. ^[Context_0]</p>\n",
            "\n",
            "\n",
            "<h2>4.3 Main Results</h2>\n",
            "\n",
            "<h3>Table 3: STSC Evaluation (SLURP)</h3>\n",
            "<table border=\"1\">\n",
            "  <tr>\n",
            "    <th>Model</th>\n",
            "    <th>WER ↓</th>\n",
            "    <th>SLU-F1 ↑</th>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>supervised SNLU</td>\n",
            "    <td>13.6</td>\n",
            "    <td>69.9</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>zero-shot ZS-Whisper-SLU</td>\n",
            "    <td>8.3</td>\n",
            "    <td>50.0</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>WHISMA (proposed)</td>\n",
            "    <td>-</td>\n",
            "    <td>46.8</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>WHISMA + SCoT</td>\n",
            "    <td>11.9</td>\n",
            "    <td>63.1</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>WHISMA + MR</td>\n",
            "    <td>13.8</td>\n",
            "    <td>63.3</td>\n",
            "  </tr>\n",
            "</table>\n",
            "\n",
            "<h3>Table 4: STUC Evaluation (FSC)</h3>\n",
            "<table border=\"1\">\n",
            "  <tr>\n",
            "    <th>Model</th>\n",
            "    <th>WER ↓</th>\n",
            "    <th>IC Acc. ↑</th>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>supervised Finstreder</td>\n",
            "    <td>-</td>\n",
            "    <td>99.7</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>zero-shot BLSP</td>\n",
            "    <td>-</td>\n",
            "    <td>77.5</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>zero-shot ZS-Whisper-SLU</td>\n",
            "    <td>0.8</td>\n",
            "    <td>95.0</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>Whisper-Llama-3 (modular)</td>\n",
            "    <td>1.5</td>\n",
            "    <td>89.3</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>WHISMA (proposed)</td>\n",
            "    <td>-</td>\n",
            "    <td>90.9</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>WHISMA + SCoT</td>\n",
            "    <td>1.7</td>\n",
            "    <td>97.3</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>WHISMA + MR</td>\n",
            "    <td>2.7</td>\n",
            "    <td>97.3</td>\n",
            "  </tr>\n",
            "</table>\n",
            "\n",
            "<h3>Table 5: STUC Evaluation (SmartLight)</h3>\n",
            "<table border=\"1\">\n",
            "  <tr>\n",
            "    <th>Model</th>\n",
            "    <th>WER ↓</th>\n",
            "    <th>IC Acc. ↑</th>\n",
            "    <th>SLU-F1 ↑</th>\n",
            "    <th>PP ↑</th>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>supervised Finstreder</td>\n",
            "    <td>6.1</td>\n",
            "    <td>-</td>\n",
            "    <td>-</td>\n",
            "    <td>88.0</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>zero-shot BLSP</td>\n",
            "    <td>-</td>\n",
            "    <td>78.8</td>\n",
            "    <td>-</td>\n",
            "    <td>-</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>zero-shot ZS-Whisper-SLU</td>\n",
            "    <td>2.7</td>\n",
            "    <td>91.6</td>\n",
            "    <td>90.9</td>\n",
            "    <td>82.5</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>zero-shot UniverSLU-14</td>\n",
            "    <td>-</td>\n",
            "    <td>44.6</td>\n",
            "    <td>-</td>\n",
            "    <td>-</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>Whisper-Llama-3 (modular)</td>\n",
            "    <td>4.0</td>\n",
            "    <td>94.3</td>\n",
            "    <td>82.4</td>\n",
            "    <td>75.7</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>WHISMA (proposed)</td>\n",
            "    <td>-</td>\n",
            "    <td>81.7</td>\n",
            "    <td>87.8</td>\n",
            "    <td>68.9</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>WHISMA + SCoT</td>\n",
            "    <td>4.5</td>\n",
            "    <td>95.9</td>\n",
            "    <td>90.2</td>\n",
            "    <td>82.5</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>WHISMA + MR</td>\n",
            "    <td>5.5</td>\n",
            "    <td>94.4</td>\n",
            "    <td>90.7</td>\n",
            "    <td>81.6</td>\n",
            "  </tr>\n",
            "</table>\n",
            "\n",
            "<h3>Table 6: UTUC Evaluation (SLU-GLUE)</h3>\n",
            "<table border=\"1\">\n",
            "  <tr>\n",
            "    <th>Model</th>\n",
            "    <th>SST-2 ↑</th>\n",
            "    <th>QQP ↑</th>\n",
            "    <th>QNLI ↑</th>\n",
            "    <th>RTE ↑</th>\n",
            "    <th>SciTail ↑</th>\n",
            "    <th>Avg. ↑</th>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>Qwen-Audio</td>\n",
            "    <td>53.4</td>\n",
            "    <td>62.4</td>\n",
            "    <td>58.8</td>\n",
            "    <td>65</td>\n",
            "    <td>54.5</td>\n",
            "    <td>58.8</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>Qwen-Audio + MR</td>\n",
            "    <td>52.9</td>\n",
            "    <td>68.4</td>\n",
            "    <td>52.4</td>\n",
            "    <td>67.2</td>\n",
            "    <td>56.3</td>\n",
            "    <td>59.4</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>Whisper-Llama-3 (modular)</td>\n",
            "    <td>88.3</td>\n",
            "    <td>74.9</td>\n",
            "    <td>81.6</td>\n",
            "    <td>76.7</td>\n",
            "    <td>68.3</td>\n",
            "    <td>78</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>WHISMA (proposed)</td>\n",
            "    <td>79</td>\n",
            "    <td>65.1</td>\n",
            "    <td>84</td>\n",
            "    <td>72.8</td>\n",
            "    <td>60.5</td>\n",
            "    <td>72.3</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>WHISMA + SCoT</td>\n",
            "    <td>88</td>\n",
            "    <td>65.1</td>\n",
            "    <td>78.9</td>\n",
            "    <td>73.7</td>\n",
            "    <td>71.7</td>\n",
            "    <td>75.5</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>WHISMA + MR</td>\n",
            "    <td>89.2</td>\n",
            "    <td>74.2</td>\n",
            "    <td>88</td>\n",
            "    <td>78</td>\n",
            "    <td>65.4</td>\n",
            "    <td>79</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>WHISMA w/o Spoken-Alpaca</td>\n",
            "    <td>78.6</td>\n",
            "    <td>45.9</td>\n",
            "    <td>50.9</td>\n",
            "    <td>61.2</td>\n",
            "    <td>52.6</td>\n",
            "    <td>57.8</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>WHISMA w/o Spoken-Alpaca + SCoT</td>\n",
            "    <td>87.6</td>\n",
            "    <td>69</td>\n",
            "    <td>64.8</td>\n",
            "    <td>74.6</td>\n",
            "    <td>66.2</td>\n",
            "    <td>72.4</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>WHISMA w/o Spoken-Alpaca + MR</td>\n",
            "    <td>86.6</td>\n",
            "    <td>52.6</td>\n",
            "    <td>73.8</td>\n",
            "    <td>70</td>\n",
            "    <td>57</td>\n",
            "    <td>68</td>\n",
            "  </tr>\n",
            "</table>\n",
            "\n",
            "<p>Detailed analysis of these results is provided in section 4.3 of the original paper. ^[Context_0]</p>\n",
            "\n",
            "\n",
            "You: quit\n",
            "Exiting chat...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# await retrieval.retrieve_chunks('what is tabular chain of thought')"
      ],
      "metadata": {
        "id": "KAfspMVmRzHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem 3: Tool Selection System**\n",
        "\n",
        "Model Requirement: gpt-4o-Mini (OpenAI key will be provided)\n",
        "Task: Develop a tool selection system based on Typeset.io's toolkit.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "● Use tools listed on our website https://typeset.io left sidebar\n",
        "\n",
        "● For every user query, LLM should select one of the tools from the above list.\n",
        "\n",
        "● Each tool should also have its own parameters that the LLM will select from the user query.\n",
        "\n",
        "● You can decide the parameters required for each tool.\n",
        "\n",
        "● Create appropriate tool descriptions and use cases\n",
        "\n",
        "● Handle edge cases and ambiguous queries\n",
        "\n",
        "E.g.\n",
        "\n",
        "User query: I want to find topics related to RLHF for LLM finetuning.\n",
        "\n",
        "Output: Tool: Topic Finder\n",
        "\n",
        "Params: Search query: RLHF for LLM finetuning"
      ],
      "metadata": {
        "id": "C3XA1w0VUCrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interact_with_pdf(pdf_file_url: str = None,\n",
        "                      operation: str = None,\n",
        "                      query: str = None,\n",
        "                      section: str = None,\n",
        "                      highlighted_text: str = None,\n",
        "                      language: str = \"English\",\n",
        "                      note: str = None,\n",
        "                      output_format: str = \"plain_text\",\n",
        "                      citation_style: str = \"APA\",\n",
        "                      related_paper_limit: int = 5) -> dict:\n",
        "    \"\"\"\n",
        "    Interact with a PDF file to perform various operations such as answering questions,\n",
        "    summarizing sections, explaining highlighted text, recommending related papers,\n",
        "    or taking notes.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    pdf_file_url : str\n",
        "        The URL or file path to the PDF. This is required for all operations.\n",
        "    operation : str\n",
        "        The operation to perform on the PDF. Options include:\n",
        "        - \"get_citation_answers\": Get answers backed by citations.\n",
        "        - \"get_summary\": Provide a section-wise or overall summary.\n",
        "        - \"highlight_explanation\": Simplify complex highlighted text.\n",
        "        - \"get_related_papers\": Recommend papers related to highlighted text.\n",
        "        - \"take_notes\": Save notes for future reference.\n",
        "    query : str, optional\n",
        "        A specific question or search query to extract information. Required for \"get_citation_answers\".\n",
        "    section : str, optional\n",
        "        Target a specific section of the PDF, e.g., \"Introduction\" or \"Conclusion\". Default is the entire document.\n",
        "    highlighted_text : str, optional\n",
        "        Specific text from the PDF for explanations or related paper recommendations.\n",
        "        Required for \"highlight_explanation\" and \"get_related_papers\".\n",
        "    language : str, optional\n",
        "        The language for the response. Default is \"English\". Supports 75+ languages.\n",
        "    note : str, optional\n",
        "        Text to save as a note in the \"take_notes\" operation.\n",
        "    output_format : str, optional\n",
        "        The format of the output. Options: \"plain_text\", \"json\", \"markdown\". Default is \"plain_text\".\n",
        "    citation_style : str, optional\n",
        "        Citation style for answers. Options: \"APA\", \"MLA\", \"Chicago\", \"Harvard\". Default is \"APA\".\n",
        "    related_paper_limit : int, optional\n",
        "        Maximum number of related papers to recommend. Default is 5.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        A dictionary containing the result of the requested operation. Keys and values depend on the operation.\n",
        "        For example:\n",
        "        - \"get_citation_answers\": { \"answers\": [...], \"citations\": [...] }\n",
        "        - \"get_summary\": { \"summary\": { \"Introduction\": \"...\", \"Conclusion\": \"...\" } }\n",
        "        - \"highlight_explanation\": { \"simplified_text\": \"...\" }\n",
        "        - \"get_related_papers\": { \"papers\": [...] }\n",
        "        - \"take_notes\": { \"status\": \"Note saved successfully.\" }\n",
        "\n",
        "    Raises:\n",
        "    -------\n",
        "    ValueError:\n",
        "        If required parameters for the chosen operation are missing or invalid.\n",
        "\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "tool1 = {\n",
        "  \"type\": \"function\",\n",
        "  \"function\": {\n",
        "    \"name\": \"chat_with_pdf\",\n",
        "    \"description\": \"\"\"This tool is for Interact with a PDF file to perform various operations such as answering questions, summarizing sections, explaining highlighted text, recommending related papers, or taking notes.\"\"\",\n",
        "    \"parameters\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"pdf_file_url\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"The URL or path to the PDF file. Required to perform any operation on the PDF.\"\n",
        "        },\n",
        "        \"query\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"A question or search query to find specific answers or sections in the PDF. Can be left blank if performing a general operation.\"\n",
        "        },\n",
        "        \"operation\": {\n",
        "          \"type\": \"string\",\n",
        "          \"enum\": [\"get_citation_answers\", \"get_summary\", \"highlight_explanation\", \"get_related_papers\", \"take_notes\"],\n",
        "          \"description\": \"The specific operation to perform on the PDF, such as answering questions, summarizing, or finding related papers.\"\n",
        "        },\n",
        "        \"section\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"The specific section of the PDF to target (e.g., 'Introduction', 'Methods', 'Conclusion'). Leave blank for the entire document.\"\n",
        "        },\n",
        "        \"highlighted_text\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"Specific text from the PDF to get a simplified explanation or find related papers. Required for 'highlight_explanation' and 'get_related_papers' operations.\"\n",
        "        },\n",
        "        \"language\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"The language in which the response should be provided. Default is 'English'. Supports 75+ languages.\",\n",
        "          \"default\": \"English\"\n",
        "        },\n",
        "        \"note\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"Text for the note to be added. Used in the 'take_notes' operation to store custom annotations.\"\n",
        "        },\n",
        "        \"output_format\": {\n",
        "          \"type\": \"string\",\n",
        "          \"enum\": [\"plain_text\", \"json\", \"markdown\"],\n",
        "          \"description\": \"The desired format for the output of the operation.\",\n",
        "          \"default\": \"plain_text\"\n",
        "        },\n",
        "        \"citation_style\": {\n",
        "          \"type\": \"string\",\n",
        "          \"enum\": [\"APA\", \"MLA\", \"Chicago\", \"Harvard\"],\n",
        "          \"description\": \"The citation style for answers backed by citations. Used in 'get_citation_answers' operation.\",\n",
        "          \"default\": \"APA\"\n",
        "        },\n",
        "        \"related_paper_limit\": {\n",
        "          \"type\": \"integer\",\n",
        "          \"description\": \"The maximum number of related papers to fetch. Only used in 'get_related_papers' operation.\",\n",
        "          \"default\": 5\n",
        "        }\n",
        "      },\n",
        "      # \"required\": [\"pdf_file_url\"]\n",
        "    }\n",
        "  },\n",
        "  \"strict\" :True\n",
        "}\n",
        "# tool1"
      ],
      "metadata": {
        "id": "5Rf8J7cmrFc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ai_writer_tool(operation: str, query: str = None, citation_source: str = None, note_content: str = None,\n",
        "                   file_format: str = None, output_format: str = \"text\", language: str = \"English\") -> str:\n",
        "    \"\"\"\n",
        "    AIWriter Tool: An intelligent assistant for writing research papers within typeset notebook with features like citation discovery, text autocompletion,\n",
        "    note management, and paper export, ensuring a seamless academic writing experience.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    operation : str\n",
        "        The type of operation to perform. Supported operations include:\n",
        "        - 'find_citations': Discover and add citations from a vast database of 280M+ research papers.\n",
        "        - 'autocomplete': Get intelligent suggestions to complete your writing.\n",
        "        - 'save_notes': Save notes within the tool's ecosystem to organize your thoughts.\n",
        "        - 'export_paper': Export your finished research paper with formatting intact.\n",
        "\n",
        "    query : str, optional\n",
        "        The specific query for the operation. For 'find_citations' or 'autocomplete', provide a topic, phrase, or context\n",
        "        related to your work. This parameter is required for these operations.\n",
        "\n",
        "    citation_source : str, optional\n",
        "        The database or source to use for finding citations. Defaults to SciSpace if not specified.\n",
        "\n",
        "    note_content : str, optional\n",
        "        The content of the note to save. This parameter is required for the 'save_notes' operation.\n",
        "\n",
        "    file_format : str, optional\n",
        "        The desired export format for the research paper. Required for the 'export_paper' operation. Supported formats:\n",
        "        - 'PDF'\n",
        "        - 'DOCX'\n",
        "\n",
        "    output_format : str, optional, default = \"text\"\n",
        "        The format for the tool's response output. Supported formats include:\n",
        "        - 'text': Plain text (default)\n",
        "        - 'json': JSON structure\n",
        "        - 'markdown': Markdown-formatted output\n",
        "\n",
        "    language : str, optional, default = \"English\"\n",
        "        The language for autocompletion suggestions or saved notes. Default is English, but supports a wide range of languages.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    str\n",
        "        A response based on the operation performed. This may include citations, autocompleted text, confirmation of saved notes,\n",
        "        or a link to the exported paper.\n",
        "\n",
        "    \"\"\"\n",
        "    pass\n",
        "tool2 = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"ai_writer\",\n",
        "        \"description\": \"\"\"An AI-powered tool to assist in writing research papers with confidence by providing citation discovery, text autocompletion, note management, and export functionality.\n",
        "User's can create new notebook, edit old and do a lot of ai powered assistant within notebook in realtime.\"\"\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"operation\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The type of operation to perform. Options include 'find_citations', 'autocomplete', 'save_notes', 'export_paper'.\",\n",
        "                    \"enum\": [\"find_citations\", \"autocomplete\", \"save_notes\", \"export_paper\"]\n",
        "                },\n",
        "                \"query\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The specific query for citation discovery or text completion. For example, a topic, phrase, or context for writing.\"\n",
        "                },\n",
        "                \"citation_source\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Optional. A specific citation database or source for finding references. If omitted, the default SciSpace database is used.\",\n",
        "                    \"default\": \"SciSpace\"\n",
        "                },\n",
        "                \"note_content\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Optional. Content of the note to save. Required for 'save_notes' operation.\",\n",
        "                },\n",
        "                \"file_format\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Optional. Format to export the paper in. Required for 'export_paper' operation. Supported formats: 'PDF', 'DOCX'.\",\n",
        "                    \"enum\": [\"PDF\", \"DOCX\"]\n",
        "                },\n",
        "                \"output_format\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Optional. Format of the response output. Options include 'text', 'json', 'markdown'. Default is 'text'.\",\n",
        "                    \"enum\": [\"text\", \"json\", \"markdown\"],\n",
        "                    \"default\": \"text\"\n",
        "                },\n",
        "                \"language\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Optional. Language for autocomplete suggestions or saved notes. Default is English.\",\n",
        "                    \"default\": \"English\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"operation\"]\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "h0OyldRYTYTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def literature_review_tool(\n",
        "    query: str,\n",
        "    review_type: str,\n",
        "    filters: dict = None,\n",
        "    custom_columns: list = None,\n",
        "    output_format: str = \"text\",\n",
        "    language: str = \"English\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Literature Review Tool: A powerful assistant for discovering and reviewing research papers using AI. Provides semantic similarity-based answers, concise reviews, and customizable features.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    query : str\n",
        "        The research topic or keywords to find relevant papers.\n",
        "\n",
        "    language : str, optional, default = \"English\"\n",
        "        The language of the review output. Default is English.\n",
        "\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "tool3 = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"literature_review\",\n",
        "        \"description\": \"Discover new research papers and perform a quick AI-powered literature survey with semantic similarity, concise reviews, and customizable features in your own language.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"query\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The research topic or keywords or concise search query for discovering and reviewing relevant papers.\"\n",
        "                },\n",
        "                \"language\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Language for the review output. Default is English.\",\n",
        "                    \"default\": \"English\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"query\"]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "M4-rYrc6b1kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_topics_tool(\n",
        "    query: str,\n",
        "    topic_type: str,\n",
        "    export_format: str = \"None\",\n",
        "    language: str = \"English\",\n",
        "    source_inclusion: bool = True\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Find Topics Tool: An AI-powered tool to dive deeper into research papers and extract insightful topics with grounded answers and explanations.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    query : str\n",
        "        The research topic or keywords for extracting and summarizing related topics.\n",
        "\n",
        "    language : str, optional, default = \"English\"\n",
        "        The language for the output. Default is English.\n",
        "\n",
        "    source_inclusion : bool, optional, default = True\n",
        "        Whether to include sources for the topics in the output.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    str\n",
        "        Extracted topics or summaries in the specified format.\n",
        "\n",
        "    \"\"\"\n",
        "    pass\n",
        "tool4 = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"find_topics\",\n",
        "        \"description\": \"Extract insightful topics from research papers and get grounded, summarized answers for top semantically similar topics in multiple languages.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"query\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The research topic or keywords to extract and summarize related topics.\"\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"query\"]\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "EiPfcmECkjFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data_tool(\n",
        "    query: str,\n",
        "    export_format: str = \"None\",\n",
        "    language: str = \"English\",\n",
        "    source_inclusion: bool = True,\n",
        "    papers: list = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Extract Data Tool: Extract summaries, conclusions, and findings from multiple research papers and provide them in a structured format.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    query : str\n",
        "        The research topic or keywords for extracting data from papers.\n",
        "\n",
        "    export_format : str, optional, default = \"None\"\n",
        "        The format for exporting the results (e.g., \"CSV\", \"Excel\", \"RIS\", etc.).\n",
        "\n",
        "    language : str, optional, default = \"English\"\n",
        "        The language for the output. Default is English.\n",
        "\n",
        "    source_inclusion : bool, optional, default = True\n",
        "        Whether to include citations and sources for the data in the output.\n",
        "\n",
        "    papers : list, optional, default = None\n",
        "        A list of PDF papers to extract data from.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    str\n",
        "        The extracted data or an exported file (depending on the format).\n",
        "\n",
        "    \"\"\"\n",
        "    pass\n",
        "tool5 = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"extract_data\",\n",
        "        \"description\": \"This Tool Extract summaries, conclusions, and findings from multiple research papers and provide them in a structured format. Features :Semantic Search, Extract & Compare information,Citation-backed insights\\\n",
        "        multiple Language support, Paper Summary, Export in multiple formats\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"files\": {\n",
        "                    \"type\": \"list\",\n",
        "                    \"description\": \"Optional,The list of files url path\"\n",
        "                },\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "odOQe8fVqKdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def paraphraser_tool(\n",
        "    input_text: str,\n",
        "    tone: str,\n",
        "    language: str = \"English\",\n",
        "    include_original: bool = False\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Paraphraser Tool: Makes academic writing clear and original by paraphrasing input text in various tones or personas.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    input_text : str\n",
        "        The text to be paraphrased.\n",
        "\n",
        "    tone : str\n",
        "        The desired tone or persona for paraphrasing, such as 'Academic', 'Fluent', 'Formal', 'Creative', etc.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    str\n",
        "        The paraphrased text, optionally including the original text for comparison.\n",
        "    \"\"\"\n",
        "    pass\n",
        "tool6 = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"paraphraser\",\n",
        "        \"description\": \"Paraphrase input text into different tones or personas with grammatical correctness in multiple languages.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"input_text\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The text to be paraphrased.\"\n",
        "                },\n",
        "                \"tone\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The tone or persona for paraphrasing, such as 'Academic', 'Fluent', 'Formal', 'Creative', etc.\"\n",
        "                }\n",
        "            },\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "j-4y4VeFwnK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def citation_generator_tool(\n",
        "    input_data: str,\n",
        "    citation_style: str,\n",
        "    export_format: str = \"BibTeX\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Citation Generator Tool: Generate citations in various formats from a title or URL and export them.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    input_data : str\n",
        "        The title or URL of the source to generate the citation.\n",
        "\n",
        "    citation_style : str\n",
        "        The desired citation format, e.g., 'APA', 'MLA', or any supported style.\n",
        "\n",
        "    export_format : str, optional, default = \"BibTeX\"\n",
        "        The format for exporting the citation, e.g., 'BibTeX', 'RIS', or 'Plain Text'.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    str\n",
        "        The generated citation in the specified format.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "tool7  = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"citation_generator\",\n",
        "        \"description\": \"Generate citations in various formats (APA, MLA, and 2300+ styles) from a title or URL and export them in BibTeX format.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"input_data\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The title or URL of the source to generate the citation.\"\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"input_data\"]\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "wAAZhwK6y-Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def academic_ai_detector_tool(\n",
        "    input_data: str,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Academic AI Detector Tool: Identify AI-generated content in scholarly documents or text input.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    input_data : str\n",
        "        The input data to analyze for AI-generated content. Can be a URL to a PDF file or plain text.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    dict\n",
        "        A dictionary with detection results, including likelihood scores for AI-generated content.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "tool8 =  {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"academic_ai_detector\",\n",
        "        \"description\": \"Detect AI-generated content (e.g., GPT-4, ChatGPT, Jasper) in scholarly documents or text input.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"input_data\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The input data to analyze for AI-generated content. Can be a URL to a PDF file or plain text.\"\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "HeL5VUCf1elL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def research_pdf_to_video(\n",
        "    pdf_url: str,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Research PDF to Video Tool: Converts research PDFs into engaging videos with features like voice-over, subtitles, and transitions.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    pdf_url : str\n",
        "        The URL or file path of the research PDF to convert into a video.\n",
        "    Returns:\n",
        "    -------\n",
        "    dict\n",
        "        A dictionary containing the status of the conversion process and the download link for the generated video.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "tool9 = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"pdf_to_video\",\n",
        "        \"description\": \"Convert research PDFs into engaging videos with voice-over, subtitles, and transitions.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"pdf_url\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The URL or file path of the research PDF to convert into a video.\"\n",
        "                },\n",
        "\n",
        "            },\n",
        "            \"required\": [\"pdf_url\"]\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "wjp9bkqr3DBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tools\n",
        "tools = [tool1,tool2,tool3,tool4,tool5,tool6,tool7,tool8,tool9]\n",
        "\n",
        "tools_str = ''\n",
        "for tool in tools:\n",
        "  tools_str += f\"title: {tool['function']['name']}\\nDescription: {tool['function']['description']}\\nparameters: {tool['function']['parameters']}\\n\\n\"\n",
        "print(tools_str)"
      ],
      "metadata": {
        "id": "j4Lwr0OEqJl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb03ef33-0028-429d-dd38-a26166a32d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "title: chat_with_pdf\n",
            "Description: This tool is for Interact with a PDF file to perform various operations such as answering questions, summarizing sections, explaining highlighted text, recommending related papers, or taking notes.\n",
            "parameters: {'type': 'object', 'properties': {'pdf_file_url': {'type': 'string', 'description': 'The URL or path to the PDF file. Required to perform any operation on the PDF.'}, 'query': {'type': 'string', 'description': 'A question or search query to find specific answers or sections in the PDF. Can be left blank if performing a general operation.'}, 'operation': {'type': 'string', 'enum': ['get_citation_answers', 'get_summary', 'highlight_explanation', 'get_related_papers', 'take_notes'], 'description': 'The specific operation to perform on the PDF, such as answering questions, summarizing, or finding related papers.'}, 'section': {'type': 'string', 'description': \"The specific section of the PDF to target (e.g., 'Introduction', 'Methods', 'Conclusion'). Leave blank for the entire document.\"}, 'highlighted_text': {'type': 'string', 'description': \"Specific text from the PDF to get a simplified explanation or find related papers. Required for 'highlight_explanation' and 'get_related_papers' operations.\"}, 'language': {'type': 'string', 'description': \"The language in which the response should be provided. Default is 'English'. Supports 75+ languages.\", 'default': 'English'}, 'note': {'type': 'string', 'description': \"Text for the note to be added. Used in the 'take_notes' operation to store custom annotations.\"}, 'output_format': {'type': 'string', 'enum': ['plain_text', 'json', 'markdown'], 'description': 'The desired format for the output of the operation.', 'default': 'plain_text'}, 'citation_style': {'type': 'string', 'enum': ['APA', 'MLA', 'Chicago', 'Harvard'], 'description': \"The citation style for answers backed by citations. Used in 'get_citation_answers' operation.\", 'default': 'APA'}, 'related_paper_limit': {'type': 'integer', 'description': \"The maximum number of related papers to fetch. Only used in 'get_related_papers' operation.\", 'default': 5}}}\n",
            "\n",
            "title: ai_writer\n",
            "Description: An AI-powered tool to assist in writing research papers with confidence by providing citation discovery, text autocompletion, note management, and export functionality.\n",
            "User's can create new notebook, edit old and do a lot of ai powered assistant within notebook in realtime.\n",
            "parameters: {'type': 'object', 'properties': {'operation': {'type': 'string', 'description': \"The type of operation to perform. Options include 'find_citations', 'autocomplete', 'save_notes', 'export_paper'.\", 'enum': ['find_citations', 'autocomplete', 'save_notes', 'export_paper']}, 'query': {'type': 'string', 'description': 'The specific query for citation discovery or text completion. For example, a topic, phrase, or context for writing.'}, 'citation_source': {'type': 'string', 'description': 'Optional. A specific citation database or source for finding references. If omitted, the default SciSpace database is used.', 'default': 'SciSpace'}, 'note_content': {'type': 'string', 'description': \"Optional. Content of the note to save. Required for 'save_notes' operation.\"}, 'file_format': {'type': 'string', 'description': \"Optional. Format to export the paper in. Required for 'export_paper' operation. Supported formats: 'PDF', 'DOCX'.\", 'enum': ['PDF', 'DOCX']}, 'output_format': {'type': 'string', 'description': \"Optional. Format of the response output. Options include 'text', 'json', 'markdown'. Default is 'text'.\", 'enum': ['text', 'json', 'markdown'], 'default': 'text'}, 'language': {'type': 'string', 'description': 'Optional. Language for autocomplete suggestions or saved notes. Default is English.', 'default': 'English'}}, 'required': ['operation']}\n",
            "\n",
            "title: literature_review\n",
            "Description: Discover new research papers and perform a quick AI-powered literature survey with semantic similarity, concise reviews, and customizable features in your own language.\n",
            "parameters: {'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'The research topic or keywords or concise search query for discovering and reviewing relevant papers.'}, 'language': {'type': 'string', 'description': 'Language for the review output. Default is English.', 'default': 'English'}}, 'required': ['query']}\n",
            "\n",
            "title: extract_data\n",
            "Description: This Tool Extract summaries, conclusions, and findings from multiple research papers and provide them in a structured format. Features :Semantic Search,Extract & Compare information,Citation-backed insights        multiple Language support,Paper Summary,Export in multiple formats\n",
            "parameters: {'type': 'object', 'properties': {'files': {'type': 'list', 'description': 'Optional,The list of files path'}}}\n",
            "\n",
            "title: extract_data\n",
            "Description: This Tool Extract summaries, conclusions, and findings from multiple research papers and provide them in a structured format. Features :Semantic Search, Extract & Compare information,Citation-backed insights        multiple Language support, Paper Summary, Export in multiple formats\n",
            "parameters: {'type': 'object', 'properties': {'files': {'type': 'list', 'description': 'Optional,The list of files url path'}}}\n",
            "\n",
            "title: paraphraser\n",
            "Description: Paraphrase input text into different tones or personas with grammatical correctness in multiple languages.\n",
            "parameters: {'type': 'object', 'properties': {'input_text': {'type': 'string', 'description': 'The text to be paraphrased.'}, 'tone': {'type': 'string', 'description': \"The tone or persona for paraphrasing, such as 'Academic', 'Fluent', 'Formal', 'Creative', etc.\"}}}\n",
            "\n",
            "title: citation_generator\n",
            "Description: Generate citations in various formats (APA, MLA, and 2300+ styles) from a title or URL and export them in BibTeX format.\n",
            "parameters: {'type': 'object', 'properties': {'input_data': {'type': 'string', 'description': 'The title or URL of the source to generate the citation.'}}, 'required': ['input_data']}\n",
            "\n",
            "title: academic_ai_detector\n",
            "Description: Detect AI-generated content (e.g., GPT-4, ChatGPT, Jasper) in scholarly documents or text input.\n",
            "parameters: {'type': 'object', 'properties': {'input_data': {'type': 'string', 'description': 'The input data to analyze for AI-generated content. Can be a URL to a PDF file or plain text.'}}}\n",
            "\n",
            "title: pdf_to_video\n",
            "Description: Convert research PDFs into engaging videos with voice-over, subtitles, and transitions.\n",
            "parameters: {'type': 'object', 'properties': {'pdf_url': {'type': 'string', 'description': 'The URL or file path of the research PDF to convert into a video.'}}, 'required': ['pdf_url']}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "think_plan_tools_selection =\"\"\"You are a tool or action selection system for Typeset.\\\n",
        "Your role involves selecting single tool or action from given tools to the user query.\n",
        "<tools>\n",
        "{tools_all}\n",
        "</tools>\n",
        "Before Selecting any tool follow this strategy:\n",
        "Think and planning:\n",
        "  - Read user input carefully. Break user query into smaller smaller parts and predict intent.\n",
        "  - Define Selection confidence scale :\n",
        "       - Certain: If the tool title or description and input parameters (if any required) semantically closely matches up to 100% and there is no ambiguity with other tools.\n",
        "       - High: If the tool title or description and input parameters (if any required) show a semantic resemblance and there is no ambiguity with other tools.\n",
        "       - Low: If there is ambiguity with other tools.\n",
        "       - very Low: If there is little to no match with any title,description.\n",
        "  - Only select tools (maximum 3) whose selection confidence is High or Certain.\n",
        "  - Finally select best tool from last 3 tools based on the selection confidence and user intent\n",
        "  - If you could not selected any tool after applying selection confidence scale then ask further details fas tool cannot be confidently selected.\n",
        "  - Always extract valid arguments value after removing spelling ,grammatical or any mistakes.\n",
        "  - Do not return default arguments.\n",
        "\n",
        "Always return your output in json format like this or empty json:\n",
        "{{\n",
        "  \"tool\":\"tool title\",\n",
        "  \"arguments\":{{\n",
        "    \"argument_name\":\"valid argument_value extracted from user input\",\n",
        "    }}\n",
        "}}\n",
        "\"\"\"\n",
        "think_plan_tools_selection = think_plan_tools_selection.format(tools_all=tools_str)\n",
        "print(think_plan_tools_selection)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cJfiUDbw9N1",
        "outputId": "4ebc7179-6dd6-447b-a0ab-0e69e052b0a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a tool or action selection system for Typeset.Your role involves selecting single tool or action from given tools to the user query.\n",
            "<tools>\n",
            "title: chat_with_pdf\n",
            "Description: This tool is for Interact with a PDF file to perform various operations such as answering questions, summarizing sections, explaining highlighted text, recommending related papers, or taking notes.\n",
            "parameters: {'type': 'object', 'properties': {'pdf_file_url': {'type': 'string', 'description': 'The URL or path to the PDF file. Required to perform any operation on the PDF.'}, 'query': {'type': 'string', 'description': 'A question or search query to find specific answers or sections in the PDF. Can be left blank if performing a general operation.'}, 'operation': {'type': 'string', 'enum': ['get_citation_answers', 'get_summary', 'highlight_explanation', 'get_related_papers', 'take_notes'], 'description': 'The specific operation to perform on the PDF, such as answering questions, summarizing, or finding related papers.'}, 'section': {'type': 'string', 'description': \"The specific section of the PDF to target (e.g., 'Introduction', 'Methods', 'Conclusion'). Leave blank for the entire document.\"}, 'highlighted_text': {'type': 'string', 'description': \"Specific text from the PDF to get a simplified explanation or find related papers. Required for 'highlight_explanation' and 'get_related_papers' operations.\"}, 'language': {'type': 'string', 'description': \"The language in which the response should be provided. Default is 'English'. Supports 75+ languages.\", 'default': 'English'}, 'note': {'type': 'string', 'description': \"Text for the note to be added. Used in the 'take_notes' operation to store custom annotations.\"}, 'output_format': {'type': 'string', 'enum': ['plain_text', 'json', 'markdown'], 'description': 'The desired format for the output of the operation.', 'default': 'plain_text'}, 'citation_style': {'type': 'string', 'enum': ['APA', 'MLA', 'Chicago', 'Harvard'], 'description': \"The citation style for answers backed by citations. Used in 'get_citation_answers' operation.\", 'default': 'APA'}, 'related_paper_limit': {'type': 'integer', 'description': \"The maximum number of related papers to fetch. Only used in 'get_related_papers' operation.\", 'default': 5}}}\n",
            "\n",
            "title: ai_writer\n",
            "Description: An AI-powered tool to assist in writing research papers with confidence by providing citation discovery, text autocompletion, note management, and export functionality.\n",
            "User's can create new notebook, edit old and do a lot of ai powered assistant within notebook in realtime.\n",
            "parameters: {'type': 'object', 'properties': {'operation': {'type': 'string', 'description': \"The type of operation to perform. Options include 'find_citations', 'autocomplete', 'save_notes', 'export_paper'.\", 'enum': ['find_citations', 'autocomplete', 'save_notes', 'export_paper']}, 'query': {'type': 'string', 'description': 'The specific query for citation discovery or text completion. For example, a topic, phrase, or context for writing.'}, 'citation_source': {'type': 'string', 'description': 'Optional. A specific citation database or source for finding references. If omitted, the default SciSpace database is used.', 'default': 'SciSpace'}, 'note_content': {'type': 'string', 'description': \"Optional. Content of the note to save. Required for 'save_notes' operation.\"}, 'file_format': {'type': 'string', 'description': \"Optional. Format to export the paper in. Required for 'export_paper' operation. Supported formats: 'PDF', 'DOCX'.\", 'enum': ['PDF', 'DOCX']}, 'output_format': {'type': 'string', 'description': \"Optional. Format of the response output. Options include 'text', 'json', 'markdown'. Default is 'text'.\", 'enum': ['text', 'json', 'markdown'], 'default': 'text'}, 'language': {'type': 'string', 'description': 'Optional. Language for autocomplete suggestions or saved notes. Default is English.', 'default': 'English'}}, 'required': ['operation']}\n",
            "\n",
            "title: literature_review\n",
            "Description: Discover new research papers and perform a quick AI-powered literature survey with semantic similarity, concise reviews, and customizable features in your own language.\n",
            "parameters: {'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'The research topic or keywords or concise search query for discovering and reviewing relevant papers.'}, 'language': {'type': 'string', 'description': 'Language for the review output. Default is English.', 'default': 'English'}}, 'required': ['query']}\n",
            "\n",
            "title: extract_data\n",
            "Description: This Tool Extract summaries, conclusions, and findings from multiple research papers and provide them in a structured format. Features :Semantic Search,Extract & Compare information,Citation-backed insights        multiple Language support,Paper Summary,Export in multiple formats\n",
            "parameters: {'type': 'object', 'properties': {'files': {'type': 'list', 'description': 'Optional,The list of files path'}}}\n",
            "\n",
            "title: extract_data\n",
            "Description: This Tool Extract summaries, conclusions, and findings from multiple research papers and provide them in a structured format. Features :Semantic Search, Extract & Compare information,Citation-backed insights        multiple Language support, Paper Summary, Export in multiple formats\n",
            "parameters: {'type': 'object', 'properties': {'files': {'type': 'list', 'description': 'Optional,The list of files url path'}}}\n",
            "\n",
            "title: paraphraser\n",
            "Description: Paraphrase input text into different tones or personas with grammatical correctness in multiple languages.\n",
            "parameters: {'type': 'object', 'properties': {'input_text': {'type': 'string', 'description': 'The text to be paraphrased.'}, 'tone': {'type': 'string', 'description': \"The tone or persona for paraphrasing, such as 'Academic', 'Fluent', 'Formal', 'Creative', etc.\"}}}\n",
            "\n",
            "title: citation_generator\n",
            "Description: Generate citations in various formats (APA, MLA, and 2300+ styles) from a title or URL and export them in BibTeX format.\n",
            "parameters: {'type': 'object', 'properties': {'input_data': {'type': 'string', 'description': 'The title or URL of the source to generate the citation.'}}, 'required': ['input_data']}\n",
            "\n",
            "title: academic_ai_detector\n",
            "Description: Detect AI-generated content (e.g., GPT-4, ChatGPT, Jasper) in scholarly documents or text input.\n",
            "parameters: {'type': 'object', 'properties': {'input_data': {'type': 'string', 'description': 'The input data to analyze for AI-generated content. Can be a URL to a PDF file or plain text.'}}}\n",
            "\n",
            "title: pdf_to_video\n",
            "Description: Convert research PDFs into engaging videos with voice-over, subtitles, and transitions.\n",
            "parameters: {'type': 'object', 'properties': {'pdf_url': {'type': 'string', 'description': 'The URL or file path of the research PDF to convert into a video.'}}, 'required': ['pdf_url']}\n",
            "\n",
            "\n",
            "</tools>\n",
            "Before Selecting any tool follow this strategy:\n",
            "Think and planning:\n",
            "  - Read user input carefully. Break user query into smaller smaller parts and predict intent.\n",
            "  - Define Selection confidence scale :\n",
            "       - Certain: If the tool title or description and input parameters (if any required) semantically closely matches up to 100% and there is no ambiguity with other tools.\n",
            "       - High: If the tool title or description and input parameters (if any required) show a semantic resemblance and there is no ambiguity with other tools.\n",
            "       - Low: If there is ambiguity with other tools.\n",
            "       - very Low: If there is little to no match with any title,description.\n",
            "  - Only select tools (maximum 3) whose selection confidence is High or Certain.\n",
            "  - Finally select best tool from last 3 tools based on the selection confidence and user intent\n",
            "  - If you could not selected any tool after applying selection confidence scale then ask further details fas tool cannot be confidently selected.\n",
            "  - Always extract valid arguments value after removing spelling ,grammatical or any mistakes.\n",
            "  - Do not return default arguments.\n",
            "\n",
            "Always return your output in json format like this or empty json:\n",
            "{\n",
            "  \"tool\":\"tool title\",\n",
            "  \"arguments\":{\n",
            "    \"argument_name\":\"valid argument_value extracted from user input\", \n",
            "    }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_KEY = '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'\n",
        "tool_model = GenResponse(\n",
        "    OPENAI_KEY,\n",
        "    base_url = 'https://api.openai.com/v1',\n",
        "    openai=True\n",
        ")"
      ],
      "metadata": {
        "id": "IWA5Lo0F0EWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [\n",
        "    # Get Citation-Backed Answers\n",
        "    \"What are the main conclusions of this study? [PDF: https://example.com/sample-paper1.pdf]\",\n",
        "    \"What methods were used in the research? [PDF: https://example.com/sample-paper2.pdf, Citation Style: MLA]\",\n",
        "    \"How does this research compare to similar studies? [PDF: https://example.com/sample-paper3.pdf, Output Format: JSON, Language: French]\",\n",
        "\n",
        "    # Get Summary\n",
        "    \"Summarize this paper section by section. [PDF: https://example.com/sample-paper4.pdf]\",\n",
        "    \"Provide a summary of the Introduction section. [PDF: https://example.com/sample-paper5.pdf, Section: Introduction]\",\n",
        "    \"Summarize the Conclusion section in Portuguese. [PDF: https://example.com/sample-paper6.pdf, Section: Conclusion, Language: Portuguese, Output Format: Markdown]\",\n",
        "\n",
        "    # Explain Highlighted Text\n",
        "    \"Explain this sentence: 'Quantum entanglement enables instantaneous state changes.' [PDF: https://example.com/sample-paper7.pdf]\",\n",
        "    \"Explain this highlighted text: 'The Transformer architecture revolutionized natural language processing.' [PDF: https://example.com/sample-paper8.pdf, Language: Spanish]\",\n",
        "    \"Explain: 'The integration of blockchain with IoT enhances security.' in markdown format. [PDF: https://example.com/sample-paper9.pdf, Output Format: Markdown, Language: Italian]\",\n",
        "\n",
        "    # Get Related Papers\n",
        "    \"Find related papers to this idea: 'Deep learning approaches in image recognition.' [PDF: https://example.com/sample-paper10.pdf]\",\n",
        "    \"Recommend 3 papers related to 'Self-attention mechanism in neural networks.' [PDF: https://example.com/sample-paper11.pdf, Related Paper Limit: 3]\",\n",
        "    \"Are there any related papers to 'Support vector machines in classification tasks' with no limit? [PDF: https://example.com/sample-paper12.pdf]\",\n",
        "\n",
        "    # Take Notes\n",
        "    \"Add this note: 'Important: Explore alternative algorithms for better performance.' [PDF: https://example.com/sample-paper13.pdf]\",\n",
        "    \"Save this note: 'Consider the scalability issues mentioned in the discussion section.' [PDF: https://example.com/sample-paper14.pdf, Output Format: JSON]\",\n",
        "    \"Add this note in Japanese: 'Review the statistical methods used in the analysis.' [PDF: https://example.com/sample-paper15.pdf, Language: Japanese, Output Format: JSON]\",\n",
        "\n",
        "    # Test for Invalid or Edge Cases\n",
        "    \"What future directions are proposed in the paper? [PDF: https://example.com/sample-paper16.pdf, Citation Style: Harvard]\",\n",
        "    \"Summarize the Abstract section of this paper. [PDF: https://example.com/sample-paper17.pdf, Section: Abstract]\",\n",
        "    \"Can you summarize this paper? [PDF: https://example.com/sample-paper18.pdf]\",\n",
        "    \"Explain this complex sentence with a lot of technical terms. [PDF: https://example.com/sample-paper19.pdf]\",\n",
        "    \"Perform an operation to answer this query: 'This should fail.' [PDF: https://example.com/sample-paper20.pdf, Operation: Invalid]\",\n",
        "    \"Describe the experimental setup in this paper. [PDF: https://example.com/sample-paper21.pdf, Citation Style: Chicago, Output Format: JSON, Language: Russian]\"\n",
        "]\n",
        "\n",
        "test_cases = [\n",
        "    # Test cases for 'find_citations'\n",
        "    \"Find citations for 'Quantum computing advancements'. [operation: 'find_citations', query: 'Quantum computing advancements']\",\n",
        "    \"Discover citations on 'Machine learning applications' from PubMed. [operation: 'find_citations', query: 'Machine learning applications', citation_source: 'PubMed']\",\n",
        "    \"Find references for 'Impact of climate change'. [operation: 'find_citations', query: 'Impact of climate change', output_format: 'markdown']\",\n",
        "\n",
        "    # Test cases for 'autocomplete'\n",
        "    \"Autocomplete text for 'Deep learning techniques in...'. [operation: 'autocomplete', query: 'Deep learning techniques in...']\",\n",
        "    \"Provide autocomplete suggestions for 'Artificial Intelligence in healthcare' in Spanish. [operation: 'autocomplete', query: 'Artificial Intelligence in healthcare', language: 'Spanish']\",\n",
        "    \"Generate suggestions for 'Blockchain applications in supply chain' with JSON output. [operation: 'autocomplete', query: 'Blockchain applications in supply chain', output_format: 'json']\",\n",
        "\n",
        "    # Test cases for 'save_notes'\n",
        "    \"Save the note: 'Remember to include related works section.' [operation: 'save_notes', note_content: 'Remember to include related works section.']\",\n",
        "    \"Save the note in German: 'Fügen Sie die Quellenangaben hinzu.' [operation: 'save_notes', note_content: 'Fügen Sie die Quellenangaben hinzu.', language: 'German']\",\n",
        "    \"Save a note with JSON format: 'Discuss experimental limitations in detail.' [operation: 'save_notes', note_content: 'Discuss experimental limitations in detail.', output_format: 'json']\",\n",
        "\n",
        "    # Test cases for 'export_paper'\n",
        "    \"Export the paper in PDF format. [operation: 'export_paper', file_format: 'PDF']\",\n",
        "    \"Export the paper in DOCX with Markdown output. [operation: 'export_paper', file_format: 'DOCX', output_format: 'markdown']\",\n",
        "    \"Export the paper to check formatting. [operation: 'export_paper', file_format: 'PDF', output_format: 'json']\",\n",
        "\n",
        "    # Combination of arguments for multiple operations\n",
        "    \"Find citations for 'Neural networks in image processing' and export results in JSON. [operation: 'find_citations', query: 'Neural networks in image processing', output_format: 'json']\",\n",
        "    \"Autocomplete 'Reinforcement learning techniques for...' and save the suggestion as a note. [operation: 'autocomplete', query: 'Reinforcement learning techniques for...', output_format: 'text', language: 'French']\",\n",
        "    \"Generate citations and export paper in DOCX. [operation: 'find_citations', query: 'AI in education', citation_source: 'Google Scholar', file_format: 'DOCX']\",\n",
        "\n",
        "    # Edge cases\n",
        "    \"Invalid operation test. [operation: 'invalid_operation']\",\n",
        "    \"Missing query for citations. [operation: 'find_citations']\",\n",
        "    \"Missing note content for saving notes. [operation: 'save_notes']\",\n",
        "    \"Export without file format. [operation: 'export_paper']\",\n",
        "    \"Provide autocomplete suggestions in an unsupported language. [operation: 'autocomplete', query: 'AI ethics', language: 'Klingon']\",\n",
        "    \"Save note with an empty string. [operation: 'save_notes', note_content: '']\",\n",
        "    \"Find citations without specifying a database. [operation: 'find_citations', query: 'Effects of vaccination on global health']\"\n",
        "]\n",
        "\n",
        "queries += test_cases\n",
        "test_cases = [\n",
        "    # Concise review\n",
        "    \"Generate a concise review on 'Artificial Intelligence in education'. [query: 'Artificial Intelligence in education', review_type: 'concise_review']\",\n",
        "    \"Summarize similar papers on 'Climate change and renewable energy' from 2010 to 2023. [query: 'Climate change and renewable energy', review_type: 'concise_review', filters: {'publication_year': [2010, 2023]}]\",\n",
        "    \"Provide a concise review of 'COVID-19 impact on mental health' in French. [query: 'COVID-19 impact on mental health', review_type: 'concise_review', language: 'French']\",\n",
        "\n",
        "    # Semantic similar\n",
        "    \"Find semantically similar papers for 'Advancements in battery technology'. [query: 'Advancements in battery technology', review_type: 'semantic_similar']\",\n",
        "    \"Retrieve similar papers on 'Deep learning in NLP' from IEEE journals. [query: 'Deep learning in NLP', review_type: 'semantic_similar', filters: {'journal': 'IEEE'}]\",\n",
        "    \"Get similar papers for 'Cancer genomics' authored by 'Dr. John Doe'. [query: 'Cancer genomics', review_type: 'semantic_similar', filters: {'author': 'Dr. John Doe'}]\",\n",
        "\n",
        "    # Custom columns\n",
        "    \"Create a review with custom columns 'Abstract' and 'Citations' for 'Blockchain applications'. [query: 'Blockchain applications', review_type: 'custom_columns', custom_columns: ['Abstract', 'Citations']]\",\n",
        "    \"Add custom columns 'Keywords' and 'References' for papers on 'Neural networks in image recognition'. [query: 'Neural networks in image recognition', review_type: 'custom_columns', custom_columns: ['Keywords', 'References']]\",\n",
        "    \"Include a custom column 'Highlights' for the topic 'Evolutionary algorithms'. [query: 'Evolutionary algorithms', review_type: 'custom_columns', custom_columns: ['Highlights'], output_format: 'markdown']\",\n",
        "\n",
        "    # Combination queries\n",
        "    \"Find similar papers for 'Robotics in healthcare' and create a concise review. [query: 'Robotics in healthcare', review_type: 'semantic_similar', filters: {'publication_year': [2015, 2023], 'language': 'English'}]\",\n",
        "    \"Generate a concise review on 'AI ethics' and save it as JSON. [query: 'AI ethics', review_type: 'concise_review', output_format: 'json']\",\n",
        "\n",
        "    # Edge cases\n",
        "    \"Generate a concise review without a query. [review_type: 'concise_review']\",\n",
        "    \"Request custom columns without specifying columns. [query: 'Machine learning in finance', review_type: 'custom_columns']\",\n",
        "    \"Request semantic similar papers in an unsupported language. [query: 'Cryptography techniques', review_type: 'semantic_similar', filters: {'language': 'Elvish'}]\",\n",
        "    \"Find concise reviews for 'Quantum entanglement' with invalid publication year. [query: 'Quantum entanglement', review_type: 'concise_review', filters: {'publication_year': [2025, 2020]}]\"\n",
        "]\n",
        "queries += test_cases\n",
        "\n",
        "test_cases = [\n",
        "    # Summarized topics\n",
        "    \"Get summarized topics for 'Deep learning in healthcare'.\",\n",
        "    \"Summarize topics for 'Climate change adaptation' in French.\",\n",
        "    \"Find summarized topics for 'Blockchain in supply chain' without sources.\",\n",
        "\n",
        "    # Explained topics\n",
        "    \"Get explained topics for 'Quantum mechanics'.\",\n",
        "    \"List topics for 'AI ethics' and export to CSV. \",\n",
        "    \"Provide explained topics on 'Renewable energy solutions' with sources. \",\n",
        "    \"Explain topics on 'Natural language processing' in Spanish. \",\n",
        "\n",
        "    # Combination of arguments\n",
        "    \"Summarize topics on 'Genomics research' and export in JSON. \",\n",
        "    \"List topics on 'Robotics in manufacturing' in German and include sources.\",\n",
        "\n",
        "    # Edge cases\n",
        "    \"Request topics without specifying query. \",\n",
        "    \"Invalid topic type test.\",\n",
        "    \"Export topics without specifying format. \",\n",
        "    \"Summarize topics for 'Artificial Intelligence' in an unsupported language. \",\n",
        "    \"Explain topics for 'Evolutionary biology' with incorrect source inclusion type. \",\n",
        "    \"Find topics without specifying topic type.\"\n",
        "]\n",
        "queries += test_cases\n",
        "\n",
        "test_cases = [\n",
        "    # Summarized data extraction\n",
        "    \"Extract summary for research papers on 'Deep learning in healthcare'.\",\n",
        "    \"Get summary of conclusions from 'Climate change adaptation' research papers in French.\",\n",
        "    \"Extract summarized findings for 'Blockchain in supply chain' without including sources.\",\n",
        "\n",
        "    # Data extraction with sources\n",
        "    \"Get conclusions from papers on 'Quantum mechanics' with sources included.\",\n",
        "    \"Extract findings from research papers on 'AI ethics' and export to CSV.\",\n",
        "    \"Provide conclusions and findings for 'Renewable energy solutions' with sources.\",\n",
        "    \"Get summarized data from papers on 'Natural language processing' in Spanish.\",\n",
        "\n",
        "    # Combination of arguments\n",
        "    \"Summarize data for 'Genomics research' and export in JSON format.\",\n",
        "    \"Extract findings for 'Robotics in manufacturing' in German with sources included.\",\n",
        "\n",
        "    # Edge cases\n",
        "    \"Request data extraction without specifying a query.\",\n",
        "    \"Invalid export format specified for data extraction.\",\n",
        "    \"Export data without specifying format.\",\n",
        "    \"Request summary for 'Artificial Intelligence' in an unsupported language.\",\n",
        "    \"Extract conclusions for 'Evolutionary biology' with incorrect source inclusion type.\",\n",
        "    \"Extract data without specifying a topic or query.\"\n",
        "]\n",
        "queries += test_cases\n",
        "\n",
        "test_cases = [\n",
        "    # Paraphrasing in different tones\n",
        "    \"Paraphrase 'The results were significant and conclusive.' in Academic tone.\",\n",
        "    \"Paraphrase 'This is an amazing breakthrough!' in Formal tone.\",\n",
        "    \"Paraphrase 'We’re super excited about this!' in Fluent tone.\",\n",
        "    \"Paraphrase 'She was sad about the incident.' in Creative tone.\",\n",
        "\n",
        "    # Paraphrasing in different languages\n",
        "    \"Paraphrase 'The weather is pleasant today.' in Academic tone in French.\",\n",
        "    \"Paraphrase 'This is a revolutionary idea.' in Creative tone in Spanish.\",\n",
        "    \"Paraphrase 'The meeting was productive.' in Formal tone in German.\",\n",
        "\n",
        "    # Including original text in output\n",
        "    \"Paraphrase 'The solution was effective and well-received.' in Fluent tone, include original text.\",\n",
        "    \"Paraphrase 'The experiment yielded unexpected results.' in Academic tone, include original text.\",\n",
        "\n",
        "    # Edge cases\n",
        "    \"Paraphrase an empty string in Academic tone.\",\n",
        "    \"Paraphrase 'This idea is groundbreaking!' without specifying a tone.\",\n",
        "    \"Request paraphrasing in an unsupported language.\",\n",
        "    \"Provide an invalid tone for paraphrasing.\",\n",
        "    \"Paraphrase with include_original set to an incorrect type.\"\n",
        "]\n",
        "queries += test_cases\n",
        "test_cases = [\n",
        "    # Generate citation from title\n",
        "    \"Generate APA citation for 'Deep Learning in Healthcare' and export in BibTeX.\",\n",
        "    \"Create MLA citation for 'The Origin of Species' and export in RIS.\",\n",
        "    \"Generate Chicago citation for 'Artificial Intelligence and Society'.\",\n",
        "    \"Produce Harvard citation for 'Climate Change Adaptation' and export in Plain Text.\",\n",
        "\n",
        "    # Generate citation from URL\n",
        "    \"Generate APA citation for 'https://arxiv.org/abs/1234.5678' and export in BibTeX.\",\n",
        "    \"Create MLA citation for 'https://doi.org/10.1016/j.jbi.2021.04.009' and export in RIS.\",\n",
        "    \"Produce Chicago citation for 'https://www.example.com/research-paper'.\",\n",
        "    \"Generate Harvard citation for 'https://journals.sagepub.com/home/tcs' and export in Plain Text.\",\n",
        "\n",
        "    # Edge cases\n",
        "    \"Request citation for an invalid URL.\",\n",
        "    \"Generate citation without specifying citation style.\",\n",
        "    \"Request citation for a title that does not exist.\",\n",
        "    \"Provide unsupported citation style for generation.\",\n",
        "    \"Request export in an unsupported format.\",\n",
        "    \"Generate citation for an empty title or URL.\"\n",
        "]\n",
        "queries += test_cases\n",
        "test_cases = [\n",
        "    # Detect AI content from text input\n",
        "    \"Detect AI content in text: 'This research explores the impact of AI on society'.\",\n",
        "    \"Analyze text with low sensitivity: 'The results are generated by ChatGPT'.\",\n",
        "    \"Detect AI content in multilingual text: 'Ce texte est généré par IA'.\",\n",
        "    \"Analyze content with high sensitivity: 'Lorem ipsum AI generator test.'.\",\n",
        "\n",
        "    # Detect AI content from PDF URL\n",
        "    \"Analyze AI-generated content in 'https://example.com/scholarly_article.pdf'.\",\n",
        "    \"Detect AI content in a PDF with high sensitivity: 'https://example.com/research.pdf'.\",\n",
        "    \"Run detection on a scholarly paper PDF with medium sensitivity: 'https://example.com/document.pdf'.\",\n",
        "\n",
        "    # Edge cases\n",
        "    \"Run detection on empty input data.\",\n",
        "    \"Provide invalid PDF URL for analysis.\",\n",
        "    \"Analyze text with an unsupported sensitivity level.\",\n",
        "    \"Run detection on mixed input (text + URL).\",\n",
        "    \"Detect AI content in non-English text input.\",\n",
        "    \"Request AI detection for corrupted or inaccessible PDF files.\"\n",
        "]\n",
        "queries += test_cases\n",
        "test_cases = [\n",
        "    # Basic conversions\n",
        "    \"Convert PDF 'https://example.com/research.pdf' to video with default settings.\",\n",
        "    \"Generate video from 'https://example.com/thesis.pdf' with Spanish voice-over.\",\n",
        "    \"Create video for 'https://example.com/article.pdf' without subtitles.\",\n",
        "    \"Convert PDF 'https://example.com/review.pdf' with zoom transitions and output as AVI.\",\n",
        "\n",
        "    # Advanced configurations\n",
        "    \"Generate video from 'https://example.com/study.pdf' with French voice-over and slide transitions.\",\n",
        "    \"Create a video for 'https://example.com/paper.pdf' in MP4 with no subtitles and English narration.\",\n",
        "    \"Export video for 'https://example.com/research_paper.pdf' with MOV format and fade transitions.\",\n",
        "\n",
        "    # Edge cases\n",
        "    \"Handle invalid PDF URL input for video conversion.\",\n",
        "    \"Request video generation with unsupported voice-over language.\",\n",
        "    \"Attempt conversion with missing PDF URL input.\",\n",
        "    \"Specify an unsupported output video format.\",\n",
        "    \"Request video with unsupported transition style.\",\n",
        "    \"Handle corrupted PDF input during conversion.\"\n",
        "]\n",
        "queries += test_cases"
      ],
      "metadata": {
        "id": "xAqaxlgfEUSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for query in queries:\n",
        "  print(f\"Query: {query}\")\n",
        "  tool_sel = await tool_model.get_response(\n",
        "      system = think_plan_tools_selection,\n",
        "      chat_history = [],\n",
        "      query = query,\n",
        "      model_id = 'gpt-4o-mini',\n",
        "      temperature = 0.7,\n",
        "      max_tokens = 2000,\n",
        "      # tools = tools,\n",
        "      response_format= {\"type\":\"json_object\"}\n",
        "    )\n",
        "  if isinstance(tool_sel,Dict):\n",
        "    raise(tool_sel)\n",
        "  else:\n",
        "    if tool_sel.choices[0].finish_reason == 'tool_calls':\n",
        "      print(tool_sel.choices[0])\n",
        "      tools = tool_sel.choices[0].message.function_call\n",
        "      print(f\"params: {tools.arguments}\")\n",
        "      print(f\"Tool: {tools.name}\")\n",
        "    print(tool_sel.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "71tjZCuB9Nc0",
        "outputId": "2bed1438-97b1-4f33-b116-a8ba9e56881e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What are the main conclusions of this study? [PDF: https://example.com/sample-paper1.pdf]\n",
            "{\n",
            "  \"tool\":\"extract_data\",\n",
            "  \"arguments\":{\n",
            "    \"files\":[\"https://example.com/sample-paper1.pdf\"]\n",
            "  }\n",
            "}\n",
            "Query: What methods were used in the research? [PDF: https://example.com/sample-paper2.pdf, Citation Style: MLA]\n",
            "{\n",
            "  \"tool\":\"chat_with_pdf\",\n",
            "  \"arguments\":{\n",
            "    \"pdf_file_url\":\"https://example.com/sample-paper2.pdf\",\n",
            "    \"query\":\"What methods were used in the research?\",\n",
            "    \"operation\":\"get_citation_answers\",\n",
            "    \"citation_style\":\"MLA\"\n",
            "  }\n",
            "}\n",
            "Query: How does this research compare to similar studies? [PDF: https://example.com/sample-paper3.pdf, Output Format: JSON, Language: French]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CancelledError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-183-a494232a5911>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Query: {query}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   tool_sel = await tool_model.get_response(\n\u001b[0m\u001b[1;32m      4\u001b[0m       \u001b[0msystem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthink_plan_tools_selection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mchat_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-c0c8c324c0d5>\u001b[0m in \u001b[0;36mget_response\u001b[0;34m(self, query, system, chat_history, model_id, temperature, max_tokens, top_p, top_k, frequency_penalty, presence_penalty, stop, response_format, tools)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             response = await self.client.chat.completions.create(\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   1660\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m         return await self._post(\n\u001b[0m\u001b[1;32m   1662\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m             body=await async_maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0masync_to_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m         )\n\u001b[0;32m-> 1843\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1845\u001b[0m     async def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1535\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m         return await self._request(\n\u001b[0m\u001b[1;32m   1538\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m             response = await self._client.send(\n\u001b[0m\u001b[1;32m   1577\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m         response = await self._send_handling_auth(\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                 response = await self._send_handling_redirects(\n\u001b[0m\u001b[1;32m   1658\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1692\u001b[0m                 \u001b[0;32mawait\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAsyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    392\u001b[0m         )\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAsyncIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection_pool.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection_pool.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = await connection.handle_async_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/connection.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAsyncNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = await self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_async/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = await self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_backends/anyio.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0manyio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfail_after\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0manyio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEndOfStream\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: nocover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anyio/streams/tls.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bytes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m65536\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_sslobject_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEndOfStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anyio/streams/tls.py\u001b[0m in \u001b[0;36m_call_sslobject_method\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    136\u001b[0m                         \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransport_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_bio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransport_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mEndOfStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_bio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_eof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             ):\n\u001b[1;32m   1202\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_reading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpause_reading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/locks.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCancelledError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aHwl36Fe9__l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}